{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a63df3b4",
   "metadata": {},
   "source": [
    "참고 : https://zero-ai.tistory.com/62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eceafe69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA 사용 가능 여부: True\n",
      "GPU 이름: NVIDIA GB10\n",
      "CUDA 버전: 13.0\n",
      "PyTorch 버전: <module 'torch.version' from '/home/vsc/LLM_TUNE/myenv/lib/python3.12/site-packages/torch/version.py'>\n",
      "bf16 지원 여부: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vsc/LLM_TUNE/myenv/lib/python3.12/site-packages/torch/cuda/__init__.py:435: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  queued_call()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"CUDA 사용 가능 여부: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU 이름: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"CUDA 버전: {torch.version.cuda}\")\n",
    "print(f\"PyTorch 버전: {torch.version}\")\n",
    "print(f\"bf16 지원 여부: {torch.cuda.is_bf16_supported()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f6440e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O : 성공 : TL_10.개인정보.ICT.zip\n",
      "O : 성공 : TL_05.행정.zip\n",
      "X : 실패 (파일 깨짐): TL_01.민사.zip (크기: 18409624 bytes)\n",
      "O : 성공 : TL_06.기업.zip\n",
      "O : 성공 : TL_02.가사.zip\n",
      "O : 성공 : TL_04.형사B(일반형).zip\n",
      "O : 성공 : TL_08.특허.저작권.zip\n",
      "O : 성공 : TL_07.근로자.zip\n",
      "O : 성공 : TL_09.금융조세.zip\n",
      "O : 성공 : TL_03.형사A(생활형).zip\n",
      "O : 성공 : VL_02.가사.zip\n",
      "O : 성공 : VL_10.개인정보.ICT.zip\n",
      "O : 성공 : VL_07.근로자.zip\n",
      "O : 성공 : VL_09.금융조세.zip\n",
      "O : 성공 : VL_05.행정.zip\n",
      "O : 성공 : VL_08.특허.저작권.zip\n",
      "O : 성공 : VL_03.형사A(생활형).zip\n",
      "O : 성공 : VL_01.민사.zip\n",
      "O : 성공 : VL_04.형사B(일반형).zip\n",
      "O : 성공 : VL_06.기업.zip\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "target_dir_list = [\"./3.개방데이터/1.데이터/Training/02.라벨링데이터\", \"./3.개방데이터/1.데이터/Validation/02.라벨링데이터\"]\n",
    "extract_dir_list = [\"./3.개방데이터/1.데이터/extract/Training/02.라벨링데이터\", \"./3.개방데이터/1.데이터/extract/Validation/02.라벨링데이터\", \"./3.개방데이터/1.데이터/raw/Training/0.1.원천데이터\"]\n",
    "\n",
    "if not os.path.exists(extract_dir_list[0]):\n",
    "    os.makedirs(extract_dir_list[0])\n",
    "if not os.path.exists(extract_dir_list[1]):\n",
    "    os.makedirs(extract_dir_list[1])\n",
    "\n",
    "for target_dir, extract_dir in zip(target_dir_list, extract_dir_list):\n",
    "    for file in os.listdir(target_dir):\n",
    "        if file.endswith(\".zip\"):\n",
    "            file_path = os.path.join(target_dir, file)\n",
    "\n",
    "            try :\n",
    "                with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
    "                    zip_ref.extractall(extract_dir)\n",
    "                    print(f\"O : 성공 : {file}\")\n",
    "            except zipfile.BadZipFile :\n",
    "                file_size = os.path.getsize(file_path)\n",
    "                print(f\"X : 실패 (파일 깨짐): {file} (크기: {file_size} bytes)\")\n",
    "        \n",
    "            except Exception as e:\n",
    "                print(f\"기타 에러 ({file}): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de7b9f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O : 성공 : 01.원천데이터\\TS_1.판례_10.개인정보.ICT.zip\n",
      "O : 성공 : 01.원천데이터\\TS_2.심결례_10.개인정보.ICT.zip\n",
      "O : 성공 : 01.원천데이터\\TS_2.심결례_06.기업.zip\n",
      "O : 성공 : 01.원천데이터\\TS_1.판례_05.행정.zip\n",
      "O : 성공 : 01.원천데이터\\TS_2.심결례_05.행정.zip\n",
      "O : 성공 : 01.원천데이터\\TS_2.심결례_09.금융조세.zip\n",
      "O : 성공 : 01.원천데이터\\TS_2.심결례_04.형사B(일반형).zip\n",
      "O : 성공 : 01.원천데이터\\TS_1.판례_02.가사.zip\n",
      "O : 성공 : 01.원천데이터\\TS_2.심결례_07.근로자.zip\n",
      "O : 성공 : 01.원천데이터\\TS_2.심결례_08.특허.저작권.zip\n",
      "O : 성공 : 01.원천데이터\\TS_1.판례_09.금융조세.zip\n",
      "O : 성공 : 01.원천데이터\\TS_1.판례_07.근로자.zip\n",
      "O : 성공 : 01.원천데이터\\TS_1.판례_08.특허.저작권.zip\n",
      "O : 성공 : 01.원천데이터\\TS_1.판례_03.형사A(생활형).zip\n",
      "O : 성공 : 01.원천데이터\\TS_2.심결례_01.민사.zip\n",
      "O : 성공 : 01.원천데이터\\TS_1.판례_06.기업.zip\n",
      "O : 성공 : 01.원천데이터\\TS_1.판례_04.형사B(일반형).zip\n",
      "O : 성공 : 01.원천데이터\\TS_1.판례_01.민사.zip\n"
     ]
    }
   ],
   "source": [
    "raw_dir_list = [\"./3.개방데이터/1.데이터/Training/01.원천데이터\"]\n",
    "\n",
    "if not os.path.exists(raw_dir_list[0]):\n",
    "    os.makedirs(raw_dir_list[0])\n",
    "\n",
    "for file in os.listdir(raw_dir_list[0]):\n",
    "    if file.endswith(\".zip\"):\n",
    "        file_path = os.path.join(raw_dir_list[0], file)\n",
    "\n",
    "        try :\n",
    "            with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(extract_dir_list[2])\n",
    "                print(f\"O : 성공 : {file}\")\n",
    "        except zipfile.BadZipFile :\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            print(f\"X : 실패 (파일 깨짐): {file} (크기: {file_size} bytes)\")\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"기타 에러 ({file}): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f912885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터 셋 전처리\n",
    "\n",
    "import json, os\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_json_files(dir) :\n",
    "    loaded_data = []\n",
    "    for filename in os.listdir(dir) :\n",
    "        if filename.endswith('.json') :\n",
    "            with open(os.path.join(dir, filename), 'r', encoding='utf-8') as f :\n",
    "                loaded_data.append(json.load(f))\n",
    "    \n",
    "    return loaded_data\n",
    "\n",
    "def create_dataset(data) :\n",
    "    dataset_dict = {\n",
    "        \"id\": [],\n",
    "        \"question\": [],\n",
    "        \"context\": [],\n",
    "        \"summary\": [],\n",
    "        \"answer\" : [],\n",
    "    }\n",
    "\n",
    "    for item in data:\n",
    "        dataset_dict[\"id\"].append(item[\"info\"][\"id\"])\n",
    "        dataset_dict[\"question\"].append(item[\"jdgmnInfo\"][0][\"question\"])\n",
    "        dataset_dict[\"context\"].append(item[\"Summary\"][0][\"summ_contxt\"])\n",
    "        dataset_dict[\"summary\"].append(item[\"Summary\"][0][\"summ_pass\"])\n",
    "        dataset_dict[\"answer\"].append(item[\"jdgmnInfo\"][0][\"answer\"])\n",
    "\n",
    "    return Dataset.from_dict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f9ff410",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_dir_list = [\"./3.개방데이터/1.데이터/extract/Training/02.라벨링데이터\", \"./3.개방데이터/1.데이터/extract/Validation/02.라벨링데이터\", \"./3.개방데이터/1.데이터/raw/Training/0.1.원천데이터\"]\n",
    "train_data, val_data = load_json_files(extract_dir_list[0]), load_json_files(extract_dir_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "498262bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 || 400 || 200\n"
     ]
    }
   ],
   "source": [
    "# 일부 데이터 셋으로 테스트\n",
    "train_dataset = create_dataset(train_data)\n",
    "val_dataset = create_dataset(val_data)\n",
    "\n",
    "seed = 42\n",
    "\n",
    "train_subset = train_dataset.shuffle(seed=seed).select(range(1000))\n",
    "val_subset = val_dataset.shuffle(seed=seed).select(range(400))\n",
    "test_subset = val_dataset.shuffle(seed=seed).select(range(401, 601))\n",
    "\n",
    "print(f\"{len(train_subset)} || {len(val_subset)} || {len(test_subset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9623ef42",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_subset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_subset\u001b[49m[\u001b[32m0\u001b[39m]))\n",
      "\u001b[31mNameError\u001b[39m: name 'train_subset' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(train_subset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "074a3fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 학습 시킬 모델 불러오기\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig, \n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import Dataset\n",
    "import os, torch, json, wandb, subprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig, \n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b232b1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 양자화 설정\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # 4bit 할 것이냐\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, #bfloat16 or float16\n",
    "    bnb_4bit_quant_type=\"nf4\", # nf4 or fp4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a33ff38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vsc/LLM_TUNE/myenv/lib/python3.12/site-packages/torch/cuda/__init__.py:435: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  queued_call()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8620817d4e94ab9a2485c2a22d39c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExaoneForCausalLM(\n",
      "  (transformer): ExaoneModel(\n",
      "    (wte): Embedding(102400, 4096, padding_idx=0)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-31): 32 x ExaoneBlock(\n",
      "        (ln_1): ExaoneRMSNorm()\n",
      "        (attn): ExaoneAttention(\n",
      "          (attention): ExaoneSdpaAttention(\n",
      "            (rotary): ExaoneRotaryEmbedding()\n",
      "            (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "            (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "            (out_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): ExaoneRMSNorm()\n",
      "        (mlp): ExaoneGatedMLP(\n",
      "          (c_fc_0): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (c_fc_1): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (c_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act): SiLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): ExaoneRMSNorm()\n",
      "    (rotary): ExaoneRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=102400, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 및 토크나이저 불러오기\n",
    "\n",
    "model_id = \"/home/vsc/LLM/model/EXAONE-3.5-7.8B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de923173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c_fc_0', 'c_proj', 'c_fc_1', 'k_proj', 'q_proj', 'v_proj', 'out_proj']\n"
     ]
    }
   ],
   "source": [
    "# LoRA를 붙힐 레이어의 명칭을 찾아주는 코드\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    " \n",
    "modules = find_all_linear_names(model)\n",
    "print(modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dabce7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=16, target_modules={'out_proj', 'v_proj', 'q_proj', 'k_proj'}, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))\n"
     ]
    }
   ],
   "source": [
    "# 어떤 부분을 학습하냐에 따라서도 결과 값이 달라짐\n",
    "modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]\n",
    "\n",
    "# 변경 가능한 파라미터들\n",
    "peft_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\", #CAUSAL_LM, FEATURE_EXTRACTION, QUESTION_ANS, SEQ_2_SEQ_LM, SEQ_CLS, TOKEN_CLS.\n",
    "    inference_mode=False, # 학습 중에는 False로 두어야 가중치 업데이트 가능\n",
    "    r=16, # r은 보통 2의 배수로 두는데, r이 클수록 학습 가능한 파라미터의 수가 더 많아짐\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=modules\n",
    ")\n",
    "\n",
    "print(peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce8e3fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 13,631,488 || all params: 7,832,080,384 || trainable%: 0.1740\n"
     ]
    }
   ],
   "source": [
    "# 전체 파라미터 중 일부만 업데이트 되는 것을 확인할 수 있음\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6942527f-b97e-4571-9fef-8fec269ad9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompts(examples):\n",
    "    prompt_list = []\n",
    "    system_message = \"당신은 법률 및 규정 관련 전문가입니다. 사용자의 질문에 대해 '긍정'및 '부정', '불명' 여부를 판단한 후, 그에 대한 구체적인 근거를 문맥(context)에서 추출해서 설명하세요.\"\n",
    "\n",
    "    for i in range(len(examples['question'])):\n",
    "        # 각 리스트에서 i번째 데이터를 추출합니다.\n",
    "        answer = str(examples[\"answer\"][i]).strip()\n",
    "        question = examples[\"question\"][i]\n",
    "        context = examples[\"context\"][i]\n",
    "        summary = examples[\"summary\"][i]\n",
    "\n",
    "        # 사용자님의 정답 생성 로직\n",
    "        if answer == \"긍정\":\n",
    "            target_answer = f\"네 그렇습니다! {summary}에 의하여 질문하신 내용은 옳습니다.\"\n",
    "        elif answer == \"부정\":\n",
    "            target_answer = f\"아니요, 그렇지 않습니다! {summary}에 의하면 상충되는 내용이 있으므로 질문하신 내용은 옳지 않습니다.\"\n",
    "        elif answer == \"불명\":\n",
    "            target_answer = f\"확실하지 않습니다만, {summary}에 적힌 내용을 근거로 판단해 볼 수 있을 것 같습니다.\"\n",
    "        else:\n",
    "            target_answer = f\"해당 사안에 대해서는 제공된 근거({summary})를 바탕으로 판단이 필요합니다.\"\n",
    "\n",
    "        # 채팅 템플릿 구성\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": f\"사용자의 질문인 {question}에 대해 {context}를 참조하여 3문단 이내로 답변하세요.\"},\n",
    "            {\"role\": \"assistant\", \"content\": target_answer}\n",
    "        ]\n",
    "\n",
    "        full_prompt = tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        \n",
    "        prompt_list.append(full_prompt)\n",
    "    \n",
    "    return prompt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ce8f6db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcfef8dd2e014982bbf1fd85910f93ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30db777cbfd94721bb54fb8325f70627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 토큰 길이: 1232\n",
      "평균 토큰 길이: 523.87\n",
      "95번째 백분위수 (P95): 823.0\n",
      "99번째 백분위수 (P99): 975.02\n"
     ]
    }
   ],
   "source": [
    "# max_length 결정 (GPU Util을 최적화 하기 위함)\n",
    "\n",
    "def generate_prompts_test(example) :\n",
    "    prompt_list = []\n",
    "    system_message = \"당신은 법률 및 규정 관련 전문가입니다. 사용자의 질문에 대해 '긍정' 및 '부정', 혹은 '불명' 여부를 판단한 후, 그에 대한 구체적인 근거를 문맥(context)에서 추출해서 설명하세요.\"\n",
    "\n",
    "    answer = str(example[\"answer\"]).strip()\n",
    "    question = example[\"question\"]\n",
    "    context = example[\"context\"]\n",
    "    summary = example[\"summary\"]\n",
    "\n",
    "    if answer == \"긍정\" :\n",
    "        target_answer = f\"네 그렇습니다! {summary}에 의하여 질문하신 내용은 옳습니다.\"\n",
    "    elif answer == \"부정\" :\n",
    "        target_answer = f\"아니요, 그렇지 않습니다! {summary}에 의하면 상충되는 내용이 있으므로 질문하신 내용은 옳지 않습니다.\"\n",
    "    elif answer == \"불명\" :\n",
    "        target_answer = f\"확실하지 않습니다만, {summary}에 적힌 내용을 근거로 판단해 볼 수 있을 것 같습니다.\"\n",
    "    else :\n",
    "        target_answer = f\"해당 사안에 대해서는 제공된 근거({summary})를 바탕으로 판단이 필요합니다.\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": f\"사용자의 질문인 {question}에 대해 {context}를 참조하여 3문단 이내로 답변하세요.\"},\n",
    "        {\"role\": \"assistant\", \"content\": target_answer}\n",
    "    ]\n",
    "\n",
    "    full_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    \n",
    "    return {\"text\": full_prompt}\n",
    "\n",
    "train_prompts, val_prompts = train_subset.map(generate_prompts_test), val_subset.map(generate_prompts_test)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "token_lengths = [len(tokenizer.encode(p[\"text\"])) for p in train_prompts] + [len(tokenizer.encode(p[\"text\"])) for p in val_prompts]\n",
    "\n",
    "print(f\"최대 토큰 길이: {np.max(token_lengths)}\")\n",
    "print(f\"평균 토큰 길이: {np.mean(token_lengths):.2f}\")\n",
    "print(f\"95번째 백분위수 (P95): {np.percentile(token_lengths, 95)}\")\n",
    "print(f\"99번째 백분위수 (P99): {np.percentile(token_lengths, 99)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14e0cf8d-41c3-4d2b-a1f0-3df1374bc7eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.11209</td></tr><tr><td>eval/runtime</td><td>126.3876</td></tr><tr><td>eval/samples_per_second</td><td>3.165</td></tr><tr><td>eval/steps_per_second</td><td>1.582</td></tr><tr><td>total_flos</td><td>23215974990888960</td></tr><tr><td>train/epoch</td><td>0.992</td></tr><tr><td>train/global_step</td><td>62</td></tr><tr><td>train/grad_norm</td><td>0.69311</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>1.1654</td></tr><tr><td>+4</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2026-01-29 01:23:44</strong> at: <a href='https://wandb.ai/uailab-unist_/PromptTuning/runs/legal_testv2' target=\"_blank\">https://wandb.ai/uailab-unist_/PromptTuning/runs/legal_testv2</a><br> View project at: <a href='https://wandb.ai/uailab-unist_/PromptTuning' target=\"_blank\">https://wandb.ai/uailab-unist_/PromptTuning</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260129_012345-legal_testv2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vsc/LLM_TUNE/115.법률-규정 텍스트 분석 데이터_고도화_상황에 따른 판례 데이터/wandb/run-20260129_012352-legal_testv3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uailab-unist_/PromptTuning/runs/legal_testv3' target=\"_blank\">2026-01-29 01:23:51</a></strong> to <a href='https://wandb.ai/uailab-unist_/PromptTuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uailab-unist_/PromptTuning' target=\"_blank\">https://wandb.ai/uailab-unist_/PromptTuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uailab-unist_/PromptTuning/runs/legal_testv3' target=\"_blank\">https://wandb.ai/uailab-unist_/PromptTuning/runs/legal_testv3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/uailab-unist_/PromptTuning/runs/legal_testv3?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0xf9cf809d1f10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습을 기록할 wanDB notebook 설정\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "os.environ[\"WANDB_PROJECT\"] = \"PromptTuning\" # 프로젝트 이름\n",
    "os.environ[\"WANDB_RUN_ID\"] = \"legal_testv3\" # 노트북 고유 ID\n",
    "os.environ[\"WANDB_RESUME\"] = \"allow\" # 해당 노트북에서 학습을 이어서 진행할 것인지\n",
    "\n",
    "wandb.init(\n",
    "    project=os.environ[\"WANDB_PROJECT\"],\n",
    "    id=os.environ[\"WANDB_RUN_ID\"],\n",
    "    resume=os.environ[\"WANDB_RESUME\"],\n",
    "    name=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") # 대시보드에 표시될 이름\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14760be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55582b52b5f455b9a42145c83678f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fba5c1378594890abdb2c9f99b25611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 먼저 1 epoch로 훈련 -> 추론이 잘 이루어지는지 확인\n",
    "# 이후 epoch를 늘려 과적합이 일어날 때 까지 학습하는 것이 좋음\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datetime import datetime\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./SFT\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    max_seq_length=1024,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    load_best_model_at_end=True,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=11,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=5e-5,\n",
    "    group_by_length=True,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_subset,\n",
    "    eval_dataset=val_subset,\n",
    "    args=training_args,\n",
    "    formatting_func=generate_prompts,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e672dbac-5f70-4149-a708-4ba39536b45a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/vsc/LLM_TUNE/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1181: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='620' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [620/620 2:59:29, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.186900</td>\n",
       "      <td>1.046735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.962100</td>\n",
       "      <td>0.953926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.912000</td>\n",
       "      <td>0.929190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.913100</td>\n",
       "      <td>0.914811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.872100</td>\n",
       "      <td>0.906348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.842000</td>\n",
       "      <td>0.903418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.877800</td>\n",
       "      <td>0.901169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.798000</td>\n",
       "      <td>0.900246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.806900</td>\n",
       "      <td>0.899870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.791700</td>\n",
       "      <td>0.900180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.807700</td>\n",
       "      <td>0.901579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.761800</td>\n",
       "      <td>0.901505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "/home/vsc/LLM_TUNE/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1181: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/vsc/LLM_TUNE/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1181: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/vsc/LLM_TUNE/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1181: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/vsc/LLM_TUNE/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1181: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/vsc/LLM_TUNE/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1181: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/vsc/LLM_TUNE/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1181: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/vsc/LLM_TUNE/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1181: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/vsc/LLM_TUNE/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1181: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/vsc/LLM_TUNE/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1181: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/vsc/LLM_TUNE/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1181: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/vsc/LLM_TUNE/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1181: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/vsc/LLM_TUNE/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1181: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▄▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/runtime</td><td>▅▄▅▄▅▅▂▄▁▃▃█</td></tr><tr><td>eval/samples_per_second</td><td>▄▄▄▄▄▄▆▆█▆▆▁</td></tr><tr><td>eval/steps_per_second</td><td>██▁█▁██████▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▂▂▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>█▃▂▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▆▄▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.90151</td></tr><tr><td>eval/runtime</td><td>127.8985</td></tr><tr><td>eval/samples_per_second</td><td>3.127</td></tr><tr><td>eval/steps_per_second</td><td>1.564</td></tr><tr><td>total_flos</td><td>2.3204562474973594e+17</td></tr><tr><td>train/epoch</td><td>9.92</td></tr><tr><td>train/global_step</td><td>620</td></tr><tr><td>train/grad_norm</td><td>0.84141</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.794</td></tr><tr><td>+4</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2026-01-29 01:23:51</strong> at: <a href='https://wandb.ai/uailab-unist_/PromptTuning/runs/legal_testv3' target=\"_blank\">https://wandb.ai/uailab-unist_/PromptTuning/runs/legal_testv3</a><br> View project at: <a href='https://wandb.ai/uailab-unist_/PromptTuning' target=\"_blank\">https://wandb.ai/uailab-unist_/PromptTuning</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20260129_012352-legal_testv3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_save_path = \"./SFT/final\"\n",
    "trainer.train()\n",
    "# trainer.train(resume_from_checkpoint=True) # 이어서 학습을 진행하고 싶은 경우, 가장 마지막 checkpoint-XX를 불러와서 학습이 진행된다\n",
    "\n",
    "trainer.save_model(final_save_path)\n",
    "tokenizer.save_pretrained(final_save_path)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5aa6f9ed-12a8-46b9-891a-1d6d4f7566cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 추론된 모델 확인\n",
    "# 중요!!! 학습 이후 커널을 내리고 다시 올려야 파인튜닝된 가중치를 모델에 올릴 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f960ed74-2364-4466-9c56-9d699b5915ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig, \n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import Dataset\n",
    "import os, torch, json, wandb, subprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig, \n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f7ea6a2-5ef9-4d84-9766-fc7f33d9a647",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # 4bit 할 것이냐\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, #bfloat16 or float16\n",
    "    bnb_4bit_quant_type=\"nf4\", # nf4 or fp4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0950899f-4b1c-4ae8-b14e-f6235d6793d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vsc/LLM_TUNE/myenv/lib/python3.12/site-packages/torch/cuda/__init__.py:435: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  queued_call()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec0907f9a824c6aa65d1dc20581cf63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델 및 토크나이저 불러오기\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "model_id = \"/home/vsc/LLM/model/EXAONE-3.5-7.8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "672d9e5e-2a0e-4488-a811-02fdfef93271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): ExaoneForCausalLM(\n",
       "      (transformer): ExaoneModel(\n",
       "        (wte): Embedding(102400, 4096, padding_idx=0)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-31): 32 x ExaoneBlock(\n",
       "            (ln_1): ExaoneRMSNorm()\n",
       "            (attn): ExaoneAttention(\n",
       "              (attention): ExaoneSdpaAttention(\n",
       "                (rotary): ExaoneRotaryEmbedding()\n",
       "                (k_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (ln_2): ExaoneRMSNorm()\n",
       "            (mlp): ExaoneGatedMLP(\n",
       "              (c_fc_0): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (c_fc_1): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (c_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (act): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): ExaoneRMSNorm()\n",
       "        (rotary): ExaoneRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=102400, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapter_path = \"./SFT/final\"\n",
    "model = PeftModel.from_pretrained(model, adapter_path) # 학습된 LoRA Config를 씌운다\n",
    "model.eval() # 추론 모드로 모델을 바꾼다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcae29ae-d7a0-425b-83cd-6a6699f38c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 || 400 || 200\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_json_files(dir) :\n",
    "    loaded_data = []\n",
    "    for filename in os.listdir(dir) :\n",
    "        if filename.endswith('.json') :\n",
    "            with open(os.path.join(dir, filename), 'r', encoding='utf-8') as f :\n",
    "                loaded_data.append(json.load(f))\n",
    "    \n",
    "    return loaded_data\n",
    "\n",
    "def create_dataset(data) :\n",
    "    dataset_dict = {\n",
    "        \"id\": [],\n",
    "        \"question\": [],\n",
    "        \"context\": [],\n",
    "        \"summary\": [],\n",
    "        \"answer\" : [],\n",
    "    }\n",
    "\n",
    "    for item in data:\n",
    "        dataset_dict[\"id\"].append(item[\"info\"][\"id\"])\n",
    "        dataset_dict[\"question\"].append(item[\"jdgmnInfo\"][0][\"question\"])\n",
    "        dataset_dict[\"context\"].append(item[\"Summary\"][0][\"summ_contxt\"])\n",
    "        dataset_dict[\"summary\"].append(item[\"Summary\"][0][\"summ_pass\"])\n",
    "        dataset_dict[\"answer\"].append(item[\"jdgmnInfo\"][0][\"answer\"])\n",
    "\n",
    "    return Dataset.from_dict(dataset_dict)\n",
    "    \n",
    "extract_dir_list = [\"./3.개방데이터/1.데이터/extract/Training/02.라벨링데이터\", \"./3.개방데이터/1.데이터/extract/Validation/02.라벨링데이터\", \"./3.개방데이터/1.데이터/raw/Training/0.1.원천데이터\"]\n",
    "train_data, val_data = load_json_files(extract_dir_list[0]), load_json_files(extract_dir_list[1])\n",
    "\n",
    "train_dataset = create_dataset(train_data)\n",
    "val_dataset = create_dataset(val_data)\n",
    "\n",
    "seed = 42\n",
    "\n",
    "train_subset = train_dataset.shuffle(seed=seed).select(range(1000))\n",
    "val_subset = val_dataset.shuffle(seed=seed).select(range(400))\n",
    "test_subset = val_dataset.shuffle(seed=seed).select(range(401, 601))\n",
    "\n",
    "print(f\"{len(train_subset)} || {len(val_subset)} || {len(test_subset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4a166fd-4188-4c83-a838-92ef9b23d50c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 41036985, 'question': '피신청인들이 제출한 자료만으로, 이 사건 처분의 효력을 정지시킬 경우 위와 같이 공익에 중대한 해를 입힐 우려가 있다고 보기 부족하고, 더욱이 국정교과서 교육의 위헌·위법성 또는 이 사건 처분의 위법성이 추후 확인될 경우, 잘못된 국정교과서로 한국사를 배운 학생들 및 그들의 학부모가 침해당할 학습권, 자녀교육권과 비교형량하여 볼 때, 피신청인이 내세우는 공공의 복리란 것이 더 중대하다고도 볼 수 있는가?', 'context': '피신청인은 이 사건 처분이 이 사건 학교를 연구학교로 지정하여 이 사건 국정교과서의 교육효과를 연구·개발·검증하기 위한 것이고, 이 사건 학교는 전국에 유일한 이 사건 국정교과서의 연구학교로서 이 사건 처분의 효력이 정지되는 경우 국가의 교육정책에 막대한 장애가 발생하여 공공의 복리에 중대한 영향이 있다고 주장한다. 그러나 집행정지의 장애사유로서의 ‘공공복리에 대한 중대한 영향을 미칠 우려’란 그와 같은 일반적·추상적인 공익에 대한 침해의 우려가 아니라 당해 처분의 효력정지로 말미암아 구체적이고도 개별적으로 공익에 중대한 해를 입힐 우려가 높은 경우를 말하는바, 피신청인들이 제출한 자료만으로, 이 사건 처분의 효력을 정지시킬 경우 위와 같이 공익에 중대한 해를 입힐 우려가 있다고 보기 부족하다. 더욱이 국정교과서 교육의 위헌·위법성 또는 이 사건 처분의 위법성이 추후 확인될 경우, 잘못된 국정교과서로 한국사를 배운 학생들 및 그들의 학부모가 침해당할 학습권, 자녀교육권과 비교형량하여 보더라도 피신청인이 내세우는 공공의 복리란 것이 더 중대하다고도 볼 수 없다.', 'summary': '집행정지의 장애사유로서의 ‘공공복리에 대한 중대한 영향을 미칠 우려’란 그와 같은 일반적·추상적인 공익에 대한 침해의 우려가 아니라 당해 처분의 효력정지로 말미암아 구체적이고도 개별적으로 공익에 중대한 해를 입힐 우려가 높은 경우를 말하는바, 피신청인들이 제출한 자료만으로, 이 사건 처분의 효력을 정지시킬 경우 위와 같이 공익에 중대한 해를 입힐 우려가 있다고 보기 부족하다. 더욱이 국정교과서 교육의 위헌·위법성 또는 이 사건 처분의 위법성이 추후 확인될 경우, 잘못된 국정교과서로 한국사를 배운 학생들 및 그들의 학부모가 침해당할 학습권, 자녀교육권과 비교형량하여 보더라도 피신청인이 내세우는 공공의 복리란 것이 더 중대하다고도 볼 수 없다.', 'answer': '부정'}\n"
     ]
    }
   ],
   "source": [
    "print(test_subset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "791e331d-da49-4468-9196-7a39411f4e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_legal_answer(question) :\n",
    "    system_message = \"당신은 법률 및 규정 관련 전문가입니다. 사용자의 질문에 대해 '긍정' 및 '부정', 혹은 '불명' 여부를 판단한 후, 그에 대한 구체적인 근거를 문맥(context)에서 추출해서 설명하세요.\"\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": f\"사용자의 질문인 {question}에 대해 3문단 이내로 답변하세요.\"},\n",
    "        ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True, # 모델이 답변을 하도록 만드는 프롬프트\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad() : # 추론 단계에서는 가중치가 업데이트 되지 않도록\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=True,\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d48eefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 및 비교 시작... 총 200개 데이터\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████| 200/200 [18:02<00:00,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[최종 검증 결과]\n",
      "- 전체 샘플 수: 200\n",
      "- Label 정확도: 69.00%\n",
      "\n",
      "검증 완료! 로그 파일: ./result/comparison_log_without_context.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 유저가 Context를 주지 않을 경우\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from difflib import SequenceMatcher # 텍스트간 유사도 측정 라이브러리 (얼마나 비슷하게 생겼는가?)\n",
    "\n",
    "# 1. 결과 파일 및 통계 초기화\n",
    "output_file_path = \"./result/comparison_log_without_context.txt\"\n",
    "os.makedirs(\"./result\", exist_ok=True)\n",
    "\n",
    "correct_labels = 0\n",
    "total_samples = len(test_subset)\n",
    "results = []\n",
    "\n",
    "print(f\"검증 및 비교 시작... 총 {total_samples}개 데이터\")\n",
    "\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== EXAONE 법률 QA 정답 비교 검증 로그 ===\\n\\n\")\n",
    "\n",
    "    for i, data in enumerate(tqdm(test_subset, desc=\"Evaluating\")):\n",
    "        question = data[\"question\"]\n",
    "        context = data[\"context\"]\n",
    "        actual_label = str(data[\"answer\"]).strip() # 실제 정답 (긍정/부정/불명)\n",
    "        \n",
    "        # 실제 학습 데이터와 동일한 형식의 '정답 텍스트' 생성 (비교용)\n",
    "        summary = data[\"summary\"]\n",
    "        if actual_label == \"긍정\":\n",
    "            ground_truth = f\"네 그렇습니다! {summary}에 의하여 질문하신 내용은 옳습니다.\"\n",
    "        elif actual_label == \"부정\":\n",
    "            ground_truth = f\"아니요, 그렇지 않습니다! {summary}에 의하면 상충되는 내용이 있으므로 질문하신 내용은 옳지 않습니다.\"\n",
    "        else:\n",
    "            ground_truth = f\"확실하지 않습니다만, {summary}에 적힌 내용을 근거로 판단해 볼 수 있을 것 같습니다.\"\n",
    "\n",
    "        # 모델 답변 생성\n",
    "        generated_answer = generate_legal_answer(question)\n",
    "\n",
    "        # 2. 결론(Label) 일치 여부 확인\n",
    "        # 모델 답변의 앞부분에 실제 라벨 키워드가 포함되어 있는지 체크\n",
    "        is_label_correct = False\n",
    "        if (actual_label == \"긍정\" and \"네\" in generated_answer[:10]) or \\\n",
    "           (actual_label == \"부정\" and \"아니요\" in generated_answer[:10]) or \\\n",
    "           (actual_label == \"불명\" and \"확실하지\" in generated_answer[:10]):\n",
    "            is_label_correct = True\n",
    "            correct_labels += 1\n",
    "\n",
    "        # 3. 텍스트 유사도 계산 (0~1 사이)\n",
    "        similarity = SequenceMatcher(None, ground_truth, generated_answer).ratio()\n",
    "\n",
    "        # 4. 로그 기록\n",
    "        f.write(f\"[{i+1}번 데이터] | Label 일치: {'O' if is_label_correct else 'X'} | 유사도: {similarity:.2f}\\n\")\n",
    "        f.write(f\"질문: {question}\\n\")\n",
    "        f.write(f\"실제 정답: {ground_truth}\\n\")\n",
    "        f.write(f\"모델 답변: {generated_answer}\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "    # 5. 최종 통계 기록\n",
    "    accuracy = (correct_labels / total_samples) * 100\n",
    "    summary_msg = f\"\\n[최종 검증 결과]\\n- 전체 샘플 수: {total_samples}\\n- Label 정확도: {accuracy:.2f}%\\n\"\n",
    "    print(summary_msg)\n",
    "    f.write(summary_msg)\n",
    "\n",
    "print(f\"검증 완료! 로그 파일: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84135781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_legal_answer_with_context(question, context) :\n",
    "    system_message = \"당신은 법률 및 규정 관련 전문가입니다. 사용자의 질문에 대해 '긍정' 및 '부정', 혹은 '불명' 여부를 판단한 후, 그에 대한 구체적인 근거를 문맥(context)에서 추출해서 설명하세요.\"\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": f\"사용자의 질문인 {question}에 대해 {context}를 참조하여 3문단 이내로 답변하세요.\"},\n",
    "        ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True, # 모델이 답변을 하도록 만드는 프롬프트\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad() : # 추론 단계에서는 가중치가 업데이트 되지 않도록\n",
    "        outputs = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=True,\n",
    "            temperature=0.1,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab16fdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 및 비교 시작... 총 200개 데이터\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  28%|███████████▊                              | 56/200 [05:15<11:01,  4.59s/it]"
     ]
    }
   ],
   "source": [
    "# 유저가 Context를 줄 경우\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from difflib import SequenceMatcher # 텍스트간 유사도 측정 라이브러리 (단순 문자열의 나열)\n",
    "\n",
    "# 1. 결과 파일 및 통계 초기화\n",
    "output_file_path = \"./result/comparison_log_with_context.txt\"\n",
    "os.makedirs(\"./result\", exist_ok=True)\n",
    "\n",
    "correct_labels = 0\n",
    "total_samples = len(test_subset)\n",
    "results = []\n",
    "\n",
    "print(f\"검증 및 비교 시작... 총 {total_samples}개 데이터\")\n",
    "\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== EXAONE 법률 QA 정답 비교 검증 로그 ===\\n\\n\")\n",
    "\n",
    "    for i, data in enumerate(tqdm(test_subset, desc=\"Evaluating\")):\n",
    "        question = data[\"question\"]\n",
    "        context = data[\"context\"]\n",
    "        actual_label = str(data[\"answer\"]).strip() # 실제 정답 (긍정/부정/불명)\n",
    "        \n",
    "        # 실제 학습 데이터와 동일한 형식의 '정답 텍스트' 생성 (비교용)\n",
    "        summary = data[\"summary\"]\n",
    "        if actual_label == \"긍정\":\n",
    "            ground_truth = f\"네 그렇습니다! {summary}에 의하여 질문하신 내용은 옳습니다.\"\n",
    "        elif actual_label == \"부정\":\n",
    "            ground_truth = f\"아니요, 그렇지 않습니다! {summary}에 의하면 상충되는 내용이 있으므로 질문하신 내용은 옳지 않습니다.\"\n",
    "        else:\n",
    "            ground_truth = f\"확실하지 않습니다만, {summary}에 적힌 내용을 근거로 판단해 볼 수 있을 것 같습니다.\"\n",
    "\n",
    "        # 모델 답변 생성\n",
    "        generated_answer = generate_legal_answer_with_context(question, context)\n",
    "\n",
    "        # 2. 결론(Label) 일치 여부 확인\n",
    "        # 모델 답변의 앞부분에 실제 라벨 키워드가 포함되어 있는지 체크\n",
    "        is_label_correct = False\n",
    "        if (actual_label == \"긍정\" and \"네\" in generated_answer[:10]) or \\\n",
    "           (actual_label == \"부정\" and \"아니요\" in generated_answer[:10]) or \\\n",
    "           (actual_label == \"불명\" and \"확실하지\" in generated_answer[:10]):\n",
    "            is_label_correct = True\n",
    "            correct_labels += 1\n",
    "\n",
    "        # 3. 텍스트 유사도 계산 (0~1 사이)\n",
    "        similarity = SequenceMatcher(None, ground_truth, generated_answer).ratio()\n",
    "\n",
    "        # 4. 로그 기록\n",
    "        f.write(f\"[{i+1}번 데이터] | Label 일치: {'O' if is_label_correct else 'X'} | 유사도: {similarity:.2f}\\n\")\n",
    "        f.write(f\"질문: {question}\\n\")\n",
    "        f.write(f\"실제 정답: {ground_truth}\\n\")\n",
    "        f.write(f\"모델 답변: {generated_answer}\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "    # 5. 최종 통계 기록\n",
    "    accuracy = (correct_labels / total_samples) * 100\n",
    "    summary_msg = f\"\\n[최종 검증 결과]\\n- 전체 샘플 수: {total_samples}\\n- Label 정확도: {accuracy:.2f}%\\n\"\n",
    "    print(summary_msg)\n",
    "    f.write(summary_msg)\n",
    "\n",
    "print(f\"검증 완료! 로그 파일: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646cd82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유저가 Context를 주지 않을 경우 + ROGUE Metrics\n",
    "\n",
    "import json\n",
    "import os\n",
    "import evaluate # HuggingFace의 평가 라이브러리\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. ROUGE 메트릭 로드\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "# 2. 결과 파일 및 통계 초기화\n",
    "output_file_path = \"./result/comparison_log_without_context_rouge.txt\"\n",
    "os.makedirs(\"./result\", exist_ok=True)\n",
    "\n",
    "correct_labels = 0\n",
    "total_samples = len(test_subset)\n",
    "all_preds = []\n",
    "all_refs = []\n",
    "individual_scores = []\n",
    "\n",
    "print(f\"검증 시작 (Zero-Context)... 총 {total_samples}개 데이터\")\n",
    "\n",
    "with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=== EXAONE 법률 QA 정답 비교 검증 로그 (ROUGE-L) ===\\n\\n\")\n",
    "\n",
    "    for i, data in enumerate(tqdm(test_subset, desc=\"Evaluating\")):\n",
    "        question = data[\"question\"]\n",
    "        actual_label = str(data[\"answer\"]).strip()\n",
    "        summary = data[\"summary\"]\n",
    "        \n",
    "        # [정답 생성] \n",
    "        if actual_label == \"긍정\":\n",
    "            ground_truth = f\"네 그렇습니다! {summary}에 의하여 질문하신 내용은 옳습니다.\"\n",
    "        elif actual_label == \"부정\":\n",
    "            ground_truth = f\"아니요, 그렇지 않습니다! {summary}에 의하면 상충되는 내용이 있으므로 질문하신 내용은 옳지 않습니다.\"\n",
    "        else:\n",
    "            ground_truth = f\"확실하지 않습니다만, {summary}에 적힌 내용을 근거로 판단해 볼 수 있을 것 같습니다.\"\n",
    "\n",
    "        # [모델 답변 생성] (Context 없이 질문만 투입)\n",
    "        generated_answer = generate_legal_answer(question)\n",
    "\n",
    "        # 3. Label 일치 여부 확인 (핵심 판단력)\n",
    "        is_label_correct = False\n",
    "        if (actual_label == \"긍정\" and \"네\" in generated_answer[:10]) or \\\n",
    "           (actual_label == \"부정\" and \"아니요\" in generated_answer[:10]) or \\\n",
    "           (actual_label == \"불명\" and \"확실하지\" in generated_answer[:10]):\n",
    "            is_label_correct = True\n",
    "            correct_labels += 1\n",
    "\n",
    "        # 4. 개별 ROUGE-L 계산\n",
    "        score = rouge_metric.compute(predictions=[generated_answer], \n",
    "                                     references=[ground_truth], \n",
    "                                     use_aggregator=False)\n",
    "        rouge_l = score['rougeL'][0]\n",
    "        \n",
    "        all_preds.append(generated_answer)\n",
    "        all_refs.append(ground_truth)\n",
    "        individual_scores.append(rouge_l) \n",
    "\n",
    "        # 5. 로그 기록\n",
    "        f.write(f\"[{i+1}번 데이터] | Label 일치: {'O' if is_label_correct else 'X'} | ROUGE-L: {rouge_l:.4f}\\n\")\n",
    "        f.write(f\"질문: {question}\\n\")\n",
    "        f.write(f\"실제 정답: {ground_truth}\\n\")\n",
    "        f.write(f\"모델 답변: {generated_answer}\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "\n",
    "    # 6. 최종 통계 산출\n",
    "    final_accuracy = (correct_labels / total_samples) * 100\n",
    "    avg_rouge_l = np.mean(individual_scores)\n",
    "    \n",
    "    summary_msg = (\n",
    "        f\"\\n\" + \"=\"*50 + \"\\n\"\n",
    "        f\"[최종 검증 리포트 - Zero-Context]\\n\"\n",
    "        f\"- 전체 샘플 수: {total_samples}\\n\"\n",
    "        f\"- Label 정확도 (맞냐 틀리냐): {final_accuracy:.2f}%\\n\"\n",
    "        f\"- 평균 ROUGE-L 점수: {avg_rouge_l:.4f}\\n\"\n",
    "        f\"=\"*50 + \"\\n\"\n",
    "    )\n",
    "    print(summary_msg)\n",
    "    f.write(summary_msg)\n",
    "\n",
    "print(f\"검증 완료! 로그 파일: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e59cc6-443a-4bd1-a4c2-ac87051f835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a910c6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace에 모델 load / unload"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QA",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
