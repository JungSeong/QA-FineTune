import os
import sys
import torch
import json
from transformers import AutoModelForCausalLM, AutoTokenizer
from typing import List, Dict

current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

from logger_config import get_infer_logger
logger = get_infer_logger()

class HeavyModelInference:
    def __init__(self, model_path: str):
        self.model_path = model_path
        
        # 1. í† í¬ë‚˜ì´ì € ë¡œë“œ ë° í•™ìŠµ ì‹œ ì„¤ì • ì´ì‹
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        
        # [ğŸŒŸ ìš”ì²­ ì‚¬í•­ ë°˜ì˜ 1] íŒ¨ë”© í† í° ì„¤ì •
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            logger.info(f"âœ… Pad token set to EOS: {self.tokenizer.pad_token}")

        # 2. ëª¨ë¸ ë¡œë“œ (Accelerate device_map="auto" ì‚¬ìš©)
        logger.info(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì‹œì‘: {model_path} (BF16)")
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,    # 32B ì •ë°€ë„ ìœ ì§€
            device_map="auto",             # ğŸŒŸ ì—¬ëŸ¬ GPUì— ìë™ ë¶„ì‚° (Accelerate í•µì‹¬)
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )

        # [ğŸŒŸ ìš”ì²­ ì‚¬í•­ ë°˜ì˜ 2] ì„ë² ë”© ë ˆì´ì–´ ëª…ì‹œì  ì—°ê²° (EXAONE êµ¬ì¡° ëŒ€ì‘)
        if hasattr(self.model, "transformer") and hasattr(self.model.transformer, "wte"):
            self.model.get_input_embeddings = lambda: self.model.transformer.wte
            logger.info("âœ… Input embeddings mapping applied (transformer.wte).")

        self.model.eval()
        self._debug_tokens()

    def _debug_tokens(self):
        """Suspicious ID 361 ê²€ì¦ì„ ìœ„í•œ ë””ë²„ê¹… ì½”ë“œ"""
        tokens_to_check = ["[|endofturn|]", "[|assistant|]", "[|user|]", "[|system|]"]
        logger.info("--- [Token ID Verification] ---")
        for t in tokens_to_check:
            tid = self.tokenizer.convert_tokens_to_ids(t)
            logger.info(f"Token: {t:15} | ID: {tid}")
        logger.info(f"EOS ID: {self.tokenizer.eos_token_id} | PAD ID: {self.tokenizer.pad_token_id}")
        logger.info("-------------------------------")

    @torch.no_grad()
    def generate_text(self, messages: List[Dict[str, str]], max_tokens=512):
        # 1. Chat Template ì ìš©
        prompt = self.tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        
        # 2. í† í°í™” ë° ì¥ì¹˜ ì´ë™
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        
        # 3. ìƒì„± ì„¤ì • (vLLM íŒŒë¼ë¯¸í„°ì™€ 1:1 ëŒ€ì‘)
        generation_params = {
            "input_ids": inputs["input_ids"],
            "attention_mask": inputs["attention_mask"],
            "max_new_tokens": max_tokens,
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 50,
            "repetition_penalty": 1.2,     # ğŸŒŸ ë£¨í”„ ë°©ì§€ ê°•í™”
            "do_sample": True,
            "pad_token_id": self.tokenizer.pad_token_id,
            "eos_token_id": self.tokenizer.convert_tokens_to_ids("[|endofturn|]")
        }

        # 4. ì¶”ë¡  ì‹¤í–‰
        outputs = self.model.generate(**generation_params)
        
        # 5. í›„ì²˜ë¦¬ (Prompt ë¶€ë¶„ ì œì™¸í•˜ê³  Decode)
        new_ids = outputs[0][inputs["input_ids"].shape[-1]:]
        return self.tokenizer.decode(new_ids, skip_special_tokens=True)

# ì‹¤í–‰ ì˜ˆì‹œ
if __name__ == "__main__":
    MODEL_PATH = "/home/vsc/LLM/model/Exaone-3.5-32B-Instruct"
    infer = HeavyModelInference(MODEL_PATH)
    
    sample_msgs = [
        {"role": "system", "content": "ë„ˆëŠ” ë„ì„œê´€ ì•ˆë‚´ ì „ë¬¸ê°€ì•¼."},
        {"role": "user", "content": "ëŒ€í™”ë„ì„œê´€ íœ´ê´€ì¼ì´ ì–¸ì œì•¼?"}
    ]
    
    result = infer.generate_text(sample_msgs)
    print(f"\n[Final Output]\n{result}")