import os
import sys
import torch
import wandb
import json
import weave
import pandas as pd
from tqdm import tqdm
from datasets import Dataset
from accelerate import Accelerator
from prompts import *

current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

from logger_config import get_infer_logger

logger = get_infer_logger()

class DataAugmentor :
    def __init__(self, model, tokenizer, config) :
        self.model = model
        self.tokenizer = tokenizer
        self.config = config
        self.accelerator = Accelerator()
        if self.tokenizer.pad_token is None :
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
        self.wandb_table = wandb.Table(columns=["faq_id", "question", "answer", "label"])
        self.labels = ["yes", "no", "info", "false"]
    def _parse_output(self, raw_output):
        """JSON íŒŒì‹± ë¡œì§"""
        try:
            if "```json" in raw_output:
                json_str = raw_output.split("```json")[1].split("```")[0].strip()
            elif "```" in raw_output:
                json_str = raw_output.split("```")[1].split("```")[0].strip()
            else:
                json_str = raw_output.strip()
            return json.loads(json_str)
        except Exception:
            return []

    def preprocess_data(self, file_path="../data/json/augmented_data.jsonl") :
        if not self.accelerator.is_main_process :
            return None
        if not os.path.exists(file_path) :
            logger.error(f"âš ï¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
            return None
        
        logger.info("ğŸ” ë°ì´í„° ë¶ˆê· í˜• ë¶„ì„ ë° ì •ì œ ì‹œì‘...")
        try :
            df = pd.read_json(file_path, lines=True)
            initial_count = len(df)
            logger.info(f"ğŸ“‚ ì´ˆê¸° ë°ì´í„° ìˆ˜: {initial_count}")

            required_cols = ["faq_id","question", "answer", "label"]
            
            # 1. í•„ìˆ˜ ì¹¼ëŸ¼ì´ ì—†ì„ ê²½ìš°, ìš°ì„  í•„ìˆ˜ ì»¬ëŸ¼ì„ Noneìœ¼ë¡œë¼ë„ ì±„ì›€
            for col in required_cols :
                if col not in df.columns :
                    df[col] = None

            # 2. ê²°ì¸¡ì¹˜ ì œê±°
            nan_rows = df[df[required_cols].isna().any(axis=1)]
            if not nan_rows.empty:
                logger.warning(f"âš ï¸ ê²°ì¸¡ì¹˜ë¡œ ì œê±°ë˜ëŠ” ë°ì´í„° (ì´ {len(nan_rows)}ê±´):")
                # ë¡œê·¸ì— ì „ì²´ í–‰ ë‚´ìš© ì¶œë ¥ (ìµœëŒ€ 20ê±´)
                logger.warning(nan_rows.head(20).to_json(orient='records', force_ascii=False, indent=2))
                if len(nan_rows) > 20:
                    logger.warning(f"...ì™¸ {len(nan_rows)-20}ê±´ ìƒëµ")

            df_clean = df.dropna(subset=required_cols)

            # 3. ë¹ˆ ë¬¸ìì—´("") ë° ê³µë°±ë§Œ ìˆëŠ” ë°ì´í„° ì œê±°
            for col in required_cols :
                df_clean[col] = df_clean[col].astype(str).str.strip()
                empty_rows = df_clean[df_clean[col] == ""]
                if not empty_rows.empty:
                    logger.warning(f"âš ï¸ '{col}' ì»¬ëŸ¼ì´ ë¹„ì–´ìˆì–´ ì œê±°ë˜ëŠ” ë°ì´í„° (ì´ {len(empty_rows)}ê±´):")
                    logger.warning(empty_rows.head(20).to_json(orient='records', force_ascii=False, indent=2))
                df_clean = df_clean[df_clean[col] != ""]

            # 4. ì¤‘ë³µ ë°ì´í„° ì œê±°
            duplicates = df_clean[df_clean.duplicated(subset=required_cols, keep='first')]
            if not duplicates.empty:
                logger.warning(f"âš ï¸ ì¤‘ë³µë˜ì–´ ì œê±°ë˜ëŠ” ë°ì´í„° (ì´ {len(duplicates)}ê±´):")
                logger.warning(duplicates.head(20).to_json(orient='records', force_ascii=False, indent=2))

            df_clean = df_clean.drop_duplicates(subset=required_cols, keep="first")

            # 5. ìœ íš¨í•˜ì§€ ì•Šì€ ë¼ë²¨ ì œê±°
            allowed_labels = self.labels
            invalid_label_rows = df_clean[~df_clean['label'].isin(allowed_labels)]
            
            if not invalid_label_rows.empty:
                logger.warning(f"âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ë¼ë²¨(Label)ë¡œ ì œê±°ë˜ëŠ” ë°ì´í„° (ì´ {len(invalid_label_rows)}ê±´):")
                logger.warning(f"ğŸ‘‰ í—ˆìš©ëœ ë¼ë²¨: {allowed_labels}")
                logger.warning(f"ğŸ‘‰ ë°œê²¬ëœ ì´ìƒ ë¼ë²¨ ì˜ˆì‹œ: {invalid_label_rows['label'].unique().tolist()[:10]}") # ì–´ë–¤ ì´ìƒí•œ ê°’ì´ ìˆëŠ”ì§€ í™•ì¸ìš©
                logger.warning(invalid_label_rows.head(20).to_json(orient='records', force_ascii=False, indent=2))
                
                if len(invalid_label_rows) > 20:
                    logger.warning(f"...ì™¸ {len(invalid_label_rows)-20}ê±´ ìƒëµ")

            # 6. ì¸ë±ìŠ¤ ì´ˆê¸°í™”
            df_clean = df_clean.reset_index(drop=True)

            final_count = len(df_clean)
            dropped_count = initial_count - final_count

            if dropped_count > 0:
                logger.warning(f"ğŸ§¹ ì •ì œ ì™„ë£Œ: {dropped_count}ê°œ ë¶ˆëŸ‰ ë°ì´í„° ì œê±°ë¨ (ìµœì¢…: {final_count}ê°œ)")
                df_clean.to_json(file_path, orient='records', lines=True, force_ascii=False)
            else:
                logger.info("âœ¨ ë°ì´í„°ê°€ ì´ë¯¸ ê¹¨ë—í•©ë‹ˆë‹¤.")

            return df_clean

        except Exception as e :
            logger.error(f"âš ï¸ ë°ì´í„° ë¶ˆê· í˜• ë¶„ì„ ë° ì •ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def find_imbalanced_tasks(self, original_dataset, file_path="../data/json/augmented_data.jsonl"):
        # 1. Main process runs preprocessing (cleaning and saving to file)
        if self.accelerator.is_main_process:
            cleaned_data = self.preprocess_data(file_path)
        
        # Wait for main process to finish writing
        self.accelerator.wait_for_everyone()

        # 2. All processes read the data to calculate counts consistently
        counts = pd.DataFrame()

        if cleaned_data is not None :
            try :
                df = cleaned_data
                if not df.empty and 'faq_id' in df.columns and 'label' in df.columns:
                    counts = df.groupby(['faq_id', 'label']).size().unstack(fill_value=0)
                    # Only main process prints the table to avoid clutter
                    if self.accelerator.is_main_process:
                        logger.info(f"ğŸ“Š í˜„ì¬ ë°ì´í„° ë¶„í¬:\n{counts}")
            except Exception as e :
                logger.error(f"âš ï¸ ë°ì´í„°ë¥¼ ì½ì–´ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")

        missing_tasks = []
        required_labels = self.labels

        for idx, row in enumerate(original_dataset) :
            faq_context = row["DES"]
            needed_labels = []

            if idx in counts.index :
                current_counts = counts.loc[idx]
                for label in self.labels:
                    if label not in current_counts or current_counts[label] < 0:
                        needed_labels.append(label)
            else:
                # ë°ì´í„°ê°€ í•˜ë‚˜ë„ ì—†ëŠ” ê²½ìš° (ì‚­ì œë¨ or ëˆ„ë½ë¨) -> ì „ë¶€ ìƒì„±
                needed_labels = self.labels

            if needed_labels:
                missing_tasks.append({
                    "idx": idx,              # ì›ë³¸ ë°ì´í„° ì¸ë±ìŠ¤
                    "context": faq_context,  # ì›ë³¸ í…ìŠ¤íŠ¸
                    "targets": needed_labels,# í•„ìš”í•œ ë¼ë²¨ë“¤
                    "meta_title": row.get('TITLE', ''),
                    "meta_des": row.get('DES', '')
                })
        
        logger.info(f"missing_tasks: {missing_tasks}")
        return missing_tasks

    def _run_model_generate(self, messages, num_return_sequences) :
        model_inputs = self.tokenizer.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_tensors="pt"
        ).to(self.accelerator.device)
        
        input_ids = model_inputs['input_ids']
        attention_mask = input_ids.ne(self.tokenizer.pad_token_id).long()
        prompt_len = input_ids.shape[-1]

        with torch.no_grad() :
            outputs = self.model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                max_new_tokens=self.config.MAX_NEW_TOKENS,  
                do_sample=True,
                temperature=self.config.TEMPERATURE,
                top_p=self.config.TOP_P,
                top_k=self.config.TOP_K,
                repetition_penalty=self.config.REPETITION_PENALTY,
                num_return_sequences=self.config.NUM_RETURN_SEQUENCES,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        all_parsed_results = []

        for i in range(self.config.NUM_RETURN_SEQUENCES) :
            raw_text = self.tokenizer.decode(
                outputs[i][prompt_len:],
                skip_special_tokens=True # 
            )
            parsed_result = self._parse_output(raw_text)
            all_parsed_results.extend(parsed_result)
        
        return all_parsed_results

    @weave.op()
    def generate_samples(self, idx, faq_context) :
        user_content = LIBRARY_QA_USER_TEMPLATE.replace("{faq_content}", faq_context)
        messages = [
            {"role": "system", "content": LIBRARY_QA_SYSTEM_PROMPT_COT_FEW_SHOT},
            {"role": "user", "content": user_content}
        ]
        logger.info(messages)
        return self._run_model_generate(messages, self.config.NUM_RETURN_SEQUENCES)
    
    @weave.op()
    def generate_targeted_samples(self, idx, faq_context, target_labels) :
        target_labels_str = ", ".join(target_labels)
        all_guidelines = ALL_GUIDELINES
        selected_guidelines = "\n".join([all_guidelines[l] for l in target_labels if l in all_guidelines])
        selected_examples = ",\n      ".join([ALL_COT_FEW_SHOT_EXAMPLES[l] for l in target_labels if l in ALL_COT_FEW_SHOT_EXAMPLES])

        system_content = LIBRARY_QA_TARGETED_SYSTEM_PROMPT_COT_FEW_SHOT.replace("{target_labels}", target_labels_str).replace("{selected_guidelines}", selected_guidelines).replace("{selected_examples}", selected_examples)
        user_content = LIBRARY_QA_USER_TEMPLATE.replace("{faq_content}", faq_context)
        messages = [
            {"role": "system", "content": system_content},
            {"role": "user", "content": user_content}
        ]
        logger.info(messages)
        return self._run_model_generate(messages, self.config.NUM_RETURN_TARGET_SEQUENCES)

    def _run_generation_loop(self, items, output_file, mode="initial") :
        if not items :
            return
        desc = "Initial Gen" if mode == "initial" else "Targeted Gen"
        disable_tqdm = not self.accelerator.is_main_process

        base, ext = os.path.splitext(output_file)
        rank_output_file = f"{base}_rank{self.accelerator.process_index}{ext}"

        with open(rank_output_file, "a", encoding="utf-8") as f:
            for item in tqdm(items, total=len(items), desc=f"{desc} (Rank {self.accelerator.process_index})", disable=disable_tqdm):
                try :
                    if mode == "initial" :
                        idx, row = item
                        faq_context = row['DES']
                        result = self.generate_samples(idx, faq_context)
                        meta = {'faq_id': idx, 'original_title': row.get('TITLE', '')}
                    else: # targeted
                        idx = item['idx']
                        faq_context = item['context']
                        targets = item['targets']
                        meta = {'faq_id': idx, 'original_title': item['meta_title']}
                        result = self.generate_targeted_samples(idx, faq_context, targets)
                    
                    logger.info(result)
                    logger.info(meta)
                    # ì €ì¥
                    if result:
                        for res in result:
                            # ğŸŒŸ [í•µì‹¬] FAQ IDë¥¼ ë°ì´í„°ì— ì£¼ì… (ë‚˜ì¤‘ì— groupbyìš©)
                            res.update(meta)
                            f.write(json.dumps(res, ensure_ascii=False) + "\n")

                            # WandB ë¡œê¹… (Main Processë§Œ, ì²« ë²ˆì§¸ ìƒ˜í”Œë§Œ)
                            if self.accelerator.is_main_process and len(result) > 0 and result.index(res) == 0:
                                self.wandb_table.add_data(
                                    res.get('faq_id'), res.get('question'), res.get('answer'), res.get('label')
                                    )
                        f.flush()
                        os.fsync(f.fileno())

                except Exception as e:
                    import traceback
                    logger.error(f"ğŸ’¥ ì¹˜ëª…ì  ì—ëŸ¬ ë°œìƒ (idx: {idx if 'idx' in locals() else '?'}): {e}")
                    logger.error(traceback.format_exc()) # <-- ì´ê²Œ ë²”ì¸ì„ ì•Œë ¤ì¤ë‹ˆë‹¤.
                    
                    # ë””ë²„ê¹…ì„ ìœ„í•´ ì—¬ê¸°ì„œ ë©ˆì¶”ê²Œ í•˜ë ¤ë©´ raiseë¥¼ ì“°ì„¸ìš”
                    raise e

    def merge_rank_files(self, output_file) :
        if not self.accelerator.is_main_process:
            return

        logger.info("ğŸ“¦ ë¶„ì‚°ëœ ë°ì´í„° íŒŒì¼ ë³‘í•© ì¤‘...")
        base, ext = os.path.splitext(output_file)

        with open(output_file, "a", encoding="utf-8") as outfile:
            for rank in range(self.accelerator.num_processes):
                rank_file = f"{base}_rank{rank}{ext}"
                if os.path.exists(rank_file):
                    with open(rank_file, "r", encoding="utf-8") as infile:
                        # íŒŒì¼ ë‚´ìš© ë³µì‚¬
                        import shutil
                        shutil.copyfileobj(infile, outfile)
                    
                    try :
                        logger.info(f"ë³‘í•© í›„ ì¡°ê° íŒŒì¼ ì‚­ì œ (ì¤‘ë³µ ë³‘í•© ë°©ì§€): {rank_file}")
                        os.remove(rank_file) 
                    except OSError as e:
                        logger.error(f"ğŸ’¥ ì¹˜ëª…ì  ì—ëŸ¬ ë°œìƒ (idx: {idx if 'idx' in locals() else '?'}): {e}")
                        logger.error(traceback.format_exc()) # <-- ì´ê²Œ ë²”ì¸ì„ ì•Œë ¤ì¤ë‹ˆë‹¤.
                        
        logger.info(f"âœ… ë³‘í•© ì™„ë£Œ: {output_file}")
    
    def run_pipeline(self, dataset, output_file="../data/json/augmented_data.jsonl"):
        # --- 1. ì´ˆê¸° ìƒì„± (Initial Pass) ---
        if not os.path.exists(output_file) or os.path.getsize(output_file) == 0:
            if self.accelerator.is_main_process:
                logger.info("ğŸš€ [Step 1] ì´ˆê¸° ë°ì´í„° ìƒì„± ì‹œì‘...")
                with open(output_file, "w", encoding="utf-8") as f: pass
            
            # ë°ì´í„°ì…‹ì— ì¸ë±ìŠ¤ ë¶€ì—¬í•´ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (idx, row)
            indexed_dataset = list(enumerate(dataset))
            
            # ë©€í‹° GPU ë¶„ì‚° (Sharding)
            my_items = indexed_dataset[self.accelerator.process_index::self.accelerator.num_processes]
            self._run_generation_loop(my_items, output_file, mode="initial")
        else:
            logger.info("ğŸ“‚ ê¸°ì¡´ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ë°ì´í„°ë¥¼ ê²€ìˆ˜í•©ë‹ˆë‹¤.")

        self.accelerator.wait_for_everyone()
        self.merge_rank_files(output_file)

        # --- 2. ë°˜ë³µ ë³´ì™„ (Iterative Refinement) ---
        max_iter = getattr(self.config, 'MAX_AUG_ITERATIONS', 3)
        
        for i in range(1, max_iter + 1):
            self.accelerator.wait_for_everyone() # ë™ê¸°í™”
            
            # ì •ì œ ë° ë¶€ì¡±ë¶„ ë¶„ì„ (ë‚´ë¶€ì ìœ¼ë¡œ ë™ê¸°í™” í¬í•¨ë¨)
            missing_tasks = self.find_imbalanced_tasks(dataset, output_file)
            logger.info(missing_tasks)
            
            if not missing_tasks:
                if self.accelerator.is_main_process:
                    logger.info("âœ¨ ëª¨ë“  ë°ì´í„° ê· í˜• ì™„ë£Œ! ì¦ê°• ì¢…ë£Œ.")
                break
            
            if self.accelerator.is_main_process:
                logger.info(f"ğŸ“‰ [Step 2-{i}] ë¶€ì¡±í•œ ì‘ì—…: {len(missing_tasks)}ê±´. ì¶”ê°€ ìƒì„±...")

            # ë¶€ì¡±ë¶„ ë¶„ì‚° ì²˜ë¦¬ (Sharding)
            my_tasks = missing_tasks[self.accelerator.process_index::self.accelerator.num_processes]
            
            if len(my_tasks) > 0:
                self._run_generation_loop(my_tasks, output_file, mode="targeted")
            
            self.accelerator.wait_for_everyone()
            self.merge_rank_files(output_file)

        # ì™„ë£Œ ë¡œê·¸
        if self.accelerator.is_main_process:
            if wandb.run:
                wandb.log({"generated_qa_samples": self.wandb_table})
                logger.info("ğŸ“Š WandB ì—…ë¡œë“œ ì™„ë£Œ")

        return output_file