import os
import asyncio
import json
import sys
import pandas as pd
import aiofiles
import wandb
import weave
import re
import time
import numpy as np
from datetime import datetime
from tqdm.asyncio import tqdm
from transformers import AutoTokenizer
from mcp import ClientSession
from mcp.client.sse import sse_client
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from kiwipiepy import Kiwi
from prompts import *
from evaluator import LLMEvaluator

current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

from logger_config import get_infer_logger
logger = get_infer_logger()

class AsyncDataAugmentor:
    def __init__(self, mcp_url, model_id, config):
        self.mcp_url = mcp_url
        self.model_id = model_id
        self.config = config
        self.tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
        self.semaphore = asyncio.Semaphore(getattr(config, 'NUM_SEMAPHORES', 3))
        self.labels = config.LABELS
        self.write_queue = asyncio.Queue()
        self.output_file = getattr(config, 'AUGMENTED_DATA_PATH', "augmented_data.jsonl")
        self.kiwi = Kiwi() # í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”
        self.perf_stats = {
            "total_tokens": 0,
            "total_time": 0.0,
            "total_samples": 0
        }
        self.evaluator = LLMEvaluator(config.EVAL_MODEL_PATH, config.EVAL_LOG_PATH)

    # 1. ğŸŒŸ ê°•ë ¥í•´ì§„ JSON íŒŒì„œ (ì—¬ëŸ¬ ê°ì²´ ë° ë§ˆí¬ë‹¤ìš´ ì™„ë²½ ëŒ€ì‘)
    def _parse_output(self, raw_output):
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
        clean_text = re.sub(r'```json\s*|```', '', raw_output).strip()
        
        # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ { ... } íŒ¨í„´ì„ ëª¨ë‘ ì¶”ì¶œ (ê°€ì¥ í™•ì‹¤í•œ ë°©ë²•)
        json_pattern = re.compile(r'\{.*?\}', re.DOTALL)
        matches = json_pattern.findall(clean_text)
        
        results = []
        for match in matches:
            try:
                # ê°œí–‰ ë¬¸ì ë“±ìœ¼ë¡œ ê¹¨ì§„ JSON ìˆ˜ì • í›„ ë¡œë“œ
                item = json.loads(match.replace('\n', ' '))
                results.append(item)
            except json.JSONDecodeError:
                continue
        return results

    def calculate_gini(self, counts):
        """ì§€ë‹ˆ ê³„ìˆ˜ ê³„ì‚°: 0(ì™„ì „ ê· í˜•) ~ 1(ì™„ì „ ë¶ˆê· í˜•)"""
        counts = np.array(counts, dtype=np.float64)
        if np.sum(counts) == 0: return 0.0
        n = len(counts)
        sorted_counts = np.sort(counts)
        index = np.arange(1, n + 1)
        return (np.sum((2 * index - n - 1) * sorted_counts)) / (n * np.sum(sorted_counts))

    def calculate_self_bleu(self, df, sample_size=100):
        if len(df) < 2: return 0.0
        sample_texts = df['answer'].sample(min(len(df), sample_size)).tolist()
        
        tokenized = []
        for text in sample_texts:
            tokens = [t.form for t in self.kiwi.tokenize(text)]
            tokenized.append(tokens)
            
        scores = []
        smooth = SmoothingFunction().method1
        for i in range(len(tokenized)):
            ref = tokenized[:i] + tokenized[i+1:]
            hypo = tokenized[i]
            score = sentence_bleu(ref, hypo, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth)
            scores.append(score)
        return np.mean(scores) if scores else 0.0

    def calculate_metrics(self, df):
        """ë°ì´í„°í”„ë ˆì„ í•˜ë‚˜ë§Œ ë°›ì•„ì„œ ë‚´ë¶€ì—ì„œ ëª¨ë“  ì§€í‘œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        if len(df) == 0:
            return 0.0, 0.0
        # 1. ë¼ë²¨ ë¹ˆë„ìˆ˜ë¥¼ ë‚´ë¶€ì—ì„œ ì§ì ‘ ì¶”ì¶œ (reindexë¡œ ëª¨ë“  ë¼ë²¨ í¬í•¨)
        counts = df['label'].value_counts().reindex(self.labels, fill_value=0).tolist()
        # 2. ì§€ë‹ˆ ê³„ìˆ˜ ê³„ì‚°
        gini_val = self.calculate_gini(counts)
        # 3. Self-BLEU ê³„ì‚°
        self_bleu_val = self.calculate_self_bleu(df)
        
        return gini_val, self_bleu_val

    def find_imbalanced_tasks(self, file_path, original_dataset):
        logger.info(f"ğŸ“Š ë°ì´í„° í’ˆì§ˆ ë° ë¶„í¬ ë¶„ì„ ì‹œì‘: {file_path}")
        if not os.path.exists(file_path) or os.path.getsize(file_path) == 0:
            df = pd.DataFrame(columns=["faq_id", "label", "answer"])
        else:
            df = pd.read_json(file_path, lines=True)

        df = df.drop_duplicates(subset=["faq_id", "question", "answer", "label"])
        total_count = len(df)
        
        # 1. ê¸°ë³¸ ë¼ë²¨ í†µê³„
        label_stats = pd.DataFrame(index=self.labels)
        if total_count > 0:
            counts = df['label'].value_counts()
            label_stats['count'] = label_stats.index.map(lambda x: counts.get(x, 0))
            label_stats['percentage'] = (label_stats['count'] / total_count * 100).round(2)
        else:
            label_stats['count'], label_stats['percentage'] = 0, 0.0

        gini_val, self_bleu_val = self.calculate_metrics(df)

        # 3. ğŸ“ ë¡œê·¸ ì¶œë ¥ (ë” ìƒì„¸í•˜ê²Œ)
        logger.info(f"\n{'â•'*50}\n"
                    f"ğŸ“ˆ ë°ì´í„°ì…‹ ë¦¬í¬íŠ¸ (ì´ {total_count}ê°œ)\n"
                    f"{'-'*50}\n"
                    f"{label_stats.to_string()}\n"
                    f"{'-'*50}\n"
                    f"âš–ï¸ ë¶ˆê· í˜• ì§€ìˆ˜ (Gini): {gini_val:.3f} (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)\n"
                    f"ğŸ§© ë‹¤ì–‘ì„± ì§€ìˆ˜ (KoBLEU): {self_bleu_val:.3f} (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)\n"
                    f"{'â•'*50}")

        # 4. WandB ë¡œê¹…
        if wandb.run:
            wandb.log({
                "quality/gini": gini_val,
                "quality/self_bleu": self_bleu_val,
                "quality/total_count": total_count,
                **{f"dist/{l}": c for l, c in label_stats['count'].items()}
            })

        # 5. ë¶€ì¡±í•œ íƒœìŠ¤í¬ ì¶”ì¶œ ë¡œì§
        dist = df.groupby(['faq_id', 'label']).size().unstack(fill_value=0)
        for l in self.labels:
            if l not in dist.columns: dist[l] = 0

        missing_tasks = []
        for idx, row in enumerate(original_dataset):
            needed = [l for l in self.labels if idx not in dist.index or dist.loc[idx, l] < 1]
            if needed:
                missing_tasks.append({
                    "idx": idx,
                    "context": row.get("DES", ""),
                    "targets": needed,
                    "meta_title": row.get("TITLE", "")
                })
                
        # ë‚˜ì¤‘ì— ë…¸ì…˜ì— ê¸°ë¡í•  ë•Œ ì“¸ í†µê³„ ë°ì´í„°ë¥¼ í•¨ê»˜ ë°˜í™˜í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤.
        return missing_tasks, {"gini": gini_val, "self_bleu": self_bleu_val, "count": total_count}

    async def _save_worker(self):
        logger.info(f"ğŸ’¾ ì‹¤ì‹œê°„ ì €ì¥ ì›Œì»¤ ì‹œì‘: {self.output_file}")
        async with aiofiles.open(self.output_file, mode='a', encoding='utf-8') as f:
            while True:
                result = await self.write_queue.get()
                if result is None: break
                
                items = result if isinstance(result, list) else [result]
                for item in items:
                    await f.write(json.dumps(item, ensure_ascii=False) + "\n")
                
                await f.flush()
                self.write_queue.task_done()

    @weave.op()
    async def _mcp_generate_single(self, session, messages, meta):
        async with self.semaphore:
            start_t = time.perf_counter()
            try:
                response = await session.call_tool("generate_text", {
                    "messages": messages,
                    "max_tokens": getattr(self.config, 'MAX_NEW_TOKENS', 1024),
                    "temperature": getattr(self.config, 'TEMPERATURE', 0.7),
                    "top_p": getattr(self.config, 'TOP_P', 0.95),
                    "top_k": getattr(self.config, 'TOP_K', 20)
                })
                raw_text = response.content[0].text
                end_t = time.perf_counter()

                duration = end_t - start_t
                parsed_results = self._parse_output(raw_text)

                logger.info(parsed_results)
                
                for res in parsed_results:
                    res.update(meta)
                
                if parsed_results:
                    for res in parsed_results:
                        token_count = len(self.tokenizer.encode(res.get('answer', '')))
                        self.perf_stats["total_tokens"] += token_count
                        self.perf_stats["total_samples"] += 1
                        res.update(meta)
                    
                    self.perf_stats["total_time"] += duration
                    await self.write_queue.put(parsed_results)
                return parsed_results
            except Exception as e:
                logger.warning(f"âš ï¸ ì¶”ë¡  ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
                return []

    def print_epoch_report(self, iteration_name, quality_stats, perf_stats):
        """ë§¤ íšŒì°¨ ì¢…ë£Œ ì‹œ ì¶œë ¥ë  í†µí•© ë¦¬í¬íŠ¸"""
        logger.info(f"\n{'#'*60}\n"
                    f"ğŸ“¢ [{iteration_name}] ë‹¨ê³„ ì™„ë£Œ ë¦¬í¬íŠ¸\n"
                    f"{'-'*60}\n"
                    f"ğŸ“Š [í’ˆì§ˆ] Gini ì§€ìˆ˜: {quality_stats['gini']:.3f} | KoBLEU: {quality_stats['self_bleu']:.3f}\n"
                    f"âš¡ [ì„±ëŠ¥] TPS: {perf_stats['tps']:.2f} | Sec/Sample: {perf_stats['sec_per_sample']:.2f}s\n"
                    f"ğŸ“ˆ [ëˆ„ì ] ì´ ìƒ˜í”Œ ìˆ˜: {quality_stats['count']}ê°œ\n"
                    f"{'#'*60}")
        
        # WandBì— ë‹¨ê³„ë³„ ê¸°ë¡
        if wandb.run:
            wandb.log({
                "epoch": iteration_name,
                "metrics/gini": quality_stats['gini'],
                "metrics/ko_bleu": quality_stats['self_bleu'],
                "perf/tps": perf_stats['tps'],
                "perf/sec_per_sample": perf_stats['sec_per_sample']
            })

    def get_final_report(self):
        tps = self.perf_stats["total_tokens"] / self.perf_stats["total_time"] if self.perf_stats["total_time"] > 0 else 0
        sec_per_sample = self.perf_stats["total_time"] / self.perf_stats["total_samples"] if self.perf_stats["total_samples"] > 0 else 0
        
        report = (
            f"\n{'='*50}\n"
            f"âš¡ ì‹¤ì‹œê°„ ìƒì„± ì„±ëŠ¥ ë¦¬í¬íŠ¸\n"
            f"{'-'*50}\n"
            f"ğŸš€ TPS (Tokens/Sec): {tps:.2f}\n"
            f"â±ï¸ Sec/Sample: {sec_per_sample:.2f}s\n"
            f"ğŸ”¢ ì´ ìƒì„± í† í°: {self.perf_stats['total_tokens']}\n"
            f"ğŸ“¦ ì´ ìƒì„± ìƒ˜í”Œ: {self.perf_stats['total_samples']}\n"
            f"{'='*50}"
        )
        logger.info(report)
        return {"tps": tps, "sec_per_sample": sec_per_sample}

    async def run_generation_batch(self, session, tasks, mode="initial"):
        async_tasks = []
        for item in tasks:
            if mode == "initial":
                idx, row = item
                faq_context = row['DES']
                meta = {'faq_id': idx, 'original_title': row.get('TITLE', ''), 'iteration': mode}
                user_content = LIBRARY_QA_USER_TEMPLATE.replace("{faq_content}", faq_context)
                messages = [
                    {"role": "system", "content": LIBRARY_QA_SYSTEM_PROMPT_NO_COT},
                    {"role": "user", "content": user_content}
                ]
            else:
                # ğŸŒŸ Targeted ëª¨ë“œ êµ¬ì¡° ìˆ˜ì • (TypeError ë°©ì§€)
                idx = item['idx']
                faq_context = item['context']
                target_labels = item['targets']
                meta = {'faq_id': idx, 'original_title': item['meta_title'], 'iteration': mode}

                logger.info(f"ë¶€ì¡±í•œ Task : {target_labels}")
                
                target_labels_str = ", ".join(target_labels)
                # í”„ë¡¬í”„íŠ¸ ì¡°ë¦½ ë¡œì§ ìƒëµ(ê¸°ì¡´ ìœ ì§€)
                system_content = LIBRARY_QA_TARGETED_SYSTEM_PROMPT_NO_COT.replace("{target_labels}", target_labels_str)
                user_content = LIBRARY_QA_USER_TEMPLATE.replace("{faq_content}", faq_context)
                messages = [
                    {"role": "system", "content": system_content},
                    {"role": "user", "content": user_content}
                ]
            
            async_tasks.append(self._mcp_generate_single(session, messages, meta))

        results = await tqdm.gather(*async_tasks, desc=f"ğŸš€ {mode} ì¦ê°• ì§„í–‰ ì¤‘")
        return [res for sublist in results for res in sublist]

    async def run_pipeline_async(self, dataset, output_file):
        async with sse_client(self.mcp_url) as (read, write):
            async with ClientSession(read, write) as session:
                await session.initialize()
                
                await session.call_tool("switch_model", {
                    "model_id": self.config.GEN_SERVER_MODEL_NAME,
                    "config": {"trust_remote_code": True, "gpu_memory_utilization": 0.7}
                })

                save_worker_task = asyncio.create_task(self._save_worker())

                try:
                    # --- [STEP 1] ì´ˆê¸° ìƒì„± (Epoch 0) ---
                    if not os.path.exists(output_file) or os.path.getsize(output_file) == 0:
                        logger.info("ğŸš€ [Initial] ì´ˆê¸° ë°ì´í„° ìƒì„± ì‹œì‘...")
                        await self.run_generation_batch(session, list(enumerate(dataset)), mode="initial")
                        
                        # ì´ˆê¸° ìƒì„± í›„ ë¦¬í¬íŠ¸
                        missing, q_stats = self.find_imbalanced_tasks(output_file, dataset)
                        p_stats = self.get_final_report() # í˜„ì¬ê¹Œì§€ì˜ ì„±ëŠ¥ ê³„ì‚°
                        self.print_epoch_report("Initial", q_stats, p_stats)
                    else:
                        # íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ (new_data í• ë‹¹ ì—ëŸ¬ ë°©ì§€)
                        new_data = []
                        async with aiofiles.open(output_file, mode='r', encoding='utf-8') as f:
                            async for line in f:
                                if line.strip(): new_data.append(json.loads(line))

                    logger.info("ğŸ§¹ [MCP] VRAM í™•ë³´ë¥¼ ìœ„í•´ ëª¨ë¸ ì–¸ë¡œë“œ ìš”ì²­...")
                    await session.call_tool("unload_model")
                    
                    self.evaluator.load()
                    try:
                        logger.info("âš–ï¸ [Eval] ë¡œì»¬ ëª¨ë¸ ê¸°ë°˜ í’ˆì§ˆ ê²€ì¦ ì‹¤í–‰")
                        # ë™ê¸° í•¨ìˆ˜ì´ë¯€ë¡œ to_threadë¡œ ì‹¤í–‰
                        validated_data = await asyncio.to_thread(
                            self.evaluator.evaluate_batch, 
                            new_data[:20], 
                            "ì§ˆë¬¸ê³¼ ë‹µë³€ì´ ë…¼ë¦¬ì ìœ¼ë¡œ ì¼ê´€ë˜ëŠ”ê°€?"
                        )
                        avg_score = np.mean([d['eval_score'] for d in validated_data])
                        logger.info(f"â­ [Initial] LLM Judge í‰ê·  ì ìˆ˜: {avg_score:.2f} / 5.0")
                    finally:
                        self.evaluator.unload()

                    # --- [STEP 2] ë°˜ë³µ ë³´ì™„ (Epoch 1, 2, ...) ---
                    max_iter = getattr(self.config, 'MAX_AUG_ITERATIONS', 2)
                    for i in range(1, max_iter + 1):
                        missing_tasks, _ = self.find_imbalanced_tasks(output_file, dataset)
                        if not missing_tasks:
                            logger.info("âœ… ëª¨ë“  ë¼ë²¨ì´ ê· í˜•ì„ ì´ë¤˜ìŠµë‹ˆë‹¤. ë³´ì™„ ì¢…ë£Œ.")
                            break
                        
                        await session.call_tool("switch_model", {
                            "model_id": self.config.GEN_SERVER_MODEL_NAME,
                            "config": {"trust_remote_code": True, "gpu_memory_utilization": 0.7}
                        })

                        logger.info(f"ğŸ“‰ [Iteration {i}] ë¶€ì¡±ë¶„ {len(missing_tasks)}ê±´ ë³´ì™„ ì‹œì‘...")
                        current_batch = await self.run_generation_batch(session, missing_tasks, mode="targeted")
                        
                        # ê° Iteration ì¢…ë£Œ í›„ ë¦¬í¬íŠ¸
                        _, q_stats = self.find_imbalanced_tasks(output_file, dataset)
                        p_stats = self.get_final_report()
                        self.print_epoch_report(f"Iteration {i}", q_stats, p_stats)

                        logger.info("ğŸ§¹ [MCP] VRAM í™•ë³´ë¥¼ ìœ„í•´ ëª¨ë¸ ì–¸ë¡œë“œ ìš”ì²­...")
                        await session.call_tool("unload_model")

                        self.evaluator.load()
                        try:
                            logger.info(f"âš–ï¸ [Eval_{i}] ë¡œì»¬ ëª¨ë¸ ê¸°ë°˜ í’ˆì§ˆ ê²€ì¦ ì‹¤í–‰")
                            # ë™ê¸° í•¨ìˆ˜ì´ë¯€ë¡œ to_threadë¡œ ì‹¤í–‰
                            validated_data = await asyncio.to_thread(
                                self.evaluator.evaluate_batch, 
                                current_batch[:20], 
                                "ì§ˆë¬¸ê³¼ ë‹µë³€ì´ ë…¼ë¦¬ì ìœ¼ë¡œ ì¼ê´€ë˜ëŠ”ê°€?"
                            )
                            avg_score = np.mean([d['eval_score'] for d in validated_data])
                            logger.info(f"â­ [Initial] LLM Judge í‰ê·  ì ìˆ˜: {avg_score:.2f} / 5.0")
                        finally:
                            self.evaluator.unload()

                finally:
                    await self.write_queue.put(None)
                    await save_worker_task

        return output_file

    if __name__ == "__main__":
        config = Config()
        augmentor = AsyncDataAugmentor(config.MCP_URL, config.GEN_HF_MODEL_ID, config)
        asyncio.run(augmentor.run_pipeline_async(dataset, f"{config.AUGMENTED_DATA_PATH}"))

    # def _run_model_generate(self, messages, num_return_sequences) :
    #     model_inputs = self.tokenizer.apply_chat_template(
    #         messages,
    #         tokenize=True,
    #         add_generation_prompt=True,
    #         return_tensors="pt"
    #     ).to(self.accelerator.device)
        
    #     input_ids = model_inputs['input_ids']
    #     attention_mask = input_ids.ne(self.tokenizer.pad_token_id).long()
    #     prompt_len = input_ids.shape[-1]

    #     with torch.no_grad() :
    #         outputs = self.model.generate(
    #             input_ids=input_ids,
    #             attention_mask=attention_mask,
    #             max_new_tokens=self.config.MAX_NEW_TOKENS,  
    #             do_sample=True,
    #             temperature=self.config.TEMPERATURE,
    #             top_p=self.config.TOP_P,
    #             top_k=self.config.TOP_K,
    #             repetition_penalty=self.config.REPETITION_PENALTY,
    #             num_return_sequences=self.config.NUM_RETURN_SEQUENCES,
    #             pad_token_id=self.tokenizer.pad_token_id,
    #             eos_token_id=self.tokenizer.eos_token_id
    #         )
        
    #     all_parsed_results = []

    #     for i in range(self.config.NUM_RETURN_SEQUENCES) :
    #         raw_text = self.tokenizer.decode(
    #             outputs[i][prompt_len:],
    #             skip_special_tokens=True # 
    #         )
    #         parsed_result = self._parse_output(raw_text)
    #         all_parsed_results.extend(parsed_result)
        
    #     return all_parsed_results

    # @weave.op()
    # def generate_samples(self, idx, faq_context) :
    #     user_content = LIBRARY_QA_USER_TEMPLATE.replace("{faq_content}", faq_context)
    #     messages = [
    #         {"role": "system", "content": LIBRARY_QA_SYSTEM_PROMPT_COT_FEW_SHOT},
    #         {"role": "user", "content": user_content}
    #     ]
    #     logger.info(messages)
    #     return self._run_model_generate(messages, self.config.NUM_RETURN_SEQUENCES)
    
    # @weave.op()
    # def generate_targeted_samples(self, idx, faq_context, target_labels) :
    #     target_labels_str = ", ".join(target_labels)
    #     all_guidelines = ALL_GUIDELINES
    #     selected_guidelines = "\n".join([all_guidelines[l] for l in target_labels if l in all_guidelines])
    #     selected_examples = ",\n      ".join([ALL_COT_FEW_SHOT_EXAMPLES[l] for l in target_labels if l in ALL_COT_FEW_SHOT_EXAMPLES])

    #     system_content = LIBRARY_QA_TARGETED_SYSTEM_PROMPT_COT_FEW_SHOT.replace("{target_labels}", target_labels_str).replace("{selected_guidelines}", selected_guidelines).replace("{selected_examples}", selected_examples)
    #     user_content = LIBRARY_QA_USER_TEMPLATE.replace("{faq_content}", faq_context)
    #     messages = [
    #         {"role": "system", "content": system_content},
    #         {"role": "user", "content": user_content}
    #     ]
    #     logger.info(messages)
    #     return self._run_model_generate(messages, self.config.NUM_RETURN_TARGET_SEQUENCES)

    # def _run_generation_loop(self, items, output_file, mode="initial") :
    #     if not items :
    #         return
    #     desc = "Initial Gen" if mode == "initial" else "Targeted Gen"
    #     disable_tqdm = not self.accelerator.is_main_process

    #     base, ext = os.path.splitext(output_file)
    #     rank_output_file = f"{base}_rank{self.accelerator.process_index}{ext}"

    #     with open(rank_output_file, "a", encoding="utf-8") as f:
    #         for item in tqdm(items, total=len(items), desc=f"{desc} (Rank {self.accelerator.process_index})", disable=disable_tqdm):
    #             try :
    #                 if mode == "initial" :
    #                     idx, row = item
    #                     faq_context = row['DES']
    #                     result = self.generate_samples(idx, faq_context)
    #                     meta = {'faq_id': idx, 'original_title': row.get('TITLE', '')}
    #                 else: # targeted
    #                     idx = item['idx']
    #                     faq_context = item['context']
    #                     targets = item['targets']
    #                     meta = {'faq_id': idx, 'original_title': item['meta_title']}
    #                     result = self.generate_targeted_samples(idx, faq_context, targets)
                    
    #                 logger.info(result)
    #                 logger.info(meta)
    #                 # ì €ì¥
    #                 if result:
    #                     for res in result:
    #                         # ğŸŒŸ [í•µì‹¬] FAQ IDë¥¼ ë°ì´í„°ì— ì£¼ì… (ë‚˜ì¤‘ì— groupbyìš©)
    #                         res.update(meta)
    #                         f.write(json.dumps(res, ensure_ascii=False) + "\n")

    #                         # WandB ë¡œê¹… (Main Processë§Œ, ì²« ë²ˆì§¸ ìƒ˜í”Œë§Œ)
    #                         if self.accelerator.is_main_process and len(result) > 0 and result.index(res) == 0:
    #                             self.wandb_table.add_data(
    #                                 res.get('faq_id'), res.get('question'), res.get('answer'), res.get('label')
    #                                 )
    #                     f.flush()
    #                     os.fsync(f.fileno())

    #             except Exception as e:
    #                 import traceback
    #                 logger.error(f"ğŸ’¥ ì¹˜ëª…ì  ì—ëŸ¬ ë°œìƒ (idx: {idx if 'idx' in locals() else '?'}): {e}")
    #                 logger.error(traceback.format_exc()) # <-- ì´ê²Œ ë²”ì¸ì„ ì•Œë ¤ì¤ë‹ˆë‹¤.
                    
    #                 # ë””ë²„ê¹…ì„ ìœ„í•´ ì—¬ê¸°ì„œ ë©ˆì¶”ê²Œ í•˜ë ¤ë©´ raiseë¥¼ ì“°ì„¸ìš”
    #                 raise e

    # def merge_rank_files(self, output_file) :
    #     if not self.accelerator.is_main_process:
    #         return

    #     logger.info("ğŸ“¦ ë¶„ì‚°ëœ ë°ì´í„° íŒŒì¼ ë³‘í•© ì¤‘...")
    #     base, ext = os.path.splitext(output_file)

    #     with open(output_file, "a", encoding="utf-8") as outfile:
    #         for rank in range(self.accelerator.num_processes):
    #             rank_file = f"{base}_rank{rank}{ext}"
    #             if os.path.exists(rank_file):
    #                 with open(rank_file, "r", encoding="utf-8") as infile:
    #                     # íŒŒì¼ ë‚´ìš© ë³µì‚¬
    #                     import shutil
    #                     shutil.copyfileobj(infile, outfile)
                    
    #                 try :
    #                     logger.info(f"ë³‘í•© í›„ ì¡°ê° íŒŒì¼ ì‚­ì œ (ì¤‘ë³µ ë³‘í•© ë°©ì§€): {rank_file}")
    #                     os.remove(rank_file) 
    #                 except OSError as e:
    #                     logger.error(f"ğŸ’¥ ì¹˜ëª…ì  ì—ëŸ¬ ë°œìƒ (idx: {idx if 'idx' in locals() else '?'}): {e}")
    #                     logger.error(traceback.format_exc()) # <-- ì´ê²Œ ë²”ì¸ì„ ì•Œë ¤ì¤ë‹ˆë‹¤.
                        
    #     logger.info(f"âœ… ë³‘í•© ì™„ë£Œ: {output_file}")
    
    # def run_pipeline(self, dataset, output_file="../data/json/augmented_data.jsonl"):
    #     # --- 1. ì´ˆê¸° ìƒì„± (Initial Pass) ---
    #     if not os.path.exists(output_file) or os.path.getsize(output_file) == 0:
    #         if self.accelerator.is_main_process:
    #             logger.info("ğŸš€ [Step 1] ì´ˆê¸° ë°ì´í„° ìƒì„± ì‹œì‘...")
    #             with open(output_file, "w", encoding="utf-8") as f: pass
            
    #         # ë°ì´í„°ì…‹ì— ì¸ë±ìŠ¤ ë¶€ì—¬í•´ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (idx, row)
    #         indexed_dataset = list(enumerate(dataset))
            
    #         # ë©€í‹° GPU ë¶„ì‚° (Sharding)
    #         my_items = indexed_dataset[self.accelerator.process_index::self.accelerator.num_processes]
    #         self._run_generation_loop(my_items, output_file, mode="initial")
    #     else:
    #         logger.info("ğŸ“‚ ê¸°ì¡´ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ë°ì´í„°ë¥¼ ê²€ìˆ˜í•©ë‹ˆë‹¤.")

    #     self.accelerator.wait_for_everyone()
    #     self.merge_rank_files(output_file)

    #     # --- 2. ë°˜ë³µ ë³´ì™„ (Iterative Refinement) ---
    #     max_iter = getattr(self.config, 'MAX_AUG_ITERATIONS', 3)
        
    #     for i in range(1, max_iter + 1):
    #         self.accelerator.wait_for_everyone() # ë™ê¸°í™”
            
    #         # ì •ì œ ë° ë¶€ì¡±ë¶„ ë¶„ì„ (ë‚´ë¶€ì ìœ¼ë¡œ ë™ê¸°í™” í¬í•¨ë¨)
    #         missing_tasks = self.find_imbalanced_tasks(dataset, output_file)
    #         logger.info(missing_tasks)
            
    #         if not missing_tasks:
    #             if self.accelerator.is_main_process:
    #                 logger.info("âœ¨ ëª¨ë“  ë°ì´í„° ê· í˜• ì™„ë£Œ! ì¦ê°• ì¢…ë£Œ.")
    #             break
            
    #         if self.accelerator.is_main_process:
    #             logger.info(f"ğŸ“‰ [Step 2-{i}] ë¶€ì¡±í•œ ì‘ì—…: {len(missing_tasks)}ê±´. ì¶”ê°€ ìƒì„±...")

    #         # ë¶€ì¡±ë¶„ ë¶„ì‚° ì²˜ë¦¬ (Sharding)
    #         my_tasks = missing_tasks[self.accelerator.process_index::self.accelerator.num_processes]
            
    #         if len(my_tasks) > 0:
    #             self._run_generation_loop(my_tasks, output_file, mode="targeted")
            
    #         self.accelerator.wait_for_everyone()
    #         self.merge_rank_files(output_file)

    #     # ì™„ë£Œ ë¡œê·¸
    #     if self.accelerator.is_main_process:
    #         if wandb.run:
    #             wandb.log({"generated_qa_samples": self.wandb_table})
    #             logger.info("ğŸ“Š WandB ì—…ë¡œë“œ ì™„ë£Œ")

    #     return output_file