[34m[1mwandb[0m: Initializing weave.
[36m[1mweave[0m: Logged in as Weights & Biases user: jungseonglian.
[36m[1mweave[0m: View Weave data at https://wandb.ai/uailab-unist_/library-qa-finetune/weave
2026-02-15 11:25:33 - INFO - [main.py:33] - Dataset({
    features: ['FAQ', 'TITLE', 'DES'],
    num_rows: 110
})
2026-02-15 11:25:33 - INFO - [model_utils.py:29] - ğŸ“‚ ë¡œì»¬ ëª¨ë¸ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤: /home/vsc/LLM/model/Exaone-3.5-2.4B-Instruct
2026-02-15 11:25:33 - INFO - [model_utils.py:32] - ğŸš€ ëª¨ë¸ ë¡œë”© ì¤‘...
`torch_dtype` is deprecated! Use `dtype` instead!
/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:435: UserWarning:
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)

  queued_call()
Traceback (most recent call last):
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/final/infer/main.py", line 49, in <module>
    main()
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/final/infer/main.py", line 35, in main
    model, tokenizer = load_or_download_model_tokenizer(config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/final/infer/model_utils.py", line 34, in load_or_download_model_tokenizer
    model = AutoModelForCausalLM.from_pretrained(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 367, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4044, in from_pretrained
    device_map = _get_device_map(model, device_map, max_memory, hf_quantizer)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/transformers/integrations/accelerate.py", line 359, in _get_device_map
    hf_quantizer.validate_environment(device_map=device_map)
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_4bit.py", line 72, in validate_environment
    raise ValueError(
ValueError: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details.
