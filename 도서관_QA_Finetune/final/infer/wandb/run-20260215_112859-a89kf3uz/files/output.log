[34m[1mwandb[0m: Initializing weave.
[36m[1mweave[0m: Logged in as Weights & Biases user: jungseonglian.
[36m[1mweave[0m: View Weave data at https://wandb.ai/uailab-unist_/library-qa-finetune/weave
2026-02-15 11:29:02 - INFO - [main.py:33] - Dataset({
    features: ['FAQ', 'TITLE', 'DES'],
    num_rows: 110
})
2026-02-15 11:29:02 - INFO - [model_utils.py:29] - ğŸ“‚ ë¡œì»¬ ëª¨ë¸ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤: /home/vsc/LLM/model/Exaone-3.5-2.4B-Instruct
2026-02-15 11:29:02 - INFO - [model_utils.py:32] - ğŸš€ ëª¨ë¸ ë¡œë”© ì¤‘...
`torch_dtype` is deprecated! Use `dtype` instead!
/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:435: UserWarning:
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)

  queued_call()
Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 272/272 [00:00<00:00, 614.60it/s, Materializing param=transformer.wte.weight]
ExaoneForCausalLM(
  (transformer): ExaoneModel(
    (wte): Embedding(102400, 2560, padding_idx=0)
    (drop): Dropout(p=0.0, inplace=False)
    (h): ModuleList(
      (0-29): 30 x ExaoneDecoderLayer(
        (ln_1): ExaoneRMSNorm((2560,), eps=1e-05)
        (attn): ExaoneAttentionBlock(
          (attention): ExaoneAttention(
            (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=False)
            (k_proj): Linear4bit(in_features=2560, out_features=640, bias=False)
            (v_proj): Linear4bit(in_features=2560, out_features=640, bias=False)
            (out_proj): Linear4bit(in_features=2560, out_features=2560, bias=False)
          )
        )
        (ln_2): ExaoneRMSNorm((2560,), eps=1e-05)
        (mlp): ExaoneMLP(
          (c_fc_0): Linear4bit(in_features=2560, out_features=7168, bias=False)
          (c_fc_1): Linear4bit(in_features=2560, out_features=7168, bias=False)
          (c_proj): Linear4bit(in_features=7168, out_features=2560, bias=False)
          (act): SiLUActivation()
        )
      )
    )
    (ln_f): ExaoneRMSNorm((2560,), eps=1e-05)
    (rotary): ExaoneRotaryEmbedding()
  )
  (lm_head): Linear(in_features=2560, out_features=102400, bias=False)
)
2026-02-15 11:29:18 - INFO - [data_augmentor.py:107] - ğŸš€ ë°ì´í„° ì¦ê°• ì‹œì‘
Data Augmentation:   0%|                                                                                                                                                                                                                                  | 0/110 [00:00<?, ?it/s][36m[1mweave[0m: ğŸ© https://wandb.ai/uailab-unist_/library-qa-finetune/r/call/019c5f21-8708-78bd-904b-982d1016c24d
íšŒì›ì¦ ëŒ€ë¦¬ë°œê¸‰ì€ ë§Œ14ì„¸ ë¯¸ë§Œ ì•„ë™, ë§Œ65ì„¸ ì´ìƒ ì–´ë¥´ì‹ , ì¥ì• ì¸, ì„ì‚°ë¶€ë§Œ ê°€ëŠ¥í•˜ë©° ì•„ë˜ êµ¬ë¹„ì„œë¥˜ë¥¼ ì§€ì°¸í•˜ì—¬ ëŒ€ë¦¬ì¸ì´ ë°©ë¬¸ ì‹œ ëŒ€ë¦¬ë°œê¸‰ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

Â· ëŒ€ìƒ : ë§Œ14ì„¸ ë¯¸ë§Œ ì•„ë™, ë§Œ65ì„¸ ì´ìƒ ì–´ë¥´ì‹ , ì¥ì• ì¸, ì„ì‚°ë¶€
Â· êµ¬ë¹„ì„œë¥˜
Â  - ê³µí†µ : â‘ ìœ„ì„ì ì‹ ë¶„ì¦, â‘¡í”¼ìœ„ì„ì(ëŒ€ë¦¬ì¸) ì‹ ë¶„ì¦
Â  - ì¥ì• ì¸ : ì¥ì• ì¸ ë³µì§€ì¹´ë“œ ë˜ëŠ” ì¥ì• ì¸ ì¦ëª…ì„œ
Â  - ì„ì‹ ë¶€ : ì‚°ëª¨ìˆ˜ì²© / ì‚°ëª¨ : ì£¼ë¯¼ë“±ë¡ë“±ë³¸(ì¶œì‚° í›„ 12ê°œì›”ê¹Œì§€)
Â· ë°©ë²• : í™ˆí˜ì´ì§€ íšŒì›ê°€ì… í›„ ìœ„ í•­ëª©ì˜ í•´ë‹¹í•˜ëŠ” êµ¬ë¹„ì„œë¥˜ë¥¼ ì§€ì°¸í•˜ì—¬ í”¼ìœ„ì„ì(ëŒ€ë¦¬ì¸)ì´ ë„ì„œê´€ ë°©ë¬¸
Type: <class 'transformers.tokenization_utils_base.BatchEncoding'>
2026-02-15 11:29:40 - INFO - [data_augmentor.py:101] - [{'question': 'ëŒ€ë¦¬ì¸ ì—†ì´ íšŒì›ì¦ ë°œê¸‰ì´ ê°€ëŠ¥í•œê°€ìš”?', 'answer': 'ëŒ€ë¦¬ì¸ ì—†ì´ ì¼ë°˜ì ì¸ íšŒì›ì¦ ë°œê¸‰ì€ ì œí•œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë°œê¸‰ ëŒ€ìƒì—ëŠ” ë§Œ14ì„¸ ë¯¸ë§Œ ì•„ë™, ë§Œ65ì„¸ ì´ìƒ ì–´ë¥´ì‹ , ì¥ì• ì¸, ì„ì‚°ë¶€ê°€ í¬í•¨ë˜ë©°, ì´ë“¤ ê°ê°ì— ë”°ë¼ ì§€ì •ëœ ì„œë¥˜ë¥¼ ì œì¶œí•´ì•¼ í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì¼ë°˜ì ì¸ ê²½ìš°ì—ëŠ” ëŒ€ë¦¬ì¸ ì—†ì´ ë°œê¸‰ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.', 'label': 'no'}, {'question': 'ì¥ì• ì¸ ë“±ë¡ì¦ ì—†ì´ë„ íšŒì›ì¦ ëŒ€ë¦¬ ë°œê¸‰ì´ ê°€ëŠ¥í•œê°€ìš”?', 'answer': 'ì¥ì• ì¸ ë“±ë¡ì¦ì´ë‚˜ ì¦ëª…ì„œ ì—†ì´ëŠ” íšŒì›ì¦ ëŒ€ë¦¬ ë°œê¸‰ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. ëŒ€ì‹  ì¥ì• ì¸ ë³µì§€ì¹´ë“œë‚˜ ì¥ì• ì¸ ì¦ëª…ì„œë¥¼ ì†Œì§€í•˜ê³  ìˆì–´ì•¼ í•©ë‹ˆë‹¤.', 'label': 'true'}, {'question': 'ì„ì‚°ë¶€ëŠ” ì–´ë–¤ ì¦ë¹™ì„œë¥˜ë¥¼ í†µí•´ ëŒ€ë¦¬ì¸ ì—†ì´ íšŒì›ì¦ì„ ë°œê¸‰ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?', 'answer': 'ì„ì‚°ë¶€ì˜ ê²½ìš° ì‚°ëª¨ìˆ˜ì²© ë˜ëŠ” ì¶œì‚° í›„ ì£¼ë¯¼ë“±ë¡ë“±ë³¸ì„ ì œì‹œí•˜ë©´ ëŒ€ë¦¬ì¸ ì—†ì´ íšŒì›ì¦ ë°œê¸‰ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. íŠ¹íˆ ì¶œì‚° í›„ ì¼ì • ê¸°ê°„ ë‚´ì— ë°œê¸‰í•´ì•¼ í•˜ëŠ”ë°, ì´ ê¸°ê°„ì€ ì•½ 1ë…„ ì´ë‚´ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.', 'label': 'info'}, {'question': 'ê³µí†µ ì„œë¥˜ ì™¸ì— ì¥ì• ì¸ì´ë‚˜ ì„ì‚°ë¶€ì—ê²Œ ì¶”ê°€ë¡œ ìš”êµ¬ë˜ëŠ” ì„œë¥˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”?', 'answer': 'ì¥ì• ì¸ ë“±ë¡ì¦ì´ë‚˜ ì¦ëª…ì„œê°€ ì¶”ê°€ë¡œ ìš”êµ¬ë˜ë©°, ì„ì‚°ë¶€ì˜ ê²½ìš° ì‚°ëª¨ìˆ˜ì²©ì´ë‚˜ ì¶œì‚° í›„ ì£¼ë¯¼ë“±ë¡ë“±ë³¸ì´ í•„ìš”í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ íŠ¹ìˆ˜í•œ ìƒí™©ì—ì„œëŠ” í•´ë‹¹ ì„œë¥˜ë“¤ì´ ë°œê¸‰ ë° ë“±ë¡ ê³¼ì •ì—ì„œ í•„ìˆ˜ì ìœ¼ë¡œ í™•ì¸ë©ë‹ˆë‹¤.', 'label': 'info'}, {'question': 'ëŒ€ë¦¬ì¸ìœ¼ë¡œì„œ ëˆ„êµ¬ë‚˜ íšŒì›ì¦ ë°œê¸‰ì´ ê°€ëŠ¥í•œê°€?', 'answer': 'ëŒ€ë¦¬ì¸ìœ¼ë¡œì„œì˜ íšŒì›ì¦ ë°œê¸‰ì€ ë§Œ14ì„¸ ë¯¸ë§Œ ì•„ë™ì´ë‚˜ ë§Œ65ì„¸ ì´ìƒ ì–´ë¥´ì‹ ë¿ ì•„ë‹ˆë¼ ì¥ì• ì¸ê³¼ ì„ì‚°ë¶€ë§Œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œëŠ” ë³¸ì¸ ë³¸ì¸ë§Œ ë°œê¸‰ë°›ì„ ìˆ˜ ìˆìœ¼ë©°, ëŒ€ë¦¬ì¸ ë°œê¸‰ì€ íŠ¹ë³„í•œ ìƒí™©ì—ì„œë§Œ í—ˆìš©ë©ë‹ˆë‹¤.', 'label': 'info'}, {'question': 'ëŒ€ë¦¬ì¸ ë°œê¸‰ì„ ìœ„í•´ í•„ìš”í•œ ì„œë¥˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”?', 'answer': 'ëŒ€ë¦¬ì¸ìœ¼ë¡œ íšŒì›ì¦ì„ ë°œê¸‰ë°›ê¸° ìœ„í•´ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì„œë¥˜ê°€ í•„ìš”í•©ë‹ˆë‹¤: ê³µí†µ ì„œë¥˜ë¡œëŠ” ë³¸ì¸ ì‹ ë¶„ì¦ê³¼ ëŒ€ë¦¬ì¸ ì‹ ë¶„ì¦ì´ í•„ìš”í•˜ë©°, ì¶”ê°€ë¡œ ì¥ì• ì¸ ë“±ë¡ì¦ ë˜ëŠ” ì„ì‚°ë¶€ì˜ ê²½ìš° ì‚°ëª¨ìˆ˜ì²©ì´ë‚˜ ì¶œì‚° í›„ 1ë…„ ì´ë‚´ ì£¼ë¯¼ë“±ë¡ë“±ë³¸ì´ í•„ìš”í•©ë‹ˆë‹¤.', 'label': 'info'}, {'question': 'ì¥ì• ì¸ ë“±ë¡ì¦ ì—†ì´ë„ ëŒ€ë¦¬ì¸ì´ íšŒì›ì¦ ë°œê¸‰ì„ ë„ì™€ì¤„ ìˆ˜ ìˆë‚˜ìš”?', 'answer': 'ì¼ë°˜ì ìœ¼ë¡œëŠ” ì¥ì• ì¸ ë“±ë¡ì¦ì´ í¬í•¨ëœ ì„œë¥˜ë§Œìœ¼ë¡œëŠ” ëŒ€ë¦¬ì¸ ë°œê¸‰ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¥ì• ì¸ ë“±ë¡ì¦ì´ë‚˜ ì¥ì• ì¸ ì¦ëª…ì„œê°€ í•„ìˆ˜ ì„œë¥˜ ì¤‘ í•˜ë‚˜ì´ê¸° ë•Œë¬¸ì—, ì´ë¥¼ ì¤€ë¹„í•˜ì§€ ì•Šì€ ê²½ìš°ì—ëŠ” ëŒ€ë¦¬ì¸ ë°œê¸‰ì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.', 'label': 'yes'}, {'question': 'ì„ì‚°ë¶€ì˜ ê²½ìš°ì—ë„ ì£¼ë¯¼ë“±ë¡ë“±ë³¸ì´ ë°œê¸‰ë°›ì•„ì•¼ í•˜ëŠ”ê°€ìš”?', 'answer': 'ì„ì‚°ë¶€ì˜ ê²½ìš°, ì£¼ë¯¼ë“±ë¡ë“±ë³¸ ëŒ€ì‹  ì‚°ëª¨ìˆ˜ì²©ì„ ì œì¶œí•´ì•¼ í•©ë‹ˆë‹¤. ì‚°ëª¨ìˆ˜ì²©ì€ ì„ì‹  ì‚¬ì‹¤ì„ ì¦ëª…í•˜ëŠ” ì¤‘ìš”í•œ ë¬¸ì„œì´ë¯€ë¡œ í•´ë‹¹ ì„œë¥˜ë¥¼ ì§€ì°¸í•˜ì—¬ ëŒ€ë¦¬ì¸ ë°œê¸‰ì„ ìš”ì²­í•´ì•¼ í•©ë‹ˆë‹¤.', 'label': 'no'}, {'question': 'ë§Œ14ì„¸ ë¯¸ë§Œ ì•„ë™ì˜ ëŒ€ë¦¬ì¸ì´ ë„ì„œê´€ì—ì„œ íšŒì›ì¦ì„ ë°œê¸‰ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?', 'answer': 'ë„¤, ë§ìŠµë‹ˆë‹¤. ë§Œ14ì„¸ ë¯¸ë§Œ ì•„ë™ì˜ ê²½ìš°ì—ëŠ” ë¶€ëª¨ë‹˜ì´ë‚˜ ë²•ì •ëŒ€ë¦¬ì¸ì˜ ë™ì˜ ì—†ì´ë„ ëŒ€ë¦¬ì¸ì„ í†µí•´ ë„ì„œê´€ íšŒì›ì¦ì„ ë°œê¸‰ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•„ìš”í•œ ì„œë¥˜ë¡œëŠ” 1ìœ„ì„ìì˜ ì‹ ë¶„ì¦ê³¼ í”¼ìœ„ì„ì(ëŒ€ë¦¬ì¸)ì˜ ì‹ ë¶„ì¦ì´ í•„ìš”í•˜ë©°, íŠ¹íˆ ì¥ì• ì¸ ë“±ë¡ì¦ì´ë‚˜ ì„ì‚°ë¶€ì˜ ê²½ìš° ì¶”ê°€ ì¦ë¹™ ì„œë¥˜ê°€ ìš”êµ¬ë  ìˆ˜ ìˆìœ¼ë‹ˆ í™•ì¸í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.', 'label': 'yes'}, {'question': 'ì¥ì• ì¸ ë“±ë¡ì¦ ì—†ì´ë„ ë§Œ14ì„¸ ë¯¸ë§Œ ì•„ë™ì˜ ëŒ€ë¦¬ì¸ì´ íšŒì›ì¦ì„ ë°œê¸‰ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?', 'answer': 'ì•„ë‹ˆìš”, ì¥ì• ì¸ ë“±ë¡ì¦ ì—†ì´ëŠ” ëŒ€ë¦¬ì¸ì´ ë„ì„œê´€ì—ì„œ íšŒì›ì¦ì„ ë°œê¸‰ë°›ì§€ ëª»í•©ë‹ˆë‹¤. ëŒ€ë¦¬ì¸ ë°œê¸‰ ì‹œì—ëŠ” ê³µí†µ ì„œë¥˜ì™€ í•¨ê»˜ ì¥ì• ì¸ ê´€ë ¨ ì¦ë¹™ ì„œë¥˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.', 'label': 'no'}, {'question': 'ì„ì‚°ë¶€ê°€ ë„ì„œê´€ íšŒì›ì¦ì„ ëŒ€ë¦¬ì¸ì„ í†µí•´ ë°œê¸‰ë°›ìœ¼ë ¤ë©´ ì–´ë–¤ ì„œë¥˜ê°€ í•„ìš”í•œê°€ìš”?', 'answer': 'ì„ì‚°ë¶€ëŠ” ì‚°ëª¨ìˆ˜ì²©ì„ ì œì¶œí•´ì•¼ í•©ë‹ˆë‹¤. ë˜í•œ ì¶œì‚° í›„ 12ê°œì›” ì´ë‚´ì—¬ì•¼ í•˜ë©°, ì‚°ëª¨ ë³¸ì¸ì˜ ì£¼ë¯¼ë“±ë¡ë“±ë³¸ë„ í•¨ê»˜ ì œì‹œí•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì„œë¥˜ë“¤ì´ ê°–ì¶”ì–´ì ¸ì•¼ ëŒ€ë¦¬ì¸ì„ í†µí•œ íšŒì›ì¦ ë°œê¸‰ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.', 'label': 'info'}, {'question': 'ë„ì„œê´€ íšŒì›ê°€ì… í›„ ëŒ€ë¦¬ì¸ì´ ë°©ë¬¸í•˜ì§€ ì•Šê³ ë„ íšŒì›ì¦ì„ ë°œê¸‰ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?', 'answer': 'í˜„ì¬ ì‹œìŠ¤í…œì—ì„œëŠ” ëŒ€ë¦¬ì¸ì´ ì§ì ‘ ë°©ë¬¸í•˜ì—¬ íšŒì›ì¦ì„ ë°œê¸‰ë°›ì•„ì•¼ í•©ë‹ˆë‹¤. ì˜¨ë¼ì¸ì„ í†µí•œ íšŒì›ê°€ì…ì€ ê¸°ë³¸ ì •ë³´ ì…ë ¥ë§Œ ê°€ëŠ¥í•˜ë©°, ì‹¤ì œ ë°œê¸‰ì€ ì˜¤í”„ë¼ì¸ ë°©ë¬¸ì„ í†µí•´ ì´ë£¨ì–´ì ¸ì•¼ í•©ë‹ˆë‹¤.', 'label': 'false'}]
Data Augmentation:   1%|â–ˆâ–‰                                                                                                                                                                                                                        | 1/110 [00:22<40:11, 22.12s/it][36m[1mweave[0m: ğŸ© https://wandb.ai/uailab-unist_/library-qa-finetune/r/call/019c5f21-dd70-7a4c-886b-7008efffecd5
ê°€ì¡±íšŒì›ì´ë€, ê³ ì–‘ì‹œ ë„ì„œê´€ ì •íšŒì›ìœ¼ë¡œ ê°€ì… í›„ ê°€ì¡±íšŒì›ìœ¼ë¡œ ë“±ë¡ ì‹œ ê°€ì¡±ëª…ì˜ë¡œ ë„ì„œëŒ€ì¶œÂ·ë„ì„œì˜ˆì•½Â·ìƒí˜¸ëŒ€ì°¨ ë™ì¼í•˜ê²Œ ì´ìš©ê°€ëŠ¥í•œ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤.
â€» ê°€ì¡±ëª…ì˜ ì´ìš© ì‹œ ìë£Œì‹¤ ì•ˆë‚´ë°ìŠ¤í¬ì— ë°˜ë“œì‹œ ë³¸ì¸ íšŒì›ì¦ì„ ì œì‹œí•˜ì—¬ì•¼ í•¨(ê°€ì¡± íšŒì›ì¦ìœ¼ë¡œ ëŒ€ì¶œ ë¶ˆê°€)
ê°€ì¡±íšŒì› ë“±ë¡ë°©ë²•
ê°€ì¡±êµ¬ì„±ì› ëª¨ë‘ ì •íšŒì› ê°€ì… í›„ ê°€ì¡±ì„ì„ ì¦ëª…í•  ìˆ˜ ìˆëŠ” ì„œë¥˜(ì£¼ë¯¼ë“±ë¡ë“±ë³¸ ë˜ëŠ” ê°€ì¡±ê´€ê³„ì¦ëª…ì„œ) ì§€ì°¸í•˜ì—¬ ë„ì„œê´€ ë°©ë¬¸
Type: <class 'transformers.tokenization_utils_base.BatchEncoding'>
2026-02-15 11:30:03 - INFO - [data_augmentor.py:101] - [{'question': 'ê°€ì¡±íšŒì›ìœ¼ë¡œ ë“±ë¡í•˜ë©´ ì–´ë–¤ íŠ¹ë³„í•œ í˜œíƒì´ ìˆë‚˜ìš”?', 'answer': 'ê°€ì¡±íšŒì›ìœ¼ë¡œ ë“±ë¡í•˜ì‹œë©´ ê°€ì¡± êµ¬ì„±ì› ëª¨ë‘ê°€ ë„ì„œê´€ì˜ ë‹¤ì–‘í•œ ì„œë¹„ìŠ¤ë¥¼ í•¨ê»˜ ì´ìš©í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë„ì„œ ëŒ€ì¶œ, ë„ì„œ ì˜ˆì•½, ê·¸ë¦¬ê³  ìƒí˜¸ëŒ€ì°¨ ì„œë¹„ìŠ¤ê¹Œì§€ ë™ì¼í•˜ê²Œ ì´ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ê°€ì¡±ëª…ì˜ë¡œ ë„ì„œë¥¼ ëŒ€ì¶œë°›ìœ¼ì‹¤ ë•ŒëŠ” ê°œì¸ ì‹ ë¶„ì¦(ê°€ì¡±íšŒì›ì¦)ì„ ì œì‹œí•´ì•¼ í•˜ë©°, ì¼ë°˜ ì •íšŒì›ì˜ ì‹ ë¶„ì¦ë§Œìœ¼ë¡œëŠ” ëŒ€ì¶œì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.', 'label': 'info'}, {'question': 'ê°€ì¡±êµ¬ì„±ì› ëª¨ë‘ê°€ ì •íšŒì›ìœ¼ë¡œ ê°€ì…ëœ í›„ ê°€ì¡±íšŒì›ìœ¼ë¡œ ë“±ë¡í•˜ê¸° ìœ„í•´ì„œëŠ” ì–´ë–¤ ì„œë¥˜ê°€ í•„ìš”í•œê°€ìš”?', 'answer': 'ê°€ì¡±íšŒì› ë“±ë¡ì„ ìœ„í•´ì„œëŠ” ëª¨ë“  ê°€ì¡± êµ¬ì„±ì›ì´ ì´ë¯¸ ë„ì„œê´€ ì •íšŒì›ìœ¼ë¡œ ê°€ì…ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. ê°€ì¡±ì„ì„ ì¦ëª…í•  ìˆ˜ ìˆëŠ” ì„œë¥˜ë¡œëŠ” ì£¼ë¯¼ë“±ë¡ë“±ë³¸ì´ë‚˜ ê°€ì¡±ê´€ê³„ì¦ëª…ì„œ ë“±ì´ ì í•©í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì„œë¥˜ë¥¼ ì§€ì°¸í•˜ì—¬ ë„ì„œê´€ì„ ë°©ë¬¸í•˜ì‹œë©´ ê°€ì¡±íšŒì› ë“±ë¡ì´ ì™„ë£Œë©ë‹ˆë‹¤.', 'label': 'info'}, {'question': 'ê°€ì¡±íšŒì›ì¦ ì—†ì´ ê°€ì¡±ëª…ì˜ë¡œ ë„ì„œë¥¼ ëŒ€ì¶œí•  ìˆ˜ ìˆëŠ” ê²½ìš°ê°€ ìˆë‚˜ìš”?', 'answer': 'ê°€ì¡±íšŒì›ì¦ ì—†ì´ ë„ì„œë¥¼ ëŒ€ì¶œí•˜ëŠ” ê²ƒì€ ì œí•œì ì…ë‹ˆë‹¤. ê°€ì¡±íšŒì›ìœ¼ë¡œ ë“±ë¡ ì‹œ ë°˜ë“œì‹œ ê°œì¸ ì‹ ë¶„ì¦ì„ ì œì‹œí•´ì•¼ í•˜ë©°, ì¼ë°˜ ì •íšŒì› ì‹ ë¶„ì¦ë§Œìœ¼ë¡œëŠ” ë„ì„œ ëŒ€ì¶œì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.', 'label': 'no'}, {'question': 'ê°€ì¡±íšŒì›ìœ¼ë¡œ ë“±ë¡í•˜ì§€ ì•Šìœ¼ë©´ ë„ì„œê´€ ì„œë¹„ìŠ¤ ì¤‘ ì¼ë¶€ë§Œ ì´ìš© ê°€ëŠ¥í•˜ë‚˜ìš”?', 'answer': 'ë„¤, ë§ìŠµë‹ˆë‹¤. ê°€ì¡±íšŒì›ìœ¼ë¡œ ë“±ë¡í•˜ì§€ ì•Šìœ¼ë©´ ëª¨ë“  ê°€ì¡± êµ¬ì„±ì›ë“¤ì´ ë™ë“±í•˜ê²Œ ë„ì„œ ëŒ€ì¶œ, ë„ì„œ ì˜ˆì•½, ìƒí˜¸ëŒ€ì°¨ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŠ¹íˆ ê°€ì¡±ëª…ì˜ë¡œ ë„ì„œ ëŒ€ì¶œ ì‹œì—ëŠ” ê°œì¸ ì‹ ë¶„ì¦ ì œì‹œê°€ í•„ìˆ˜ì ì…ë‹ˆë‹¤.', 'label': 'yes'}, {'question': 'ê°€ì¡±íšŒì›ìœ¼ë¡œ ë“±ë¡í•˜ë©´ ì–´ë–¤ í˜œíƒì´ ìˆë‚˜ìš”?', 'answer': 'ê°€ì¡±íšŒì›ìœ¼ë¡œ ë“±ë¡í•˜ì‹œë©´ ê°€ì¡± êµ¬ì„±ì› ëª¨ë‘ê°€ ë„ì„œê´€ì˜ ë‹¤ì–‘í•œ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë„ì„œ ëŒ€ì¶œ, ë„ì„œ ì˜ˆì•½, ê·¸ë¦¬ê³  ìƒí˜¸ëŒ€ì°¨ ì„œë¹„ìŠ¤ê¹Œì§€ ê°€ì¡± ë‹¨ìœ„ë¡œ í¸ë¦¬í•˜ê²Œ ì´ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ê°€ì¡±ëª…ì˜ë¡œ ë„ì„œë¥¼ ëŒ€ì¶œí•  ë•ŒëŠ” ë³¸ì¸ ì‹ ë¶„ì¦ì„ ì œì‹œí•´ì•¼ í•˜ë©°, ê¸°ì¡´ ì •íšŒì› ê°œì¸ ì‹ ë¶„ì¦ìœ¼ë¡œëŠ” ëŒ€ì¶œì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.', 'label': 'info'}, {'question': 'ê°€ì¡±íšŒì› ë“±ë¡ ì‹œ í•„ìš”í•œ ì„œë¥˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”?', 'answer': 'ê°€ì¡±íšŒì› ë“±ë¡ì„ ìœ„í•´ì„œëŠ” ê°€ì¡± êµ¬ì„±ì› ëª¨ë‘ê°€ ê³ ì–‘ì‹œ ë„ì„œê´€ì˜ ì •íšŒì›ìœ¼ë¡œ ê°€ì…í•œ í›„, ê°€ì¡±ì„ì„ ì¦ëª…í•  ìˆ˜ ìˆëŠ” ì„œë¥˜ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì£¼ë¯¼ë“±ë¡ë“±ë³¸ì´ë‚˜ ê°€ì¡±ê´€ê³„ì¦ëª…ì„œ ê°™ì€ ì¦ë¹™ì„œë¥˜ë¥¼ ì§€ì°¸í•˜ì—¬ ë„ì„œê´€ì„ ë°©ë¬¸í•˜ì‹œë©´ ë©ë‹ˆë‹¤.', 'label': 'info'}, {'question': 'ê°€ì¡±íšŒì› ë“±ë¡ í›„ì—ë„ ê¸°ì¡´ ì •íšŒì›ê³¼ í•¨ê»˜ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•  ìˆ˜ ìˆë‚˜ìš”?', 'answer': 'ê°€ì¡±íšŒì›ìœ¼ë¡œ ë“±ë¡ëœ ê²½ìš°, ê°€ì¡± êµ¬ì„±ì› ê°ê°ì€ ë…ë¦½ì ìœ¼ë¡œ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ê¸°ì¡´ ì •íšŒì›ì˜ ê³„ì • ì •ë³´ë¥¼ í†µí•´ ê°€ì¡±ëª…ì˜ë¡œ ì„œë¹„ìŠ¤ë¥¼ ê´€ë¦¬í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ë‹¨, ê°€ì¡±ëª…ì˜ë¡œ ë„ì„œë¥¼ ëŒ€ì¶œí•  ë•ŒëŠ” ë³¸ì¸ ì‹ ë¶„ì¦ì„ ì œì‹œí•´ì•¼ í•©ë‹ˆë‹¤.', 'label': 'info'}, {'question': 'ê°€ì¡±íšŒì› ë“±ë¡ ì‹œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì œí•œ ì‚¬í•­ì€ ë¬´ì—‡ì¸ê°€ìš”?', 'answer': 'ê°€ì¡±íšŒì›ìœ¼ë¡œ ë“±ë¡ëœ ê²½ìš°ì—ë„ ê¸°ì¡´ ì •íšŒì›ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ì—°ì²´ ê¸°ê°„ì´ ìˆëŠ” ë„ì„œëŠ” ëŒ€ì¶œì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. ë˜í•œ, ê°€ì¡±ëª…ì˜ë¡œ ë„ì„œë¥¼ ëŒ€ì¶œí•˜ë ¤ë©´ ë°˜ë“œì‹œ ë³¸ì¸ ì‹ ë¶„ì¦ì„ ì œì‹œí•´ì•¼ í•˜ë©°, ì •íšŒì› ê°œì¸ ì‹ ë¶„ì¦ìœ¼ë¡œëŠ” ëŒ€ì¶œì´ ì œí•œë©ë‹ˆë‹¤.', 'label': 'no'}, {'question': 'ê°€ì¡±íšŒì› ë“±ë¡ í›„ ê°œì¸ ëª…ì˜ë¡œ ë„ì„œ ëŒ€ì¶œì´ ê°€ëŠ¥í•©ë‹ˆê¹Œ?', 'answer': 'ê°€ì¡±íšŒì›ìœ¼ë¡œ ë“±ë¡ëœ ê²½ìš°ì—ëŠ” ê°œì¸ ëª…ì˜ë¡œ ë„ì„œëŒ€ì¶œì´ ê°€ëŠ¥í•˜ë‚˜, ê°€ì¡±íšŒì›ì¦ì„ í†µí•´ ì§ì ‘ ëŒ€ì¶œí•˜ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. ë„ì„œê´€ ìë£Œì‹¤ì— ë°©ë¬¸í•˜ì—¬ ë³¸ì¸ì˜ ì •íšŒì›ì¦ì„ ì œì‹œí•´ì•¼ í•©ë‹ˆë‹¤.', 'label': 'yes'}, {'question': 'ê°€ì¡±íšŒì› ìê²©ì„ ê°–ì¶”ê¸° ìœ„í•´ í•„ìš”í•œ ì£¼ìš” ì„œë¥˜ëŠ” ë¬´ì—‡ì…ë‹ˆê¹Œ?', 'answer': 'ê°€ì¡±íšŒì› ìê²©ì„ ì–»ê¸° ìœ„í•´ì„œëŠ” ëª¨ë“  ê°€ì¡± êµ¬ì„±ì›ì´ ë¨¼ì € ì •íšŒì›ìœ¼ë¡œ ê°€ì…í•´ì•¼ í•˜ë©°, ì´ë¥¼ ì¦ë¹™í•˜ê¸° ìœ„í•œ ì„œë¥˜ë¡œëŠ” ì£¼ë¯¼ë“±ë¡ë“±ë³¸ì´ë‚˜ ê°€ì¡±ê´€ê³„ì¦ëª…ì„œë¥¼ ì§€ì°¸í•˜ì—¬ ë„ì„œê´€ì„ ë°©ë¬¸í•´ì•¼ í•©ë‹ˆë‹¤.', 'label': 'info'}, {'question': 'ì •íšŒì›ìœ¼ë¡œ ë“±ë¡ëœ ê°€ì¡± êµ¬ì„±ì›ë“¤ì´ í•¨ê»˜ ë„ì„œë¥¼ ì˜ˆì•½í•  ìˆ˜ ìˆìŠµë‹ˆê¹Œ?', 'answer': 'ì •íšŒì›ìœ¼ë¡œ ë“±ë¡ëœ ê°€ì¡± êµ¬ì„±ì›ë“¤ì€ ê°ì ê°œë³„ì ìœ¼ë¡œ ë„ì„œ ì˜ˆì•½ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ê°€ì¡± ë‹¨ìœ„ë¡œ í†µí•©ì ì¸ ê´€ë¦¬ë‚˜ ê³µë™ ì˜ˆì•½ ì‹œìŠ¤í…œì€ í˜„ì¬ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.', 'label': 'yes'}, {'question': 'ê°€ì¡±íšŒì›ì¦ ì—†ì´ ê°€ì¡±ëª…ì˜ë¡œ ë„ì„œë¥¼ ëŒ€ì¶œë°›ëŠ” ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?', 'answer': 'ê°€ì¡±íšŒì›ì¦ ì—†ì´ ê°€ì¡±ëª…ì˜ë¡œ ë„ì„œë¥¼ ì§ì ‘ ëŒ€ì¶œë°›ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ê°€ì¡±íšŒì›ì¦ì„ í†µí•œ ê°„ì ‘ì ì¸ ì ‘ê·¼ì´ ê¶Œì¥ë©ë‹ˆë‹¤.', 'label': 'false'}]
Data Augmentation:   2%|â–ˆâ–ˆâ–ˆâ–‰                                                                                                                                                                                                                      | 2/110 [00:44<39:58, 22.21s/it][36m[1mweave[0m: ğŸ© https://wandb.ai/uailab-unist_/library-qa-finetune/r/call/019c5f22-3473-7153-93d3-3d4bfda467ad
ë³¸ì¸ëª…ì˜ íœ´ëŒ€í° ì¸ì¦ì´ ë¶ˆê°€í•œ ë§Œ14ì„¸ ë¯¸ë§Œ ì–´ë¦°ì´ì˜Â ê²½ìš° ì•„ì´í•€(I-PIN) ë³¸ì¸ì¸ì¦ ê³¼ì •ì„ ê±°ì³ í™ˆí˜ì´ì§€ ê°€ì…ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

í™ˆí˜ì´ì§€ íšŒì›ê°€ì… - ë§Œ14ì„¸ ë¯¸ë§Œ íšŒì›ê°€ì…(ì–´ë¦°ì´íšŒì› ê°€ì…í•˜ê¸°) - ë³´í˜¸ì íœ´ëŒ€í° ì¸ì¦ í›„ ê°€ì…ì(ì–´ë¦°ì´)ì˜ ì•„ì´í•€ì„ í†µí•´ í™ˆí˜ì´ì§€ì— ê°€ì…í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

íšŒì›ê°€ì… ë°”ë¡œê°€ê¸°
https://www.goyanglib.or.kr/center/program/memberJoinType.do
Type: <class 'transformers.tokenization_utils_base.BatchEncoding'>
Data Augmentation:   2%|â–ˆâ–ˆâ–ˆâ–‰                                                                                                                                                                                                                      | 2/110 [00:58<53:00, 29.45s/it]
Traceback (most recent call last):
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/final/infer/main.py", line 49, in <module>
    main()
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/final/infer/main.py", line 39, in main
    output_file = augmentor.run_augmentation(dataset)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/final/infer/data_augmentor.py", line 119, in run_augmentation
    result = self.generate_samples(idx, faq_context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/weave/trace/op.py", line 1278, in wrapper
    res, _ = _call_sync_func(
             ^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/weave/trace/op.py", line 543, in _call_sync_func
    res = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/final/infer/data_augmentor.py", line 75, in generate_samples
    outputs = self.model.generate(
              ^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 124, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/transformers/generation/utils.py", line 2638, in generate
    result = decoding_method(
             ^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/transformers/generation/utils.py", line 2843, in _sample
    outputs = model_forward(**model_inputs, return_dict=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/transformers/utils/generic.py", line 834, in wrapper
    output = func(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/.cache/huggingface/modules/transformers_modules/Exaone_hyphen_3_dot_5_hyphen_2_dot_4B_hyphen_Instruct/modeling_exaone.py", line 514, in forward
    outputs: BaseModelOutputWithPast = self.transformer(
                                       ^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/transformers/utils/generic.py", line 1001, in wrapper
    outputs = func(self, *args, **kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/.cache/huggingface/modules/transformers_modules/Exaone_hyphen_3_dot_5_hyphen_2_dot_4B_hyphen_Instruct/modeling_exaone.py", line 433, in forward
    hidden_states = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/transformers/modeling_layers.py", line 93, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/.cache/huggingface/modules/transformers_modules/Exaone_hyphen_3_dot_5_hyphen_2_dot_4B_hyphen_Instruct/modeling_exaone.py", line 281, in forward
    hidden_states = self.mlp(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/.cache/huggingface/modules/transformers_modules/Exaone_hyphen_3_dot_5_hyphen_2_dot_4B_hyphen_Instruct/modeling_exaone.py", line 238, in forward
    output_proj = self.c_proj(self.act(self.c_fc_0(x)) * self.c_fc_1(x))
                                                         ^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1776, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1787, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/bitsandbytes/nn/modules.py", line 557, in forward
    return bnb.matmul_4bit(x, weight, bias=bias, quant_state=quant_state).to(inp_dtype)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py", line 401, in matmul_4bit
    return MatMul4Bit.apply(A, B, out, bias, quant_state)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/torch/autograd/function.py", line 583, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py", line 315, in forward
    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/bitsandbytes/functional.py", line 984, in dequantize_4bit
    absmax = dequantize_blockwise(quant_state.absmax, quant_state.state2)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/bitsandbytes/functional.py", line 709, in dequantize_blockwise
    return torch.ops.bitsandbytes.dequantize_blockwise.default(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/torch/_ops.py", line 819, in __call__
    return self._op(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/torch/_compile.py", line 54, in inner
    return disable_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py", line 1168, in _fn
    _maybe_set_eval_frame(_callback_from_stance(self.callback))
KeyboardInterrupt
Exception ignored in atexit callback: <bound method AsyncBatchProcessor.stop_accepting_new_work_and_flush_queue of <weave.trace_server_bindings.async_batch_processor.AsyncBatchProcessor object at 0xf9979ecb5250>>
Traceback (most recent call last):
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/weave/trace_server_bindings/async_batch_processor.py", line 146, in stop_accepting_new_work_and_flush_queue
    self.processing_thread.join()
  File "/usr/lib/python3.12/threading.py", line 1147, in join
    self._wait_for_tstate_lock()
  File "/usr/lib/python3.12/threading.py", line 1167, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt:
Exception ignored in atexit callback: <bound method AsyncBatchProcessor.stop_accepting_new_work_and_flush_queue of <weave.trace_server_bindings.async_batch_processor.AsyncBatchProcessor object at 0xf9979ecb5280>>
Traceback (most recent call last):
  File "/home/vsc/LLM_TUNE/QA-FineTune/ë„ì„œê´€_QA_Finetune/.venv/lib/python3.12/site-packages/weave/trace_server_bindings/async_batch_processor.py", line 146, in stop_accepting_new_work_and_flush_queue
    self.processing_thread.join()
  File "/usr/lib/python3.12/threading.py", line 1147, in join
    self._wait_for_tstate_lock()
  File "/usr/lib/python3.12/threading.py", line 1167, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt:
