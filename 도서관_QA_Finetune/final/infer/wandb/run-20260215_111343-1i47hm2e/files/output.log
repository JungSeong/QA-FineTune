[34m[1mwandb[0m: Initializing weave.
[36m[1mweave[0m: Logged in as Weights & Biases user: jungseonglian.
[36m[1mweave[0m: View Weave data at https://wandb.ai/uailab-unist_/library-qa-finetune/weave
2026-02-15 11:13:45 - INFO - [main.py:33] - Dataset({
    features: ['FAQ', 'TITLE', 'DES'],
    num_rows: 110
})
2026-02-15 11:13:45 - INFO - [model_utils.py:29] - üìÇ Î°úÏª¨ Î™®Îç∏ÏùÑ Î∞úÍ≤¨ÌñàÏäµÎãàÎã§: /home/vsc/LLM/model/Exaone-3.5-32B-Instruct
2026-02-15 11:13:45 - INFO - [model_utils.py:32] - üöÄ Î™®Îç∏ Î°úÎî© Ï§ë...
`torch_dtype` is deprecated! Use `dtype` instead!
/home/vsc/LLM_TUNE/QA-FineTune/ÎèÑÏÑúÍ¥Ä_QA_Finetune/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:435: UserWarning:
    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.
    Minimum and Maximum cuda capability supported by this version of PyTorch is
    (8.0) - (12.0)

  queued_call()
Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 579/579 [00:34<00:00, 16.59it/s, Materializing param=transformer.wte.weight]
Traceback (most recent call last):
  File "/home/vsc/LLM_TUNE/QA-FineTune/ÎèÑÏÑúÍ¥Ä_QA_Finetune/final/infer/main.py", line 49, in <module>
    main()
  File "/home/vsc/LLM_TUNE/QA-FineTune/ÎèÑÏÑúÍ¥Ä_QA_Finetune/final/infer/main.py", line 35, in main
    model, tokenizer = load_or_download_model_tokenizer(config)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ÎèÑÏÑúÍ¥Ä_QA_Finetune/final/infer/model_utils.py", line 34, in load_or_download_model_tokenizer
    model = AutoModelForCausalLM.from_pretrained(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ÎèÑÏÑúÍ¥Ä_QA_Finetune/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 367, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ÎèÑÏÑúÍ¥Ä_QA_Finetune/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4063, in from_pretrained
    loading_info, disk_offload_index = cls._load_pretrained_model(model, state_dict, checkpoint_files, load_config)
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ÎèÑÏÑúÍ¥Ä_QA_Finetune/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4182, in _load_pretrained_model
    loading_info, disk_offload_index = convert_and_load_state_dict_in_model(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ÎèÑÏÑúÍ¥Ä_QA_Finetune/.venv/lib/python3.12/site-packages/transformers/core_model_loading.py", line 1192, in convert_and_load_state_dict_in_model
    realized_value = mapping.convert(
                     ^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ÎèÑÏÑúÍ¥Ä_QA_Finetune/.venv/lib/python3.12/site-packages/transformers/core_model_loading.py", line 665, in convert
    collected_tensors = self.materialize_tensors()
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ÎèÑÏÑúÍ¥Ä_QA_Finetune/.venv/lib/python3.12/site-packages/transformers/core_model_loading.py", line 641, in materialize_tensors
    tensors = [future.result() for future in tensors if future.result() is not None]
                                                        ^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/concurrent/futures/_base.py", line 451, in result
    self._condition.wait(timeout)
  File "/usr/lib/python3.12/threading.py", line 355, in wait
    waiter.acquire()
KeyboardInterrupt
Exception ignored in atexit callback: <bound method AsyncBatchProcessor.stop_accepting_new_work_and_flush_queue of <weave.trace_server_bindings.async_batch_processor.AsyncBatchProcessor object at 0xe8d7f9b92900>>
Traceback (most recent call last):
  File "/home/vsc/LLM_TUNE/QA-FineTune/ÎèÑÏÑúÍ¥Ä_QA_Finetune/.venv/lib/python3.12/site-packages/weave/trace_server_bindings/async_batch_processor.py", line 146, in stop_accepting_new_work_and_flush_queue
    self.processing_thread.join()
  File "/usr/lib/python3.12/threading.py", line 1147, in join
    self._wait_for_tstate_lock()
  File "/usr/lib/python3.12/threading.py", line 1167, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt:
Exception ignored in atexit callback: <bound method AsyncBatchProcessor.stop_accepting_new_work_and_flush_queue of <weave.trace_server_bindings.async_batch_processor.AsyncBatchProcessor object at 0xe8d7fac3e450>>
Traceback (most recent call last):
  File "/home/vsc/LLM_TUNE/QA-FineTune/ÎèÑÏÑúÍ¥Ä_QA_Finetune/.venv/lib/python3.12/site-packages/weave/trace_server_bindings/async_batch_processor.py", line 146, in stop_accepting_new_work_and_flush_queue
    self.processing_thread.join()
  File "/usr/lib/python3.12/threading.py", line 1147, in join
    self._wait_for_tstate_lock()
  File "/usr/lib/python3.12/threading.py", line 1167, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt:
