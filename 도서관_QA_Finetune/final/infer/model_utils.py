import os
import logging
import torch
import sys
from transformers import AutoModelForCausalLM, AutoTokenizer
from huggingface_hub import snapshot_download
from peft import prepare_model_for_kbit_training, get_peft_model
from config import Config

current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

from logger_config import get_infer_logger

logger = get_infer_logger()

def load_or_download_model_tokenizer(config):
    # 1. ë””ë ‰í† ë¦¬ í™•ì¸ ë° ë‹¤ìš´ë¡œë“œ
    if not os.path.exists(config.LOCAL_MODEL_DIR) or not os.listdir(config.LOCAL_MODEL_DIR):
        logger.info(f"ğŸ“¡ ëª¨ë¸ì´ {config.LOCAL_MODEL_DIR}ì— ì—†ìŠµë‹ˆë‹¤. ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        try : 
            snapshot_download(repo_id=config.MODEL_ID, local_dir=config.LOCAL_MODEL_DIR)
            logger.info("âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
        except Exception as e :
            logger.error(f"âŒ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise e
    else:
        logger.info(f"ğŸ“‚ ë¡œì»¬ ëª¨ë¸ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤: {config.LOCAL_MODEL_DIR}")

    # 2. ëª¨ë¸ ë¡œë“œ
    logger.info("ğŸš€ ëª¨ë¸ ë¡œë”© ì¤‘...")
    tokenizer = AutoTokenizer.from_pretrained(config.LOCAL_MODEL_DIR)
    model = AutoModelForCausalLM.from_pretrained(
        config.LOCAL_MODEL_DIR,
        quantization_config=config.QUANTIZATION_CONFIG,
        trust_remote_code=True,
        device_map="auto", # Accelerateê°€ ìë™ ê´€ë¦¬í•˜ë„ë¡ ì„¤ì •
        torch_dtype=torch.bfloat16
    )

    # íŒ¨ë”© í† í° ì„¤ì • (ìƒì„± ì‘ì—… í•„ìˆ˜)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model.eval()

    return model, tokenizer