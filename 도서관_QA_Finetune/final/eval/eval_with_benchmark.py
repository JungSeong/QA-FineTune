import os
import sys
import torch
import pandas as pd
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from config import Config

current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

from train.preprocess_dataset import preprocess_dataset
from logger_config import get_eval_logger

logger = get_eval_logger()

config = Config()

def load_LoRA_model():
    base_model_id = Config.MODEL_ID
    local_model_dir = Config.LOCAL_MODEL_DIR
    adapter_path = f"{Config.ADAPTER_PATH}/final"

    model = AutoModelForCausalLM.from_pretrained(
        config.LOCAL_MODEL_DIR,
        quantization_config=config.QUANTIZATION_CONFIG,
        trust_remote_code=True,
        device_map="auto", # Accelerateê°€ ìë™ ê´€ë¦¬í•˜ë„ë¡ ì„¤ì •
    )

    if hasattr(model, "transformer") and hasattr(model.transformer, "wte"):
        model.get_input_embeddings = lambda: model.transformer.wte

    tokenizer = AutoTokenizer.from_pretrained(
        adapter_path,
        trust_remote_code=True,
    )
    model = PeftModel.from_pretrained(model, adapter_path)
    
    model.eval()

    return model, tokenizer

def generate_answer(model, tokenizer, question, context) :
    system_message = (
        "ë‹¹ì‹ ì€ ë„ì„œê´€ ìš´ì˜ì— ëŒ€í•œ ì „ë¬¸ì ì¸ ì§€ì‹ì„ ê°€ì§„ ì¸ê³µì§€ëŠ¥ ì‚¬ì„œì…ë‹ˆë‹¤. "
        "ì œê³µëœ [ë„ì„œê´€ ì •ë³´]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µí•˜ì‹­ì‹œì˜¤. "
        "ì •ë³´ì— ì—†ëŠ” ë‚´ìš©ì€ í•¨ë¶€ë¡œ ì¶”ì¸¡í•˜ì§€ ë§ê³  ì •ì¤‘íˆ í™•ì¸ì´ ì–´ë µë‹¤ê³  ë‹µí•˜ì„¸ìš”."
    )

    user_content = (
        f"### [ë„ì„œê´€ ì •ë³´]\n{context}\n\n"
        f"### [ì§ˆë¬¸]\n{question}\n\n"
        f"### [ì§€ì‹œ ì‚¬í•­]\n"
        f"1. ì¹œì ˆí•œ ë§íˆ¬ë¡œ ê·œì •ì— ê·¼ê±°í•˜ì—¬ ë‹µë³€í•  ê²ƒ.\n"
        f"2. 3ë¬¸ë‹¨ ì´ë‚´ë¡œ ë‹µë³€í•  ê²ƒ.\n"
        f"3. ë‹µë³€ ëì— ì§€ì‹œ ì‚¬í•­ì„ ë°˜ë³µí•˜ì§€ ë§ ê²ƒ."
    )

    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": user_content}
    ]

    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=1024,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id
        )

    response = tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:], skip_special_tokens=True, trust_remote_code=True)
    return response

def run_benchmark():
    model, tokenizer = load_LoRA_model()
    results = []

    logger.info(f"ğŸ§ í•™ìŠµëœ {Config.MODEL_ID}/final ëª¨ë¸ë¡œ ì¶”ë¡  ì‹œì‘...")
    dataset = preprocess_dataset()
    test_dataset = dataset['test']

    for row in tqdm(test_dataset):
        question = row['question']
        context = row['original_title']
        ground_truth = row['answer']
        
        # ëª¨ë¸ì˜ ë‹µë³€ ìƒì„±
        model_generated = generate_answer(model, tokenizer, question, context)
        logger.info(f"Question: {question}")
        logger.info(f"Ground Truth: {ground_truth}")
        logger.info(f"Model Generated: {model_generated}")
        
        results.append({
            "question": question,
            "ground_truth": ground_truth,
            "model_generated": model_generated
        })

    # ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥ (ë‚˜ì¤‘ì— íŒì‚¬ ëª¨ë¸ì—ê²Œ ì „ë‹¬ìš©)
    result_df = pd.DataFrame(results)
    result_df.to_csv(f"{Config.BENCHMARK_PATH}/benchmark_results_{Config.MODEL_ID}.csv", index=False)
    return result_df

# ì‹¤í–‰ ë¶€ë¶„
if __name__ == "__main__":
    run_benchmark()