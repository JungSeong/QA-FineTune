import sys
import os
import glob
import pandas as pd
import wandb
import torch
import sys
from trl import SFTTrainer, SFTConfig
from config import Config
from datasets import Dataset
from model_utils import load_or_download_model_tokenizer
from preprocess_dataset import preprocess_dataset
from accelerate import Accelerator
from prompts import generate_prompts
from functools import partial

current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
sys.path.append(parent_dir)

from logger_config import get_train_logger

logger = get_train_logger()

def main() :
    logger.info("ğŸ’ª í›ˆë ¨ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
    config = Config()

    wandb.init(
        project="library-qa-finetune-train",
        name="train-with-accelerator",
        config=config.__dict__
    )

    logger.info("Loading Dataset...")
    dataset = preprocess_dataset()
    train_dataset, val_dataset, test_dataset = dataset['train'], dataset['val'], dataset['test']
    logger.info("Dataset loaded successfully")
    logger.info(f"train_dataset size: {len(train_dataset)}")
    logger.info(f"val_dataset size: {len(val_dataset)}")
    logger.info(f"test_dataset size: {len(test_dataset)}")

    logger.info("Loading Model with LoRA and Tokenizer...")
    model, tokenizer = load_or_download_model_tokenizer()
    logger.info(model)

    training_args = config.TRAINING_ARGS
    trainer = SFTTrainer(
        model=model,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        args=training_args,
        processing_class=tokenizer,
        formatting_func=generate_prompts
    )

    torch.load = partial(torch.load, weights_only=False)
    last_checkpoint = None
    if os.path.isdir(config.OUTPUT_DIR):
        # OUTPUT_DIR ë‚´ì˜ 'checkpoint-'ë¡œ ì‹œì‘í•˜ëŠ” í´ë”ë“¤ì„ ê²€ìƒ‰
        checkpoints = glob.glob(os.path.join(config.OUTPUT_DIR, "checkpoint-*"))
        if checkpoints:
            # ê°€ì¥ ìˆ«ìê°€ ë†’ì€(ìµœì‹ ) ì²´í¬í¬ì¸íŠ¸ë¥¼ ì„ íƒ
            last_checkpoint = max(checkpoints, key=os.path.getctime)
            logger.info(f"âœ… ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: {last_checkpoint}. í•™ìŠµì„ ì¬ê°œí•©ë‹ˆë‹¤.")
    
    if last_checkpoint:
        trainer.train(resume_from_checkpoint=last_checkpoint)
    else:
        logger.info("ğŸš€ Starting training...")
        trainer.train()

    accelerator = trainer.accelerator

    accelerator.wait_for_everyone()
    if accelerator.is_main_process:
        trainer.save_model(f"{config.OUTPUT_DIR}/final")
        tokenizer.save_pretrained(f"{config.OUTPUT_DIR}/final")
        print("ğŸ“ Trained finished! Model saved.")
        wandb.finish()

if __name__ == "__main__" :
    main()