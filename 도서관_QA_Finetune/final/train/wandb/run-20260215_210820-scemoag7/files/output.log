2026-02-15 21:08:21 - INFO - [main.py:33] - Loading Dataset...
2026-02-15 21:08:21 - INFO - [preprocess_dataset.py:22] - Loading Dataset...
2026-02-15 21:08:21 - INFO - [main.py:36] - Dataset loaded successfully
2026-02-15 21:08:21 - INFO - [main.py:37] - train_dataset size: 716
2026-02-15 21:08:21 - INFO - [main.py:38] - val_dataset size: 308
2026-02-15 21:08:21 - INFO - [main.py:39] - test_dataset size: 257
2026-02-15 21:08:21 - INFO - [main.py:41] - Loading Model with LoRA and Tokenizer...
2026-02-15 21:08:21 - INFO - [model_utils.py:31] - üìÇ Î°úÏª¨ Î™®Îç∏ÏùÑ Î∞úÍ≤¨ÌñàÏäµÎãàÎã§: /home/vsc/LLM/model/Exaone-3.5-32B-Instruct
2026-02-15 21:08:21 - INFO - [model_utils.py:34] - üöÄ Î™®Îç∏ Î°úÎî© Ï§ë...
`torch_dtype` is deprecated! Use `dtype` instead!
Traceback (most recent call last):
  File "/home/vsc/LLM_TUNE/QA-FineTune/ÎèÑÏÑúÍ¥Ä_QA_Finetune/final/train/main.py", line 65, in <module>
    main()
  File "/home/vsc/LLM_TUNE/QA-FineTune/ÎèÑÏÑúÍ¥Ä_QA_Finetune/final/train/main.py", line 42, in main
    model, tokenizer = load_or_download_model_tokenizer()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/ÎèÑÏÑúÍ¥Ä_QA_Finetune/final/train/model_utils.py", line 36, in load_or_download_model_tokenizer
    model = AutoModelForCausalLM.from_pretrained(
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/lib-QA/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 367, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/lib-QA/lib/python3.12/site-packages/transformers/modeling_utils.py", line 4044, in from_pretrained
    device_map = _get_device_map(model, device_map, max_memory, hf_quantizer)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vsc/LLM_TUNE/QA-FineTune/lib-QA/lib/python3.12/site-packages/transformers/integrations/accelerate.py", line 359, in _get_device_map
    hf_quantizer.validate_environment(device_map=device_map)
  File "/home/vsc/LLM_TUNE/QA-FineTune/lib-QA/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_4bit.py", line 72, in validate_environment
    raise ValueError(
ValueError: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details.
