{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49e5b86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available hf token\n",
      "--- 로딩 시작: JungSeong2/library-QA-Adapter (revision: v1.0) ---\n",
      "[1/3] 토크나이저 저장 완료: ./LoRA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec88a8809d8d44b39c6459c8bb9a2ef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4cc795c083a44df9acb2121d43fa477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767d16e942bb4fa498ad0fca9936538e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vsc/LLM_TUNE/QA-FineTune/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:435: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  queued_call()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb41c45736f244c4a8f6ebf47c30163a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/272 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa7d4880ba34655b3477b29962a3eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/134 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/3] 베이스 모델 로드 완료\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9079581b10b74a2b8d16c39b0c7981ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/1.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NotImplementedError",
     "evalue": "`get_input_embeddings` not auto‑handled for ExaoneModel; please override in the subclass.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[2/3] 베이스 모델 로드 완료\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# 4. 어댑터(LoRA) 로드 및 저장\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# 베이스 모델 위에 어댑터를 올린 뒤, save_pretrained를 호출합니다.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m model = \u001b[43mPeftModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m model.save_pretrained(save_path)\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[3/3] 어댑터(LoRA) 저장 완료: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM_TUNE/QA-FineTune/.venv/lib/python3.12/site-packages/peft/peft_model.py:560\u001b[39m, in \u001b[36mPeftModel.from_pretrained\u001b[39m\u001b[34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, key_mapping, **kwargs)\u001b[39m\n\u001b[32m    552\u001b[39m     model = \u001b[38;5;28mcls\u001b[39m(\n\u001b[32m    553\u001b[39m         model,\n\u001b[32m    554\u001b[39m         config,\n\u001b[32m   (...)\u001b[39m\u001b[32m    557\u001b[39m         low_cpu_mem_usage=low_cpu_mem_usage,\n\u001b[32m    558\u001b[39m     )\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m560\u001b[39m     model = \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m        \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m load_result = model.load_adapter(\n\u001b[32m    569\u001b[39m     model_id,\n\u001b[32m    570\u001b[39m     adapter_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    575\u001b[39m     **kwargs,\n\u001b[32m    576\u001b[39m )\n\u001b[32m    578\u001b[39m \u001b[38;5;66;03m# 1. Remove VB-LoRA vector bank, since it's a shared parameter set via the VBLoRAModel\u001b[39;00m\n\u001b[32m    579\u001b[39m \u001b[38;5;66;03m# 2. Remove the prompt encoder, as it does not need to be part of the checkpoint\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM_TUNE/QA-FineTune/.venv/lib/python3.12/site-packages/peft/peft_model.py:1885\u001b[39m, in \u001b[36mPeftModelForCausalLM.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, **kwargs)\u001b[39m\n\u001b[32m   1882\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m   1883\u001b[39m     \u001b[38;5;28mself\u001b[39m, model: torch.nn.Module, peft_config: PeftConfig, adapter_name: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m, **kwargs\n\u001b[32m   1884\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1885\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1886\u001b[39m     \u001b[38;5;28mself\u001b[39m.base_model_prepare_inputs_for_generation = \u001b[38;5;28mself\u001b[39m.base_model.prepare_inputs_for_generation\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM_TUNE/QA-FineTune/.venv/lib/python3.12/site-packages/peft/peft_model.py:129\u001b[39m, in \u001b[36mPeftModel.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    127\u001b[39m     ctx = init_empty_weights \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage \u001b[38;5;28;01melse\u001b[39;00m nullcontext\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx():\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m         \u001b[38;5;28mself\u001b[39m.base_model = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.base_model, \u001b[33m\"\u001b[39m\u001b[33m_cast_adapter_dtype\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    132\u001b[39m     \u001b[38;5;28mself\u001b[39m.base_model._cast_adapter_dtype(\n\u001b[32m    133\u001b[39m         adapter_name=adapter_name, autocast_adapter_dtype=autocast_adapter_dtype\n\u001b[32m    134\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM_TUNE/QA-FineTune/.venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:298\u001b[39m, in \u001b[36mBaseTuner.__init__\u001b[39m\u001b[34m(self, model, peft_config, adapter_name, low_cpu_mem_usage, state_dict)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;28mself\u001b[39m._pre_injection_hook(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.peft_config[adapter_name], adapter_name)\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m peft_config != PeftType.XLORA \u001b[38;5;129;01mor\u001b[39;00m peft_config[adapter_name] != PeftType.XLORA:\n\u001b[32m--> \u001b[39m\u001b[32m298\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[32m    301\u001b[39m \u001b[38;5;28mself\u001b[39m.model.peft_config = \u001b[38;5;28mself\u001b[39m.peft_config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM_TUNE/QA-FineTune/.venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:711\u001b[39m, in \u001b[36mBaseTuner.inject_adapter\u001b[39m\u001b[34m(self, model, adapter_name, autocast_adapter_dtype, low_cpu_mem_usage, state_dict)\u001b[39m\n\u001b[32m    706\u001b[39m \u001b[38;5;66;03m# Note: If possible, all checks should be performed *at the start of this method*.\u001b[39;00m\n\u001b[32m    707\u001b[39m \u001b[38;5;66;03m# This way, we can raise early if something goes wrong, without leaving the model\u001b[39;00m\n\u001b[32m    708\u001b[39m \u001b[38;5;66;03m# in a bad (half-initialized) state.\u001b[39;00m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._check_new_adapter_config(peft_config)\n\u001b[32m--> \u001b[39m\u001b[32m711\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_tied_modules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    713\u001b[39m model_config = \u001b[38;5;28mself\u001b[39m.get_model_config(model)\n\u001b[32m    715\u001b[39m peft_config = \u001b[38;5;28mself\u001b[39m._prepare_adapter_config(peft_config, model_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM_TUNE/QA-FineTune/.venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:1202\u001b[39m, in \u001b[36mBaseTuner._check_tied_modules\u001b[39m\u001b[34m(self, model, peft_config)\u001b[39m\n\u001b[32m   1199\u001b[39m modules_to_save = \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(peft_config, \u001b[33m\"\u001b[39m\u001b[33mmodules_to_save\u001b[39m\u001b[33m\"\u001b[39m, []) \u001b[38;5;129;01mor\u001b[39;00m [])\n\u001b[32m   1200\u001b[39m is_embedding_to_save = \u001b[38;5;28many\u001b[39m(m \u001b[38;5;129;01min\u001b[39;00m EMBEDDING_LAYER_NAMES \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m modules_to_save)\n\u001b[32m-> \u001b[39m\u001b[32m1202\u001b[39m tied_weight_keys = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module_names_tied_with_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(peft_config, \u001b[33m\"\u001b[39m\u001b[33mensure_weight_tying\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1205\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_embedding_to_save \u001b[38;5;129;01mand\u001b[39;00m tied_weight_keys:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM_TUNE/QA-FineTune/.venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:1178\u001b[39m, in \u001b[36mBaseTuner._get_module_names_tied_with_embedding\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module_names_tied_with_embedding\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m-> \u001b[39m\u001b[32m1178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_module_names_tied_with_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM_TUNE/QA-FineTune/.venv/lib/python3.12/site-packages/peft/utils/other.py:1613\u001b[39m, in \u001b[36m_get_module_names_tied_with_embedding\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m   1605\u001b[39m         tied_weights_keys.update(\n\u001b[32m   1606\u001b[39m             {\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m module_tied_weights_keys.items()}\n\u001b[32m   1607\u001b[39m         )\n\u001b[32m   1609\u001b[39m \u001b[38;5;66;03m# technically it would be sufficient to just return candidates since that contains all the keys of\u001b[39;00m\n\u001b[32m   1610\u001b[39m \u001b[38;5;66;03m# all models that are tied (not just equal!) to the input embeddings. the only reason why we aren't\u001b[39;00m\n\u001b[32m   1611\u001b[39m \u001b[38;5;66;03m# doing that is because we need to filter out the original embedding name since we promise to just\u001b[39;00m\n\u001b[32m   1612\u001b[39m \u001b[38;5;66;03m# return the keys of the tying targets.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1613\u001b[39m input_embedding_params = \u001b[38;5;28mset\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_input_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.parameters())\n\u001b[32m   1614\u001b[39m candidates = [n \u001b[38;5;28;01mfor\u001b[39;00m n, p \u001b[38;5;129;01min\u001b[39;00m model.named_parameters(remove_duplicate=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m input_embedding_params]\n\u001b[32m   1616\u001b[39m \u001b[38;5;66;03m# Consider the case that sources and targets are already wrapped by a PEFT method. In that case we won't\u001b[39;00m\n\u001b[32m   1617\u001b[39m \u001b[38;5;66;03m# find them by their old names. Therefore, we need to create a map of the new names to the old names so\u001b[39;00m\n\u001b[32m   1618\u001b[39m \u001b[38;5;66;03m# that we can translate back and forth.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM_TUNE/QA-FineTune/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:1033\u001b[39m, in \u001b[36mEmbeddingAccessMixin.get_input_embeddings\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1031\u001b[39m     base_model = \u001b[38;5;28mself\u001b[39m.base_model\n\u001b[32m   1032\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m base_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m base_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1033\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbase_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_input_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m   1036\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`get_input_embeddings` not auto‑handled for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m; please override in the subclass.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1037\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM_TUNE/QA-FineTune/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:1035\u001b[39m, in \u001b[36mEmbeddingAccessMixin.get_input_embeddings\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1032\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m base_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m base_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m   1033\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m base_model.get_input_embeddings()\n\u001b[32m-> \u001b[39m\u001b[32m1035\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m   1036\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`get_input_embeddings` not auto‑handled for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m; please override in the subclass.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1037\u001b[39m )\n",
      "\u001b[31mNotImplementedError\u001b[39m: `get_input_embeddings` not auto‑handled for ExaoneModel; please override in the subclass."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "hf_token = os.environ.get(\"hf_token\")\n",
    "\n",
    "from huggingface_hub import login\n",
    "if hf_token :\n",
    "    print(\"available hf token\")\n",
    "    login(token=hf_token) \n",
    "else :\n",
    "    print(\"No hf token\")\n",
    "\n",
    "# 1. 설정 (사용 중인 경로 및 ID)\n",
    "base_model_id = \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\"\n",
    "adapter_repo_id = \"JungSeong2/library-QA-Adapter\"\n",
    "revision = \"v1.0\"  # 특정 버전(브랜치) 지정\n",
    "save_path = \"./LoRA\" # 저장할 로컬 디렉토리\n",
    "\n",
    "# 폴더가 없으면 생성\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "print(f\"--- 로딩 시작: {adapter_repo_id} (revision: {revision}) ---\")\n",
    "\n",
    "# 2. 토크나이저 로드 및 저장\n",
    "# 어댑터 저장소에 올린 토크나이저를 가져와서 지정된 경로에 저장합니다.\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_repo_id, revision=revision, token=hf_token)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"[1/3] 토크나이저 저장 완료: {save_path}\")\n",
    "\n",
    "# 3. 베이스 모델 로드\n",
    "# 저장만 하는 목적이라면 양자화 없이 로드하는 것이 나중에 활용도가 높습니다.\n",
    "# 메모리가 부족하다면 device_map=\"cpu\"를 고려하세요.\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(f\"[2/3] 베이스 모델 로드 완료\")\n",
    "\n",
    "# 4. 어댑터(LoRA) 로드 및 저장\n",
    "# 베이스 모델 위에 어댑터를 올린 뒤, save_pretrained를 호출합니다.\n",
    "model = PeftModel.from_pretrained(base_model, adapter_repo_id, revision=revision)\n",
    "model.save_pretrained(save_path)\n",
    "print(f\"[3/3] 어댑터(LoRA) 저장 완료: {save_path}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"작업 완료! 모든 파일이 '{os.path.abspath(save_path)}'에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5513a609",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QA",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
