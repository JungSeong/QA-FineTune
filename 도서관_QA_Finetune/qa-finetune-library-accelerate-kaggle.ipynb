{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install uv\n!uv init","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python3 --version","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --system : ì‹¤ì œ ë…¸íŠ¸ë¶ ì»¤ë„ì´ íŒ¨í‚¤ì§€ë¥¼ ì•Œê²Œë” í•¨\n\n!uv pip install --system --force-reinstall torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n!uv pip install --system huggingface_hub pyarrow datasets peft bitsandbytes\n!uv pip install --system \"trl==0.11.4\" \"transformers==4.45.2\" \"accelerate==0.34.2\" \"peft==0.13.2\" \"bitsandbytes>=0.45.0\" \"numpy<2.2\"\n!uv pip install --system -U wandb\n!uv pip install --system -U transformers accelerate peft\n!uv pip install --system -U transformers trl peft bitsandbytes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport transformers\n\nprint(f\"íŒŒì´í† ì¹˜ ë²„ì „: {torch.__version__}\")\nprint(f\"CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€: {torch.cuda.is_available()}\")\nprint(f\"CUDA ë²„ì „: {torch.version.cuda}\")\nprint(f\"PyTorch ë²„ì „: {torch.version}\")\nprint(f\"NumPy version: {np.__version__}\")\nprint(f\"transformers version: {transformers.__version__}\")\n\nif torch.cuda.is_available() :\n    capability = torch.cuda.get_device_capability()\n    print(f\"GPU ì´ë¦„: {torch.cuda.get_device_name()}\")\n    print(f\"Compute Capability: {capability[0]}.{capability[1]}\")\n    print(f\"BF16 ì§€ì› ì—¬ë¶€: {torch.cuda.is_bf16_supported()}\")\nelse:\n    print(\"CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM,\n    BitsAndBytesConfig, \n    TrainingArguments,\n)\nfrom datasets import Dataset\nimport os, torch, json, wandb, subprocess\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nfrom peft import (\n    get_peft_model,\n    LoraConfig, \n    TaskType,\n    prepare_model_for_kbit_training\n)\nfrom accelerate import Accelerator, notebook_launcher","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import snapshot_download\n\nsave_path = f\"/kaggle/working/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\"\n\nsnapshot_download(\n    repo_id=\"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\",\n    local_dir=save_path,\n    local_dir_use_symlinks=False,  # ì‹¤ì œ íŒŒì¼ì„ í•´ë‹¹ ê²½ë¡œì— ì§ì ‘ ë³µì‚¬/ë‹¤ìš´ë¡œë“œ\n    max_workers=8 # ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ì†ë„ í–¥ìƒ\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"notebook_launcher <- X","metadata":{}},{"cell_type":"code","source":"script = '''#!/usr/bin/env python3\n\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ['TZ'] = 'Asia/Seoul'\ntime.tzset()\n\nimport torch\nimport wandb\nfrom datetime import datetime\nfrom functools import partial\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom trl import SFTTrainer, SFTConfig\nfrom peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig\nfrom datasets import load_dataset\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\n\n# ============================================\n# 1. HuggingFace ë¡œê·¸ì¸\n# ============================================\ntry:\n    user_secrets = UserSecretsClient()\n    hf_token = user_secrets.get_secret(\"hf_token\")\n    if hf_token:\n        login(token=hf_token)\n        print(\"âœ… Logged in to HuggingFace\")\nexcept Exception as e:\n    print(f\"âš ï¸ Could not retrieve HF secret: {e}\")\n    hf_token = None\n\nPROJECT_NAME=\"library-faq-finetune\"\n\nos.environ[\"WANDB_PROJECT\"] = PROJECT_NAME # í”„ë¡œì íŠ¸ ì´ë¦„\nos.environ[\"WANDB_RUN_ID\"] = \"v1\" # ë…¸íŠ¸ë¶ ê³ ìœ  ID\nos.environ[\"WANDB_RESUME\"] = \"allow\" # í•´ë‹¹ ë…¸íŠ¸ë¶ì—ì„œ í•™ìŠµì„ ì´ì–´ì„œ ì§„í–‰í•  ê²ƒì¸ì§€\n\ntry:\n    wandb_token = user_secrets.get_secret(\"wandb\")\n    if wandb_token:\n        wandb.login(key=wandb_token)\n        # timeout ëŠ˜ë¦¬ê¸°\n        wandb.init(\n            project=os.environ[\"WANDB_PROJECT\"],\n            id=os.environ[\"WANDB_RUN_ID\"],\n            resume=os.environ[\"WANDB_RESUME\"],\n            name=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") # ëŒ€ì‹œë³´ë“œì— í‘œì‹œë  ì´ë¦„\n        )\n        print(\"âœ… Logged in to W&B\")\nexcept Exception as e:\n    print(f\"âš ï¸ W&B login failed: {e}\")\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# ============================================\n# 2. ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¨¼ì € ë¡œë“œ (generate_promptsì—ì„œ ì‚¬ìš©)\n# ============================================\nmodel_id = \"/kaggle/working/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\"\n\nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# ============================================\n# 3. ë°ì´í„°ì…‹ ë¡œë“œ\n# ============================================\nprint(\"Loading dataset...\")\ndataset = load_dataset(\"JungSeong2/library-qa\", token=hf_token)\n\ntrain_dataset = dataset[\"train\"]\nval_dataset = dataset[\"validation\"]\ntest_dataset = dataset[\"test\"]\n\nprint(f\"âœ… Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n\n# ============================================\n# 4. Formatting Function (í† í¬ë‚˜ì´ì €ê°€ ì´ë¯¸ ë¡œë“œë¨)\n# ============================================\ndef generate_prompts(examples):\n    \"\"\"ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ í•¨ìˆ˜\"\"\"\n    question = examples[\"question\"]\n    answer = examples[\"answer\"]\n    context = examples[\"DES\"]\n    \n    system_message = (\n        \"ë‹¹ì‹ ì€ ë„ì„œê´€ ìš´ì˜ì— ëŒ€í•œ ì „ë¬¸ì ì¸ ì§€ì‹ì„ ê°€ì§„ ì¸ê³µì§€ëŠ¥ ì‚¬ì„œìž…ë‹ˆë‹¤. \"\n        \"ì œê³µëœ [ë„ì„œê´€ ì •ë³´]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µí•˜ì‹­ì‹œì˜¤. \"\n        \"ì •ë³´ì— ì—†ëŠ” ë‚´ìš©ì€ í•¨ë¶€ë¡œ ì¶”ì¸¡í•˜ì§€ ë§ê³  ì •ì¤‘ížˆ í™•ì¸ì´ ì–´ë µë‹¤ê³  ë‹µí•˜ì„¸ìš”.\"\n    )\n\n    user_content = (\n        f\"### [ë„ì„œê´€ ì •ë³´]\\\\n{context}\\\\n\\\\n\"\n        f\"### [ì§ˆë¬¸]\\\\n{question}\\\\n\\\\n\"\n        f\"### [ì§€ì‹œ ì‚¬í•­]\\\\n\"\n        f\"1. ì¹œì ˆí•œ ë§íˆ¬ë¡œ ê·œì •ì— ê·¼ê±°í•˜ì—¬ ë‹µë³€í•  ê²ƒ.\\\\n\"\n        f\"2. 3ë¬¸ë‹¨ ì´ë‚´ë¡œ ë‹µë³€í•  ê²ƒ.\\\\n\"\n        f\"3. ë‹µë³€ ëì— ì§€ì‹œ ì‚¬í•­ì„ ë°˜ë³µí•˜ì§€ ë§ ê²ƒ.\"\n    )\n\n    messages = [\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": user_content},\n        {\"role\": \"assistant\", \"content\": answer}\n    ]\n\n    # apply_chat_template ì ìš©\n    full_prompt = tokenizer.apply_chat_template(\n        messages, \n        tokenize=False, \n        add_generation_prompt=False\n    )\n    \n    return full_prompt\n\n# ============================================\n# 5. Config\n# ============================================\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True\n)\n\n# âœ… EXAONE ëª¨ë¸ìš©: o_proj ì‚¬ìš© (out_proj ì•„ë‹˜)\npeft_config = LoraConfig(\n    task_type=\"CAUSAL_LM\",\n    inference_mode=False,\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]  # âœ… ìˆ˜ì •\n)\n\n# ============================================\n# 6. ëª¨ë¸ ë¡œë“œ ë° PEFT ì ìš©\n# ============================================\nprint(\"Loading model...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.bfloat16,\n    quantization_config=quantization_config,\n    trust_remote_code=True,\n    device_map=\"auto\"\n)\n\nif hasattr(model, \"transformer\") and hasattr(model.transformer, \"wte\"):\n    model.get_input_embeddings = lambda: model.transformer.wte\n\nprint(\"Applying PEFT...\")\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n\n# ============================================\n# 7. Training Arguments\n# ============================================\ntraining_args = SFTConfig(\n    output_dir=\"./SFT_single_gpu\",\n    num_train_epochs=1,\n    \n    # ë°°ì¹˜ ì„¤ì •\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8,\n    \n    # ìµœì í™”\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.1,\n    max_grad_norm=1.0,\n    \n    # ì •ë°€ë„\n    fp16=False,\n    bf16=True,\n    \n    # Gradient checkpointing\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    \n    # ë°ì´í„° ë¡œë”©\n    dataloader_num_workers=0,\n    \n    # SFT ì„¤ì •\n    max_length=1024, \n    packing=False,  # âœ… ì¼ë‹¨ Falseë¡œ (ì•ˆì •ì„±)\n    \n    # Logging & Saving\n    logging_steps=10,\n    logging_strategy=\"steps\",\n    save_strategy=\"steps\",\n    save_steps=100,\n    save_total_limit=2,\n    \n    # Evaluation (ì„ íƒì‚¬í•­)\n    eval_strategy=\"steps\",\n    eval_steps=100,\n    \n    # Reporting\n    report_to=\"wandb\"\n)\n\n# ============================================\n# 8. Trainer ìƒì„± ë° í•™ìŠµ\n# ============================================\nprint(\"Creating trainer...\")\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,  # âœ… validation ì¶”ê°€\n    processing_class=tokenizer,  # âœ… processing_class ì‚¬ìš©\n    formatting_func=generate_prompts,\n)\n\n# ì €ìž¥ íŒ¨ì¹˜\ntorch.load = partial(torch.load, weights_only=False)\n\nprint(\"ðŸš€ Starting training...\")\ntrainer.train()\n\n# ============================================\n# 9. ëª¨ë¸ ì €ìž¥\n# ============================================\nprint(\"Saving model...\")\ntrainer.save_model(\"./SFT_single_gpu/final\")\ntokenizer.save_pretrained(\"./SFT_single_gpu/final\")\nprint(\"âœ… Training Complete!\")\n'''\n\nwith open(\"train_single.py\", \"w\") as f:\n    f.write(script)\n\nprint(\"âœ… Script created: train_single.py\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python3 train_single.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"script = '''#!/usr/bin/env python3\n\nimport os\nimport torch\nimport wandb\nimport time\nfrom datetime import datetime\nfrom functools import partial\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom trl import SFTTrainer, SFTConfig\nfrom peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig\nfrom datasets import load_dataset\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\nfrom accelerate import Accelerator\n\n# ============================================\n# 1. ì´ˆê¸°í™” ë° í™˜ê²½ ì„¤ì •\n# ============================================\naccelerator = Accelerator()\n# ì‹œê°„ì„ ì„œìš¸ ì‹œê°„ìœ¼ë¡œ ì„¤ì • (ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ)\nif accelerator.is_main_process:\n    os.environ['TZ'] = 'Asia/Seoul'\n    time.tzset()\n\n# HuggingFace ë¡œê·¸ì¸\ntry:\n    user_secrets = UserSecretsClient()\n    hf_token = user_secrets.get_secret(\"hf_token\")\n    if hf_token:\n        login(token=hf_token)\nexcept Exception:\n    hf_token = None\n\n# W&B ì„¤ì • (ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ ì´ˆê¸°í™”)\nPROJECT_NAME = \"library-faq-finetune\"\nif accelerator.is_main_process:\n    try:\n        wandb_token = user_secrets.get_secret(\"wandb\")\n        if wandb_token:\n            wandb.login(key=wandb_token)\n            wandb.init(\n                project=PROJECT_NAME,\n                name=f\"multi-gpu-{datetime.now().strftime('%m%d-%H%M')}\"\n            )\n    except:\n        os.environ[\"WANDB_DISABLED\"] = \"true\"\nelse:\n    os.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# ============================================\n# 2. ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ\n# ============================================\nmodel_id = \"/kaggle/working/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# ============================================\n# 3. ë°ì´í„°ì…‹ ë¡œë“œ\n# ============================================\nprint(\"Loading dataset...\")\ndataset = load_dataset(\"JungSeong2/library-qa\", token=hf_token)\n\ntrain_dataset = dataset[\"train\"]\nval_dataset = dataset[\"validation\"]\ntest_dataset = dataset[\"test\"]\n\nprint(f\"âœ… Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n\ndef generate_prompts(examples):\n    \"\"\"ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ í•¨ìˆ˜\"\"\"\n    question = examples[\"question\"]\n    answer = examples[\"answer\"]\n    context = examples[\"DES\"]\n    \n    system_message = (\n        \"ë‹¹ì‹ ì€ ë„ì„œê´€ ìš´ì˜ì— ëŒ€í•œ ì „ë¬¸ì ì¸ ì§€ì‹ì„ ê°€ì§„ ì¸ê³µì§€ëŠ¥ ì‚¬ì„œìž…ë‹ˆë‹¤. \"\n        \"ì œê³µëœ [ë„ì„œê´€ ì •ë³´]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µí•˜ì‹­ì‹œì˜¤. \"\n        \"ì •ë³´ì— ì—†ëŠ” ë‚´ìš©ì€ í•¨ë¶€ë¡œ ì¶”ì¸¡í•˜ì§€ ë§ê³  ì •ì¤‘ížˆ í™•ì¸ì´ ì–´ë µë‹¤ê³  ë‹µí•˜ì„¸ìš”.\"\n    )\n\n    user_content = (\n        f\"### [ë„ì„œê´€ ì •ë³´]\\\\n{context}\\\\n\\\\n\"\n        f\"### [ì§ˆë¬¸]\\\\n{question}\\\\n\\\\n\"\n        f\"### [ì§€ì‹œ ì‚¬í•­]\\\\n\"\n        f\"1. ì¹œì ˆí•œ ë§íˆ¬ë¡œ ê·œì •ì— ê·¼ê±°í•˜ì—¬ ë‹µë³€í•  ê²ƒ.\\\\n\"\n        f\"2. 3ë¬¸ë‹¨ ì´ë‚´ë¡œ ë‹µë³€í•  ê²ƒ.\\\\n\"\n        f\"3. ë‹µë³€ ëì— ì§€ì‹œ ì‚¬í•­ì„ ë°˜ë³µí•˜ì§€ ë§ ê²ƒ.\"\n    )\n\n    messages = [\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": user_content},\n        {\"role\": \"assistant\", \"content\": answer}\n    ]\n\n    # apply_chat_template ì ìš©\n    full_prompt = tokenizer.apply_chat_template(\n        messages, \n        tokenize=False, \n        add_generation_prompt=False\n    )\n    \n    return full_prompt\n\n# ============================================\n# 4. Config & ëª¨ë¸ ë¡œë“œ (Multi-GPU ìµœì í™”)\n# ============================================\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True\n)\n\npeft_config = LoraConfig(\n    task_type=\"CAUSAL_LM\",\n    inference_mode=False,\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    # EXAONE-3.5-2.4B ê¸°ì¤€ íƒ€ê²Ÿ ëª¨ë“ˆ í™•ì¸ í•„ìš”\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"] \n)\n\n# device_map ì„¤ì • ë³€ê²½: í˜„ìž¬ í”„ë¡œì„¸ìŠ¤ì˜ GPU ë²ˆí˜¸ë¥¼ ëª…ì‹œì ìœ¼ë¡œ í• ë‹¹\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.bfloat16,\n    quantization_config=quantization_config,\n    trust_remote_code=True,\n    device_map={\"\": accelerator.process_index} # í•µì‹¬: í˜„ìž¬ í”„ë¡œì„¸ìŠ¤ ì¸ë±ìŠ¤ í• ë‹¹\n)\n\nif hasattr(model, \"transformer\") and hasattr(model.transformer, \"wte\"):\n    model.get_input_embeddings = lambda: model.transformer.wte\n\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, peft_config)\n\n# ============================================\n# 5. Training Arguments (DDP ìµœì í™”)\n# ============================================\ntraining_args = SFTConfig(\n    output_dir=\"./SFT_multi_gpu\",\n    num_train_epochs=1,\n    per_device_train_batch_size=1, # ê° GPUë‹¹ ë°°ì¹˜ ì‚¬ì´ì¦ˆ\n    gradient_accumulation_steps=4, # GPUê°€ 2ê°œì´ë¯€ë¡œ 4*2=8 íš¨ê³¼\n    learning_rate=2e-4,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.1,\n    bf16=True,\n    gradient_checkpointing=True,\n    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n    logging_steps=5,\n    save_strategy=\"no\", # í•™ìŠµ ì¢…ë£Œ í›„ ìˆ˜ë™ ì €ìž¥ ê¶Œìž¥\n    eval_strategy=\"no\",\n    report_to=\"wandb\" if accelerator.is_main_process else \"none\",\n    # DDP ì„¤ì •\n    ddp_find_unused_parameters=False,\n    max_length=1024,\n    run_name=f\"{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\"\n)\n\n# ============================================\n# 6. Trainer ìƒì„± ë° í•™ìŠµ\n# ============================================\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"validation\"],\n    processing_class=tokenizer,\n    formatting_func=generate_prompts,\n)\n\n# í•™ìŠµ ì‹œìž‘\ntrainer.train()\n\n# ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ ëª¨ë¸ ì €ìž¥\nif accelerator.is_main_process:\n    trainer.save_model(\"./SFT_multi_gpu/final\")\n    tokenizer.save_pretrained(\"./SFT_multi_gpu/final\")\n    print(\"ðŸš€ All processes finished! Model saved.\")\n    wandb.finish()\n'''\n\nwith open(\"train_multi_gpu.py\", \"w\") as f:\n    f.write(script)\n\nprint(\"âœ… Script created: train_multi_gpu.py\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T05:26:36.986247Z","iopub.execute_input":"2026-02-13T05:26:36.986940Z","iopub.status.idle":"2026-02-13T05:26:36.995450Z","shell.execute_reply.started":"2026-02-13T05:26:36.986903Z","shell.execute_reply":"2026-02-13T05:26:36.994696Z"}},"outputs":[{"name":"stdout","text":"âœ… Script created: train_multi_gpu.py\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!accelerate launch --multi_gpu --num_processes 2 --num_machines 1 --mixed_precision bf16 train_multi_gpu.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T05:26:37.861328Z","iopub.execute_input":"2026-02-13T05:26:37.862345Z","iopub.status.idle":"2026-02-13T05:52:25.696226Z","shell.execute_reply.started":"2026-02-13T05:26:37.862307Z","shell.execute_reply":"2026-02-13T05:52:25.695218Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"The following values were not passed to `accelerate launch` and had defaults used instead:\n\t`--dynamo_backend` was set to a value of `'no'`\nTo avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\nThe repository /kaggle/working/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct contains custom code which must be executed to correctly load the model. You can inspect the repository content at /kaggle/working/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct .\n You can inspect the repository content at https://hf.co//kaggle/working/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N] \u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Using explicit session credentials for https://api.wandb.ai.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjungseonglian\u001b[0m (\u001b[33muailab-unist_\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m Waiting for wandb.init()...\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m Waiting for wandb.init()...\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m Waiting for wandb.init()...\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.25.0\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20260213_142659-i5qd3q6x\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmulti-gpu-0213-1426\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/uailab-unist_/library-faq-finetune\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/uailab-unist_/library-faq-finetune/runs/i5qd3q6x\u001b[0m\nThe repository /kaggle/working/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct contains custom code which must be executed to correctly load the model. You can inspect the repository content at /kaggle/working/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct .\n You can inspect the repository content at https://hf.co//kaggle/working/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N] You are using a model of type exaone to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\nLoading dataset...\nâœ… Train: 1131, Val: 485, Test: 405\n`torch_dtype` is deprecated! Use `dtype` instead!\nYou are using a model of type exaone to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\nLoading dataset...\nâœ… Train: 1131, Val: 485, Test: 405\n`torch_dtype` is deprecated! Use `dtype` instead!\nLoading weights: 100%|â–ˆ| 272/272 [00:03<00:00, 84.37it/s, Materializing param=tr\nwarmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\nLoading weights: 100%|â–ˆ| 272/272 [00:02<00:00, 92.22it/s, Materializing param=tr\nwarmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\nThe tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 361}.\nThe tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 361}.\n{'loss': '3.08', 'grad_norm': '1.242', 'learning_rate': '5.333e-05', 'entropy': '1.546', 'num_tokens': '1.181e+04', 'mean_token_accuracy': '0.5042', 'epoch': '0.03534'}\n{'loss': '2.907', 'grad_norm': '1.133', 'learning_rate': '0.00012', 'entropy': '1.677', 'num_tokens': '2.369e+04', 'mean_token_accuracy': '0.4989', 'epoch': '0.07067'}\n{'loss': '2.243', 'grad_norm': '1', 'learning_rate': '0.0001867', 'entropy': '1.965', 'num_tokens': '3.496e+04', 'mean_token_accuracy': '0.564', 'epoch': '0.106'}\n{'loss': '1.713', 'grad_norm': '1.141', 'learning_rate': '0.0001995', 'entropy': '2.179', 'num_tokens': '4.712e+04', 'mean_token_accuracy': '0.6414', 'epoch': '0.1413'}\n{'loss': '1.23', 'grad_norm': '0.8086', 'learning_rate': '0.0001975', 'entropy': '1.646', 'num_tokens': '5.861e+04', 'mean_token_accuracy': '0.746', 'epoch': '0.1767'}\n{'loss': '0.9794', 'grad_norm': '0.5625', 'learning_rate': '0.0001941', 'entropy': '1.1', 'num_tokens': '6.999e+04', 'mean_token_accuracy': '0.7925', 'epoch': '0.212'}\n{'loss': '0.8289', 'grad_norm': '0.4707', 'learning_rate': '0.0001892', 'entropy': '0.9365', 'num_tokens': '8.162e+04', 'mean_token_accuracy': '0.8238', 'epoch': '0.2473'}\n{'loss': '0.9271', 'grad_norm': '0.4199', 'learning_rate': '0.0001829', 'entropy': '0.9712', 'num_tokens': '9.376e+04', 'mean_token_accuracy': '0.8119', 'epoch': '0.2827'}\n{'loss': '0.7725', 'grad_norm': '0.3965', 'learning_rate': '0.0001754', 'entropy': '0.866', 'num_tokens': '1.055e+05', 'mean_token_accuracy': '0.8368', 'epoch': '0.318'}\n{'loss': '0.7393', 'grad_norm': '0.4902', 'learning_rate': '0.0001667', 'entropy': '0.831', 'num_tokens': '1.18e+05', 'mean_token_accuracy': '0.8458', 'epoch': '0.3534'}\n{'loss': '0.7817', 'grad_norm': '0.5156', 'learning_rate': '0.000157', 'entropy': '0.8475', 'num_tokens': '1.302e+05', 'mean_token_accuracy': '0.8386', 'epoch': '0.3887'}\n{'loss': '0.729', 'grad_norm': '0.4355', 'learning_rate': '0.0001464', 'entropy': '0.805', 'num_tokens': '1.417e+05', 'mean_token_accuracy': '0.851', 'epoch': '0.424'}\n{'loss': '0.5577', 'grad_norm': '0.5742', 'learning_rate': '0.0001351', 'entropy': '0.6385', 'num_tokens': '1.539e+05', 'mean_token_accuracy': '0.8756', 'epoch': '0.4594'}\n{'loss': '0.7507', 'grad_norm': '0.5234', 'learning_rate': '0.0001233', 'entropy': '0.759', 'num_tokens': '1.656e+05', 'mean_token_accuracy': '0.8371', 'epoch': '0.4947'}\n{'loss': '0.6172', 'grad_norm': '0.4473', 'learning_rate': '0.0001111', 'entropy': '0.6671', 'num_tokens': '1.779e+05', 'mean_token_accuracy': '0.8676', 'epoch': '0.53'}\n{'loss': '0.6097', 'grad_norm': '0.5352', 'learning_rate': '9.876e-05', 'entropy': '0.6857', 'num_tokens': '1.895e+05', 'mean_token_accuracy': '0.861', 'epoch': '0.5654'}\n{'loss': '0.712', 'grad_norm': '0.4707', 'learning_rate': '8.644e-05', 'entropy': '0.7317', 'num_tokens': '2.023e+05', 'mean_token_accuracy': '0.8485', 'epoch': '0.6007'}\n{'loss': '0.6209', 'grad_norm': '0.5117', 'learning_rate': '7.432e-05', 'entropy': '0.7042', 'num_tokens': '2.15e+05', 'mean_token_accuracy': '0.8582', 'epoch': '0.636'}\n{'loss': '0.584', 'grad_norm': '0.4844', 'learning_rate': '6.259e-05', 'entropy': '0.6481', 'num_tokens': '2.266e+05', 'mean_token_accuracy': '0.8646', 'epoch': '0.6714'}\n{'loss': '0.4871', 'grad_norm': '0.4609', 'learning_rate': '5.143e-05', 'entropy': '0.5465', 'num_tokens': '2.381e+05', 'mean_token_accuracy': '0.8872', 'epoch': '0.7067'}\n{'loss': '0.5167', 'grad_norm': '0.4473', 'learning_rate': '4.102e-05', 'entropy': '0.5627', 'num_tokens': '2.498e+05', 'mean_token_accuracy': '0.8823', 'epoch': '0.742'}\n{'loss': '0.5804', 'grad_norm': '0.4531', 'learning_rate': '3.151e-05', 'entropy': '0.5771', 'num_tokens': '2.623e+05', 'mean_token_accuracy': '0.8737', 'epoch': '0.7774'}\n{'loss': '0.4875', 'grad_norm': '0.5742', 'learning_rate': '2.304e-05', 'entropy': '0.5646', 'num_tokens': '2.733e+05', 'mean_token_accuracy': '0.8829', 'epoch': '0.8127'}\n{'loss': '0.4459', 'grad_norm': '0.5273', 'learning_rate': '1.575e-05', 'entropy': '0.5052', 'num_tokens': '2.842e+05', 'mean_token_accuracy': '0.8931', 'epoch': '0.8481'}\n{'loss': '0.5804', 'grad_norm': '0.4531', 'learning_rate': '9.75e-06', 'entropy': '0.6165', 'num_tokens': '2.96e+05', 'mean_token_accuracy': '0.8701', 'epoch': '0.8834'}\n{'loss': '0.5829', 'grad_norm': '0.5625', 'learning_rate': '5.126e-06', 'entropy': '0.6428', 'num_tokens': '3.076e+05', 'mean_token_accuracy': '0.8722', 'epoch': '0.9187'}\n{'loss': '0.5674', 'grad_norm': '0.5273', 'learning_rate': '1.952e-06', 'entropy': '0.6177', 'num_tokens': '3.189e+05', 'mean_token_accuracy': '0.8707', 'epoch': '0.9541'}\n{'loss': '0.6583', 'grad_norm': '0.5703', 'learning_rate': '2.752e-07', 'entropy': '0.7033', 'num_tokens': '3.308e+05', 'mean_token_accuracy': '0.8531', 'epoch': '0.9894'}\n{'train_runtime': '1494', 'train_samples_per_second': '0.757', 'train_steps_per_second': '0.095', 'train_loss': '0.9339', 'entropy': '0.6273', 'num_tokens': '3.34e+05', 'mean_token_accuracy': '0.8682', 'epoch': '1'}\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 142/142 [24:54<00:00, 10.52s/it]\nðŸš€ All processes finished! Model saved.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m updating run metadata (0.3s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m updating run metadata (0.3s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m updating run metadata (0.3s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m updating run metadata (0.3s)\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n\u001b[34m\u001b[1mwandb\u001b[0m:             train/entropy â–…â–†â–‡â–ˆâ–†â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–‚â–â–‚â–‚\n\u001b[34m\u001b[1mwandb\u001b[0m:               train/epoch â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n\u001b[34m\u001b[1mwandb\u001b[0m:           train/grad_norm â–ˆâ–‡â–†â–‡â–„â–‚â–‚â–â–â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–‚â–‚â–‚\n\u001b[34m\u001b[1mwandb\u001b[0m:       train/learning_rate â–ƒâ–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–…â–…â–„â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–â–â–â–\n\u001b[34m\u001b[1mwandb\u001b[0m:                train/loss â–ˆâ–ˆâ–†â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–‚\n\u001b[34m\u001b[1mwandb\u001b[0m: train/mean_token_accuracy â–â–â–‚â–„â–…â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆ\n\u001b[34m\u001b[1mwandb\u001b[0m:          train/num_tokens â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n\u001b[34m\u001b[1mwandb\u001b[0m:                total_flos 4318199520690176.0\n\u001b[34m\u001b[1mwandb\u001b[0m:             train/entropy 0.62727\n\u001b[34m\u001b[1mwandb\u001b[0m:               train/epoch 1\n\u001b[34m\u001b[1mwandb\u001b[0m:         train/global_step 142\n\u001b[34m\u001b[1mwandb\u001b[0m:           train/grad_norm 0.57031\n\u001b[34m\u001b[1mwandb\u001b[0m:       train/learning_rate 0.0\n\u001b[34m\u001b[1mwandb\u001b[0m:                train/loss 0.65832\n\u001b[34m\u001b[1mwandb\u001b[0m: train/mean_token_accuracy 0.86816\n\u001b[34m\u001b[1mwandb\u001b[0m:          train/num_tokens 334039\n\u001b[34m\u001b[1mwandb\u001b[0m:                train_loss 0.93387\n\u001b[34m\u001b[1mwandb\u001b[0m:                        +3 ...\n\u001b[34m\u001b[1mwandb\u001b[0m: \n\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mmulti-gpu-0213-1426\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/uailab-unist_/library-faq-finetune/runs/i5qd3q6x\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/uailab-unist_/library-faq-finetune\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20260213_142659-i5qd3q6x/logs\u001b[0m\n[rank0]:[W213 14:52:22.451756948 ProcessGroupNCCL.cpp:1553] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from peft import PeftModel\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM,\n    BitsAndBytesConfig, \n    TrainingArguments,\n)\nimport torch\n\n# 1. í•™ìŠµ ì™„ë£Œ í›„ ì €ìž¥ëœ ì–´ëŒ‘í„° ê²½ë¡œ\nbase_model_path = \"/kaggle/working/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\"\nadapter_path = \"/kaggle/working/SFT_multi_gpu/final\"\nrepo_id = \"JungSeong2/library-QA-Adapter\" # í—ˆê¹…íŽ˜ì´ìŠ¤ ì €ìž¥ì†Œ ì´ë¦„\ntokenizer = AutoTokenizer.from_pretrained(base_model_path)\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True, # 4bit í•  ê²ƒì´ëƒ\n    bnb_4bit_compute_dtype=torch.bfloat16, #bfloat16 or float16\n    bnb_4bit_quant_type=\"nf4\", # nf4 or fp4\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model_path,\n    torch_dtype=torch.bfloat16,\n    trust_remote_code=True,\n    quantization_config=quantization_config,\n    device_map=\"auto\"\n)\n\nif hasattr(model, \"transformer\") and hasattr(model.transformer, \"wte\"):\n    model.get_input_embeddings = lambda: model.transformer.wte\n\n# 2. í† í¬ë‚˜ì´ì €ì™€ ì–´ëŒ‘í„° ëª¨ë¸ ë¡œë“œ (ë˜ëŠ” í•™ìŠµ ì¤‘ì¸ trainer.model ì‚¬ìš©)\nmodel = PeftModel.from_pretrained(model, adapter_path)\ntokenizer = AutoTokenizer.from_pretrained(adapter_path)\n\n# 3. ì–´ëŒ‘í„° ê°€ì¤‘ì¹˜ë§Œ ì—…ë¡œë“œ\nmodel.push_to_hub(\n    repo_id=repo_id,\n    commit_message=\"[1 Epoch] FineTuned with EXAONE-3.5-2.4B-Instruct\",\n    revision=\"v1.0\" # v2.0 ë¸Œëžœì¹˜ì— í•´ë‹¹ ì–´ëŒ‘í„°ë¥¼ ì—…ë¡œë“œ\n)\n\ntokenizer.push_to_hub(\n    repo_id,\n    commit_message=\"[1 Epoch] FineTuned with EXAONE-3.5-2.4B-Instruct\",\n    revision=\"v1.0\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-13T05:54:41.130528Z","iopub.execute_input":"2026-02-13T05:54:41.131405Z","iopub.status.idle":"2026-02-13T05:54:53.369064Z","shell.execute_reply.started":"2026-02-13T05:54:41.131360Z","shell.execute_reply":"2026-02-13T05:54:53.368276Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"The repository /kaggle/working/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct contains custom code which must be executed to correctly load the model. You can inspect the repository content at /kaggle/working/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct .\n You can inspect the repository content at https://hf.co//kaggle/working/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/272 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d545a03068fe4abca324a293ed61678b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b0b1e00316147229c1b49072ae3888f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b0468ea05b14f1ba531e0800ee78bfb"}},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/JungSeong2/library-QA-Adapter/commit/e3ba893494db6f29bf4fa83fef18945620436c9c', commit_message='[1 Epoch] FineTuned with EXAONE-3.5-2.4B-Instruct', commit_description='', oid='e3ba893494db6f29bf4fa83fef18945620436c9c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/JungSeong2/library-QA-Adapter', endpoint='https://huggingface.co', repo_type='model', repo_id='JungSeong2/library-QA-Adapter'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}