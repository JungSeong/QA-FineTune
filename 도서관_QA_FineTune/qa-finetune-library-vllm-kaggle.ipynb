{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9760679,"sourceType":"datasetVersion","datasetId":5977063},{"sourceId":14794513,"sourceType":"datasetVersion","datasetId":9458905,"isSourceIdPinned":false},{"sourceId":14796359,"sourceType":"datasetVersion","datasetId":9460337}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\n\nprint(f\"íŒŒì´í† ì¹˜ ë²„ì „: {torch.__version__}\")\nprint(f\"CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€: {torch.cuda.is_available()}\")\nprint(f\"GPU ì´ë¦„: {torch.cuda.get_device_name(0)}\")\nprint(f\"CUDA ë²„ì „: {torch.version.cuda}\")\nprint(f\"PyTorch ë²„ì „: {torch.version}\")\nprint(f\"bf16 ì§€ì› ì—¬ë¶€: {torch.cuda.is_bf16_supported()}\")\nprint(f\"í˜„ì¬ GPU : {torch.cuda.current_device()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\ndef check_vram_torch():\n    # 1. í˜„ì¬ í…ì„œë“¤ì´ ì‹¤ì œë¡œ ì ìœ í•˜ê³  ìˆëŠ” ë©”ëª¨ë¦¬\n    allocated = torch.cuda.memory_allocated() / (1024**3)\n    # 2. PyTorchê°€ OSë¡œë¶€í„° í• ë‹¹ë°›ì•„ ìºì‹±í•˜ê³  ìˆëŠ” ì „ì²´ ë©”ëª¨ë¦¬\n    reserved = torch.cuda.memory_reserved() / (1024**3)\n    # 3. í•´ë‹¹ GPUì˜ ì „ì²´ ìš©ëŸ‰\n    total = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n\n    print(f\"--- GPU Memory Status ---\")\n    print(f\"Allocated: {allocated:.2f} GB\")\n    print(f\"Reserved:  {reserved:.2f} GB\")\n    print(f\"Total:     {total:.2f} GB\")\n    print(f\"Free (in Reserved): {reserved - allocated:.2f} GB\")\n    print(f\"--------------------------\")\n\ncheck_vram_torch()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import snapshot_download\n\nsave_path = \"/kaggle/working/Exaone-3.5-2.4B-Instruct-AWQ\"\n\nsnapshot_download(\n    repo_id=\"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct-AWQ\",\n    local_dir=save_path,\n    local_dir_use_symlinks=False,  # ì‹¤ì œ íŒŒì¼ì„ í•´ë‹¹ ê²½ë¡œì— ì§ì ‘ ë³µì‚¬/ë‹¤ìš´ë¡œë“œ\n    max_workers=8 # ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ì†ë„ í–¥ìƒ\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\nimport pandas as pd\nimport glob\n\nfile_path = \"/kaggle/input/datasets/jungseong2/goyanglibrary-faq1/FAQ1.xlsx\"\ndf = pd.read_excel(file_path)\ndataset = Dataset.from_pandas(df)\n\nprint(dataset)\nprint(dataset[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T05:32:47.891849Z","iopub.execute_input":"2026-02-12T05:32:47.892225Z","iopub.status.idle":"2026-02-12T05:32:48.937731Z","shell.execute_reply.started":"2026-02-12T05:32:47.892187Z","shell.execute_reply":"2026-02-12T05:32:48.936769Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport os\nimport asyncio, aiofiles\nimport wandb\nfrom tqdm.asyncio import tqdm as async_tqdm\nfrom openai import AsyncOpenAI\nimport weave\nfrom kaggle_secrets import UserSecretsClient\n\nif wandb.run is not None:\n    wandb.finish()\n\ntry:\n    user_secrets = UserSecretsClient()\n    wandb_key = user_secrets.get_secret(\"wandb\")\n    os.environ[\"WANDB_API_KEY\"] = wandb_key\n    wandb.login(key=wandb_key, relogin=True)\nexcept Exception as e:\n    print(f\"Could not retrieve W&B secret: {e}\")\n\nPROJECT_NAME=\"library-faq-augmentation-AWQ\"\n\nconfig = {\n    \"model\": \"/kaggle/working/Exaone-3.5-2.4B-Instruct-AWQ\",\n    \"quantization\": \"awq\",\n    \"engine\": \"vLLM-v1\",\n    \"temperature\": 0.7,\n    \"max_tokens\" : 2048,\n    \"top_p\" : 0.7,\n    \"max_model_len\": 16384,\n    \"gpu_util\": 0.7\n}\n\nwandb.init(project=PROJECT_NAME, name=\"EXAONE-3.5-2.4B-AWQ-vLLM-Run\")\nweave.init(PROJECT_NAME)\nsample_table = wandb.Table(columns=[\"row_id\", \"question\", \"answer\", \"label\"])\nclient = AsyncOpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"EMPTY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T05:32:48.938997Z","iopub.execute_input":"2026-02-12T05:32:48.939399Z","iopub.status.idle":"2026-02-12T05:32:54.158052Z","shell.execute_reply.started":"2026-02-12T05:32:48.939371Z","shell.execute_reply":"2026-02-12T05:32:54.157346Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import subprocess\nimport time\n\n# vLLM ì„œë²„ë¥¼ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰\nvllm_cmd = [\n    \"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\",\n    \"--model\", \"/kaggle/working/Exaone-3.5-2.4B-Instruct-AWQ\",\n    \"--port\", \"8000\",\n    \"--tensor-parallel-size\", \"2\",\n    \"--gpu-memory-utilization\", \"0.8\",\n    \"--max-model-len\", \"16384\",\n    \"--quantization\", \"awq\",\n    \"--trust-remote-code\",\n    \"--distributed-executor-backend\", \"mp\"\n]\n\n# ë¡œê·¸ë¥¼ íŒŒì¼ë¡œ ë‚¨ê¸°ë©° ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰\nwith open(\"/kaggle/working/vllm_server.log\", \"w\") as f:\n    process = subprocess.Popen(vllm_cmd, stdout=f, stderr=f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T05:32:54.159721Z","iopub.execute_input":"2026-02-12T05:32:54.160272Z","iopub.status.idle":"2026-02-12T05:32:54.167844Z","shell.execute_reply.started":"2026-02-12T05:32:54.160231Z","shell.execute_reply":"2026-02-12T05:32:54.166969Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!tail -n 40 /kaggle/working/vllm_server.log","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T05:33:58.283716Z","iopub.execute_input":"2026-02-12T05:33:58.284406Z","iopub.status.idle":"2026-02-12T05:33:59.445388Z","shell.execute_reply.started":"2026-02-12T05:33:58.284349Z","shell.execute_reply":"2026-02-12T05:33:59.444382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@weave.op()\nasync def generate_augmented_data_async(faq_content, row_id, config) :\n    system_message = (\n    \"ë‹¹ì‹ ì€ ë„ì„œê´€ FAQ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”í˜• í•™ìŠµ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\\n\"\n    \"ì œê³µëœ ì •ë³´ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ 4ê°€ì§€ ìœ í˜•ì˜ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”. ê° ìœ í˜•ë³„ë¡œ ìµœì†Œ 3ê°œì”©, ì´ 12ê°œì˜ ì˜ˆì‹œë¥¼ ë§Œë“œì„¸ìš”.\\n\\n\"\n    \"ì´ë•Œ íŒë‹¨ì´ ì‰¬ìš´ ì˜ˆì‹œì™€ ë³´í†µ, ì–´ë ¤ìš´ ì˜ˆì‹œ ì´ 3ê°€ì§€ ìƒí™©ì„ ê°€ì •í•˜ì„¸ìš”.\"\n    \n    \"### [ë‹µë³€ ê°€ì´ë“œë¼ì¸]\\n\"\n    \"1. **label: 'yes' (ê¸ì • í™•ì¸)**\\n\"\n    \"   - ì§ˆë¬¸ì´ '~í•  ìˆ˜ ìˆë‚˜ìš”?', '~ì¸ê°€ìš”?'ì²˜ëŸ¼ 'ê°€ë¶€'ë¥¼ ë¬¼ì„ ë•Œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.\\n\"\n    \"   - ë¬¸êµ¬: 'ë„¤, ê°€ëŠ¥í•©ë‹ˆë‹¤! ë„ì„œê´€ ê·œì •ì— ë”°ë¥´ë©´...'\\n\"\n    \"   - í™œìš©: ì§ˆë¬¸ì˜ ì¡°ê±´ì´ FAQì™€ ì¼ì¹˜í•  ë•Œ ì‚¬ìš©.\\n\"\n    \"2. **label: 'no' (ë¶€ì •/ì œí•œ)**\\n\"\n    \"   - ì§ˆë¬¸ì´ '~í•  ìˆ˜ ìˆë‚˜ìš”?', '~ì¸ê°€ìš”?'ì²˜ëŸ¼ 'ê°€ë¶€'ë¥¼ ë¬¼ì„ ë•Œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.\\n\"\n    \"   - ë¬¸êµ¬: 'ì£„ì†¡í•˜ì§€ë§Œ ì–´ë µìŠµë‹ˆë‹¤. ê·¸ ì´ìœ ëŠ”...'\\n\"\n    \"   - í™œìš©: ì§ˆë¬¸ì˜ ì¡°ê±´ì´ FAQ ê·œì •ì— ì–´ê¸‹ë‚  ë•Œ ì‚¬ìš©. 'ì‚¬ì‹¤ê³¼ ë‹¤ë¥´ë‹¤'ëŠ” í‘œí˜„ ëŒ€ì‹  'ê·œì •ìƒ ì–´ë µë‹¤'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\\n\"\n    \"3. **label: 'info' (ë‹¨ìˆœ ì •ë³´ ì œê³µ)**\\n\"\n    \"   - ì§ˆë¬¸ì´ '~ì€ ë¬´ì—‡ì¸ê°€ìš”?', '~ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?', '~ì€ ì–¸ì œì¸ê°€ìš”?'ì²˜ëŸ¼ ì ˆì°¨/ì‹œê°„/ë°©ë²•ì„ ë¬¼ì„ ë•Œ ì‚¬ìš©í•˜ì„¸ìš”.\\n\"\n    \"   - ë¬¸êµ¬: 'ë¬¸ì˜í•˜ì‹  ë‚´ìš©ì— ëŒ€í•´ ì•ˆë‚´í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ê´€ë ¨ ì„œë¥˜ëŠ”...'\\n\"\n    \"   - í™œìš©: ë„¤/ì•„ë‹ˆì˜¤ íŒë‹¨ì´ ì•„ë‹Œ ì •ë³´ ì§ˆë¬¸ì— ì‚¬ìš©.\\n\"\n    \"4. **label: 'false' (íŒë‹¨ ë¶ˆê°€)**\\n\"\n    \"   - ë¬¸êµ¬: 'ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì œê³µëœ ì •ë³´ë§Œìœ¼ë¡œëŠ” í•´ë‹¹ ë‚´ìš©ì„ í™•ì¸í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.'\\n\"\n    \"   - í™œìš©: FAQì— ì—†ëŠ” ë‚´ìš©ì´ê±°ë‚˜ ì§ˆë¬¸ì´ ëª¨í˜¸í•  ë•Œ ì‚¬ìš©.\\n\\n\"\n    \n    \"ëª¨ë“  ë‹µë³€ì€ ì‹¤ì œ ë„ì„œê´€ ì‚¬ì„œê°€ ë°©ë¬¸ê°ì—ê²Œ ì„¤ëª…í•˜ë“¯ ì¹œì ˆí•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\"\n    \"íŒë‹¨ ë¶ˆê°€ì˜ ê²½ìš° ë¬¸êµ¬ ì™¸ì— ë‹¤ë¥¸ ë¬¸ì¥ì€ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.\"\n    \"JSON ë‹µë³€ ì™¸ì— ë‹¤ë¥¸ ë¬¸ì¥ì€ ì¼ì²´ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.\"\n    )\n\n    # ì‚¬ìš©ìê°€ ì •ì˜í•œ user_content\n    user_content = f\"\"\"\n        [ë„ì„œê´€ FAQ ì •ë³´]\n        {faq_content}\n        \n        ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ê³¼ ë‹µë³€ì´ ë…¼ë¦¬ì ìœ¼ë¡œ ì™„ë²½í•˜ê²Œ ì´ì–´ì§€ëŠ” JSON ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.\n        \n        [ì¶œë ¥ ì˜ˆì‹œ]:\n        [\n          {{\"question\": \"ì„ì‚°ë¶€ì¸ë° ë‚¨í¸ì´ ëŒ€ì‹  ë°œê¸‰ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?\", \"answer\": \"ë„¤, ê°€ëŠ¥í•©ë‹ˆë‹¤! ë„ì„œê´€ ê·œì •ì— ë”°ë¥´ë©´ ì„ì‚°ë¶€ì˜ ê²½ìš° ëŒ€ë¦¬ ë°œê¸‰ ëŒ€ìƒì— í¬í•¨ë©ë‹ˆë‹¤.\", \"label\": \"yes\"}},\n          {{\"question\": \"ì„±ì¸ ì§ì¥ì¸ì¸ë° ì¹œêµ¬ê°€ ëŒ€ì‹  ê°€ë„ ë˜ë‚˜ìš”?\", \"answer\": \"ì£„ì†¡í•˜ì§€ë§Œ, í•´ë‹¹ ì¡°ê±´ìœ¼ë¡œëŠ” ëŒ€ë¦¬ ë°œê¸‰ì´ ì–´ë µìŠµë‹ˆë‹¤. ëŒ€ë¦¬ ë°œê¸‰ì€ ì•„ë™, ì–´ë¥´ì‹ , ì¥ì• ì¸, ì„ì‚°ë¶€ë¡œ ëŒ€ìƒì´ ì œí•œë˜ì–´ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\", \"label\": \"no\"}},\n          {{\"question\": \"ì¥ì• ì¸ ëŒ€ë¦¬ ë°œê¸‰ ì‹œ ì–´ë–¤ ì„œë¥˜ê°€ í•„ìš”í•œê°€ìš”?\", \"answer\": \"ë¬¸ì˜í•˜ì‹  ë‚´ìš©ì— ëŒ€í•´ ì•ˆë‚´í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì¥ì• ì¸ ë³µì§€ì¹´ë“œ ë˜ëŠ” ì¥ì• ì¸ ì¦ëª…ì„œë¥¼ ì§€ì°¸í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\", \"label\": \"info\"}},\n          {{\"question\": \"ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì œê³µëœ ì •ë³´ë§Œìœ¼ë¡œëŠ” í•´ë‹¹ ë‚´ìš©ì„ í™•ì¸í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.\", \"label\": \"false\"}}\n        ]\n    \"\"\"\n    \n    try :\n        chat_completion = await client.chat.completions.create(\n            model=config[\"model\"],  # MODEL_NAME ëŒ€ì‹  config[\"model\"] ì‚¬ìš©\n            messages=[\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": user_content},\n            ],\n            temperature=config[\"temperature\"],\n            max_tokens=config[\"max_tokens\"],\n            top_p=config[\"top_p\"],\n        )\n\n        raw_output = chat_completion.choices[0].message.content\n\n        if \"```json\" in raw_output:\n            raw_output = raw_output.split(\"```json\")[1].split(\"```\")[0].strip()\n        elif \"```\" in raw_output:\n            raw_output = raw_output.split(\"```\")[1].split(\"```\")[0].strip()\n\n        parsed_list = json.loads(raw_output)\n        return parsed_list\n\n    except Exception as e :\n        print(f\"Error at {row_id} : {e}\")\n        return []\n    \nasync def process_dataset(dataset, config) :\n    file_name = config[\"model\"].split(\"/\")[-1]\n    output_file = f\"library_faq_quant_{file_name}_vLLM_async.jsonl\"\n    async with aiofiles.open(output_file, \"w\", encoding=\"utf-8\") as f:\n        semaphore = asyncio.Semaphore(16)\n        lock = asyncio.Lock()\n\n        async def semaphore_task(row, idx) :\n            async with semaphore :\n                results = await generate_augmented_data_async(row[\"DES\"], idx, config)\n                \n                if results:  # ê²°ê³¼ê°€ ìˆì„ ë•Œë§Œ ì“°ê¸°\n                    async with lock:\n\n                        for item in results[:2]:\n                            sample_table.add_data(idx, item.get(\"question\"), item.get(\"answer\"), item.get(\"label\"))\n\n                        for item in results:\n                            item.update({\n                                'faq': row['FAQ'], \n                                'title': row['TITLE'], \n                                'DES': row['DES']\n                            })\n                            await f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n                \n                return len(results)\n            return 0\n\n        tasks = [semaphore_task(row, i) for i, row in enumerate(dataset)]\n\n        total_created = 0\n\n        for coro in async_tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"ë°ì´í„° ìƒì„±\"):\n            total_created += await coro\n\n    wandb.log({\"augmented_samples\": sample_table})\n    print(f\"âœ… ìƒì„± ì™„ë£Œ! ì´ {total_created}ê°œì˜ ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n\n    return output_file","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T05:32:57.987167Z","iopub.execute_input":"2026-02-12T05:32:57.987581Z","iopub.status.idle":"2026-02-12T05:32:58.010064Z","shell.execute_reply.started":"2026-02-12T05:32:57.987547Z","shell.execute_reply":"2026-02-12T05:32:58.009192Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"outputfile_name = await process_dataset(dataset, config)\nartifact = wandb.Artifact(f\"{config['model']}_{wandb.run.id}\", type='dataset')\nartifact.add_file(outputfile_name)\nwandb.log_artifact(artifact)\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-12T05:34:06.630413Z","iopub.execute_input":"2026-02-12T05:34:06.630789Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport json\n\ndata = []\nwith open(\"/kaggle/input/augmented-llibrary-faq-quant-exaone-jsonl/augmented_library_faq_quant_EXAONE_32B_vLLM.jsonl\", \"r\", encoding=\"utf-8\") as f :\n    for line in f :\n        data.append(json.loads(line))\n\ndf = pd.DataFrame(data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df[\"label\"].value_counts())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df.isnull().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df.info())\ndf = df.drop(columns=\"url\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df.info())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx = df[df[\"answer\"].isna()].index\ndf.drop(idx, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.reset_index(drop=True)\nprint(df.info())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport json\n\n# 2. ë¼ë²¨ ë¶„í¬ í™•ì¸ (ê· í˜•ì„±)\nprint(\"--- [1. Label Distribution] ---\")\nprint(df['label'].value_counts())\n\nplt.figure(figsize=(8, 5))\nsns.countplot(data=df, x='label', order=['yes', 'no', 'info', 'false'])\nplt.title(\"Label Distribution\")\n# plt.savefig(\"/kaggle/working/label_distribution.png\")\nplt.savefig(\"/kaggle/working/label_distribution4.png\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pivot = df.groupby([\"title\", \"label\"]).size().unstack(fill_value=0) # ì¸ë±ìŠ¤ ê°’ì„ ì»¬ëŸ¼ìœ¼ë¡œ\nneed_aug_idx = pivot[(pivot[\"yes\"]<=1) | (pivot[\"no\"]<=2) | (pivot[\"false\"] <=2)]\n\nprint(pivot)\nprint(len(need_aug_idx))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_target_labels(title, pivot_table):\n    \"\"\"íŠ¹ì • titleì—ì„œ ì¦ê°•ì´ í•„ìš”í•œ ë¼ë²¨ ëª©ë¡ì„ ë°˜í™˜\"\"\"\n    row = pivot_table.loc[title]\n    targets = []\n    # ë¶€ì¡±í•œ ë¼ë²¨ íŒë‹¨ ê¸°ì¤€ (ì˜ˆ: 3ê°œ ì´í•˜)\n    if row.get(\"yes\", 0) <= 3: targets.append(\"yes\")\n    if row.get(\"no\", 0) <= 3: targets.append(\"no\")\n    if row.get(\"false\", 0) <= 3: targets.append(\"false\")\n    return targets","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@weave.op()\nasync def generate_targeted_data_async(faq_content, target_labels, config):\n    # í•„ìš”í•œ ë¼ë²¨ì— ëŒ€í•œ ê°€ì´ë“œë¼ì¸ / ì˜ˆì‹œë§Œ í”„ë¡¬í”„íŠ¸ë¡œ ë„£ì–´ì„œ, ë‹¤ë¥¸ ì˜ˆì‹œê°€ ìƒì„±ë˜ì§€ ì•Šë„ë¡ í•˜ê¸°\n    all_guidelines = {\n        \"yes\" : \"ì§ˆë¬¸ì´ '~í•  ìˆ˜ ìˆë‚˜ìš”?', '~ì¸ê°€ìš”?'ì²˜ëŸ¼ 'ê°€ë¶€'ë¥¼ ë¬¼ì„ ë•Œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.\\n - ë¬¸êµ¬: 'ë„¤, ê°€ëŠ¥í•©ë‹ˆë‹¤! ë„ì„œê´€ ê·œì •ì— ë”°ë¥´ë©´...'\\n - í™œìš©: ì§ˆë¬¸ì˜ ì¡°ê±´ì´ FAQì™€ ì¼ì¹˜í•  ë•Œ ì‚¬ìš©.\\n\",\n        \"no\" : \"ì§ˆë¬¸ì´ '~í•  ìˆ˜ ìˆë‚˜ìš”?', '~ì¸ê°€ìš”?'ì²˜ëŸ¼ 'ê°€ë¶€'ë¥¼ ë¬¼ì„ ë•Œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.\\n - ë¬¸êµ¬: 'ì£„ì†¡í•˜ì§€ë§Œ ì–´ë µìŠµë‹ˆë‹¤. ê·¸ ì´ìœ ëŠ”...'\\n - í™œìš©: ì§ˆë¬¸ì˜ ì¡°ê±´ì´ FAQ ê·œì •ì— ì–´ê¸‹ë‚  ë•Œ ì‚¬ìš©. 'ì‚¬ì‹¤ê³¼ ë‹¤ë¥´ë‹¤'ëŠ” í‘œí˜„ ëŒ€ì‹  'ê·œì •ìƒ ì–´ë µë‹¤'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\\n\",\n        \"info\" : \"ì§ˆë¬¸ì´ '~ì€ ë¬´ì—‡ì¸ê°€ìš”?', '~ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?', '~ì€ ì–¸ì œì¸ê°€ìš”?'ì²˜ëŸ¼ ì ˆì°¨/ì‹œê°„/ë°©ë²•ì„ ë¬¼ì„ ë•Œ ì‚¬ìš©í•˜ì„¸ìš”.\\n ë¬¸êµ¬: 'ë¬¸ì˜í•˜ì‹  ë‚´ìš©ì— ëŒ€í•´ ì•ˆë‚´í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ê´€ë ¨ ì„œë¥˜ëŠ”...'\\n - í™œìš©: ë„¤/ì•„ë‹ˆì˜¤ íŒë‹¨ì´ ì•„ë‹Œ ì •ë³´ ì§ˆë¬¸ì— ì‚¬ìš©.\\n\",\n        \"false\" : \"ë¬¸êµ¬: 'ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì œê³µëœ ì •ë³´ë§Œìœ¼ë¡œëŠ” í•´ë‹¹ ë‚´ìš©ì„ í™•ì¸í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.'\\n - í™œìš©: FAQì— ì—†ëŠ” ë‚´ìš©ì´ê±°ë‚˜ ì§ˆë¬¸ì´ ëª¨í˜¸í•  ë•Œ ì‚¬ìš©.\\n\\n\"\n    }\n    selected_guidelines = \"\\n\".join([all_guidelines[l] for l in target_labels if l in all_guidelines])\n\n    all_examples = {\n        \"yes\": '{\"question\": \"ì„ì‚°ë¶€ì¸ë° ë‚¨í¸ì´ ëŒ€ì‹  ë°œê¸‰ë°›ì„ ìˆ˜ ìˆë‚˜ìš”?\", \"answer\": \"ë„¤, ê°€ëŠ¥í•©ë‹ˆë‹¤! ë„ì„œê´€ ê·œì •ì— ë”°ë¥´ë©´ ì„ì‚°ë¶€ì˜ ê²½ìš° ëŒ€ë¦¬ ë°œê¸‰ ëŒ€ìƒì— í¬í•¨ë©ë‹ˆë‹¤.\", \"label\": \"yes\"}',\n        \"no\": '{\"question\": \"ì„±ì¸ ì§ì¥ì¸ì¸ë° ì¹œêµ¬ê°€ ëŒ€ì‹  ê°€ë„ ë˜ë‚˜ìš”?\", \"answer\": \"ì£„ì†¡í•˜ì§€ë§Œ, í•´ë‹¹ ì¡°ê±´ìœ¼ë¡œëŠ” ëŒ€ë¦¬ ë°œê¸‰ì´ ì–´ë µìŠµë‹ˆë‹¤. ëŒ€ë¦¬ ë°œê¸‰ì€ ì•„ë™, ì–´ë¥´ì‹ , ì¥ì• ì¸, ì„ì‚°ë¶€ë¡œ ëŒ€ìƒì´ ì œí•œë˜ì–´ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\", \"label\": \"no\"}',\n        \"info\": '{\"question\": \"ì¥ì• ì¸ ëŒ€ë¦¬ ë°œê¸‰ ì‹œ ì–´ë–¤ ì„œë¥˜ê°€ í•„ìš”í•œê°€ìš”?\", \"answer\": \"ë¬¸ì˜í•˜ì‹  ë‚´ìš©ì— ëŒ€í•´ ì•ˆë‚´í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì¥ì• ì¸ ë³µì§€ì¹´ë“œ ë˜ëŠ” ì¥ì• ì¸ ì¦ëª…ì„œë¥¼ ì§€ì°¸í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\", \"label\": \"info\"}',\n        \"false\": '{\"question\": \"ë„ì„œê´€ ê·¼ì²˜ì— ë§›ìˆëŠ” ì‹ë‹¹ì´ ì–´ë””ì¸ê°€ìš”?\", \"answer\": \"ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì œê³µëœ ì •ë³´ë§Œìœ¼ë¡œëŠ” í•´ë‹¹ ë‚´ìš©ì„ í™•ì¸í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.\", \"label\": \"false\"}'\n    }   \n\n    target_str = \", \".join(target_labels)\n    selected_examples = \",\\n      \".join([all_examples[l] for l in target_labels if l in all_examples])\n    system_message = (\n        f\"ë‹¹ì‹ ì€ ë„ì„œê´€ FAQ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¶€ì¡±í•œ ìœ í˜•ì˜ í•™ìŠµ ë°ì´í„°ë¥¼ ë³´ì¶©í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\\n\"\n        f\"ì œê³µëœ ì •ë³´ë¥¼ ë¶„ì„í•˜ì—¬ ë°˜ë“œì‹œ ë‹¤ìŒ ë¼ë²¨ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë§Œ ìƒì„±í•˜ì„¸ìš” : **[{target_str}]**\\n\\n\"\n        f\"ì´ë•Œ íŒë‹¨ì´ ì‰¬ìš´ ì˜ˆì‹œì™€ ë³´í†µ, ì–´ë ¤ìš´ ì˜ˆì‹œ ì´ 3ê°€ì§€ ìƒí™©ì„ ê°€ì •í•˜ì—¬ ê° ìƒí™©ë³„ë¡œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.\"\n        f\"ê° ë¼ë²¨ë§ˆë‹¤ ìµœì†Œ 2~3ê°œì˜ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”\"\n        \n        f\"### [ë‹µë³€ ê°€ì´ë“œë¼ì¸]\\n{selected_guidelines}\\n\\n\"\n        \"ëª¨ë“  ë‹µë³€ì€ ì‹¤ì œ ë„ì„œê´€ ì‚¬ì„œê°€ ë°©ë¬¸ê°ì—ê²Œ ì„¤ëª…í•˜ë“¯ ì¹œì ˆí•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.\"\n        f\"**[{target_str}]**ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ë¼ë²¨ì€ ì ˆëŒ€ë¡œ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”\"\n        \"íŒë‹¨ ë¶ˆê°€ì˜ ê²½ìš° ë¬¸êµ¬ ì™¸ì— ë‹¤ë¥¸ ë¬¸ì¥ì€ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.\"\n        \"JSON ë‹µë³€ ì™¸ì— ë‹¤ë¥¸ ë¬¸ì¥ì€ ì¼ì²´ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”.\"\n    )\n\n    # ì‚¬ìš©ìê°€ ì •ì˜í•œ user_content\n    user_content = f\"\"\"\n    [ë„ì„œê´€ FAQ ì •ë³´]\n    {faq_content}\n    \n    ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ê³¼ ë‹µë³€ì´ ë…¼ë¦¬ì ìœ¼ë¡œ ì™„ë²½í•˜ê²Œ ì´ì–´ì§€ëŠ” JSON ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.\n    \n    [ì¶œë ¥ ì˜ˆì‹œ]:\n    {selected_examples}\n    \"\"\"\n    \n    messages = [\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": user_content},\n    ]\n\n    print(messages)\n\n    try :\n        chat_completion = await client.chat.completions.create(\n            model=config[\"model\"],  # MODEL_NAME ëŒ€ì‹  config[\"model\"] ì‚¬ìš©\n            messages=[\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": user_content},\n            ],\n            temperature=config[\"temperature\"],\n            max_tokens=config[\"max_tokens\"],\n            top_p=config[\"top_p\"],\n        )\n\n        raw_output = chat_completion.choices[0].message.content\n\n        if \"```json\" in raw_output:\n            raw_output = raw_output.split(\"```json\")[1].split(\"```\")[0].strip()\n        elif \"```\" in raw_output:\n            raw_output = raw_output.split(\"```\")[1].split(\"```\")[0].strip()\n\n        parsed_list = json.loads(raw_output)\n        return parsed_list\n\n    except Exception as e :\n        print(f\"Error : {e}\")\n        return []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\nasync def process_targeted_dataset(dataset, pivot_table, config) :\n    file_name = config[\"model\"].split(\"/\")[-1]\n\n    # original_input_file = \"/kaggle/input/augmented-llibrary-faq-quant-exaone-jsonl/augmented_library_faq_quant_EXAONE_32B_vLLM.jsonl\"\n    # original_input_file = f\"/kaggle/working/augmented_library_faq_quant_{file_name}_v2.jsonl\"\n    original_input_file = f\"/kaggle/working/augmented_library_faq_quant_{file_name}_v3.jsonl\"\n    output_file = f\"/kaggle/working/augmented_library_faq_quant_{file_name}_v4.jsonl\"\n\n    shutil.copy(original_input_file, output_file)\n    \n    async with aiofiles.open(output_file, \"a\", encoding=\"utf-8\") as f:\n        semaphore = asyncio.Semaphore(16)\n        lock = asyncio.Lock()\n\n        async def semaphore_task(row, idx) :\n            target_labels = get_target_labels(row[\"TITLE\"], pivot_table)\n            print(target_labels)\n            \n            async with semaphore :\n                results = await generate_targeted_data_async(row[\"DES\"], target_labels, config)\n                \n                if results:  # ê²°ê³¼ê°€ ìˆì„ ë•Œë§Œ ì“°ê¸°\n                    async with lock:\n                        for item in results[:2]:\n                            sample_table.add_data(idx, item.get(\"question\"), item.get(\"answer\"), item.get(\"label\"))\n\n                        for item in results:\n                            item.update({\n                                'faq': row['FAQ'], \n                                'title': row['TITLE'], \n                                'DES': row['DES']\n                            })\n                            await f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n                \n                return len(results)\n            return 0\n\n        tasks = [semaphore_task(row, i) for i, row in enumerate(dataset)]\n\n        total_created = 0\n\n        for coro in async_tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=\"ë°ì´í„° ìƒì„±\"):\n            total_created += await coro\n\n    wandb.log({\"augmented_samples\": sample_table})\n    print(f\"âœ… ìƒì„± ì™„ë£Œ! ì´ {total_created}ê°œì˜ ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n\n    return output_file","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_dataset = dataset.to_pandas()\n\ntarget_titles = need_aug_idx.index.tolist()\nfiltered_df = df_dataset[df_dataset['TITLE'].isin(target_titles)][['FAQ', 'TITLE', 'DES']].drop_duplicates()\n\ntarget_data_list = filtered_df.to_dict('records')\nprint(len(target_data_list))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"outputfile_name = await process_targeted_dataset(target_data_list, pivot, config)\nartifact = wandb.Artifact(f\"{config['model']}_{wandb.run.id}\", type='dataset')\nartifact.add_file(outputfile_name)\nwandb.log_artifact(artifact)\nwandb.finish()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport json\n\ndata = []\nwith open(\"/kaggle/working/QA-FineTune/ë„ì„œê´€_QA_FineTune/augmented_library_faq_quant_Exaone-3.5-7.8B-Instruct-AWQ_v4.jsonl\", \"r\", encoding=\"utf-8\") as f :\n    for line in f :\n        data.append(json.loads(line))\n\ndf = pd.DataFrame(data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df[\"label\"].value_counts())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df.isnull().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df.info())\ndf = df.drop(columns=\"url\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df.info())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"idx = df[df[\"answer\"].isna()].index\ndf.drop(idx, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.reset_index(drop=True)\nprint(df.info())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset\n\ndf['faq'] = df['faq'].astype(str)\nhf_dataset = Dataset.from_pandas(df[[\"question\", \"answer\",\"DES\"]])\nseed = 42\n\nds_split = hf_dataset.train_test_split(test_size=0.2, seed=seed)\ntrain_val_split = ds_split[\"train\"].train_test_split(test_size = 0.3, seed=seed)\n\nfrom datasets import DatasetDict\n\nfinal_dataset = DatasetDict({\n    'train': train_val_split['train'],\n    'val': train_val_split['test'],\n    'test': ds_split['test']\n})\n\ntrain_dataset, val_dataset, test_dataset = final_dataset['train'], final_dataset['val'], final_dataset['test']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_dataset)\nprint(train_dataset[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ[\"HF_TOKEN\"]=hf_token","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ìƒì„±í•œ ë°ì´í„° ì…‹ ì €ì¥\nfrom datasets import Dataset, DatasetDict\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\n\nhf_token = user_secrets.get_secret(\"hf_token\")\n\nlogin(token=hf_token)\n\ndataset_dict = DatasetDict({\n    \"train\": train_dataset,\n    \"validation\": val_dataset,\n    \"test\": test_dataset\n})\n\ndataset_dict.push_to_hub(\"JungSeong2/library-qa\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\nfrom kaggle_secrets import UserSecretsClient\n\n# 1. í™˜ê²½ ì„¤ì •\nuser_secrets = UserSecretsClient()\nGITHUB_TOKEN = user_secrets.get_secret(\"GITHUB_TOKEN\") \nUSER_EMAIL = user_secrets.get_secret(\"USER_EMAIL\")\nUSER_NAME = \"JungSeong\"\nREPO_NAME = \"QA-FineTune\"\nSUB_DIR = \"ë„ì„œê´€_QA_FineTune\"\nFILE_NAME = \"label_distribution4.png\"\n\n# 2. Git ì„¤ì •\n!git config --global user.email \"{USER_EMAIL}\"\n!git config --global user.name \"{USER_NAME}\"\n\n# 3. ë ˆí¬ì§€í† ë¦¬ í´ë¡  (í´ë”ê°€ ì—†ì„ ë•Œë§Œ)\nif not os.path.exists(REPO_NAME):\n    !git clone https://{GITHUB_TOKEN}@github.com/{USER_NAME}/{REPO_NAME}.git\nelse:\n    print(\"âœ… Repository already exists.\")\n\n# 4. í´ë” ìƒì„± ë° íŒŒì¼ ë³µì‚¬ (Python ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©)\nworking_path = \"/kaggle/working\"\nrepo_path = os.path.join(working_path, REPO_NAME)\ntarget_dir = os.path.join(repo_path, SUB_DIR)\n\n# ë§Œì•½ ê°™ì€ ì´ë¦„ì˜ 'íŒŒì¼'ì´ ì¡´ì¬í•˜ë©´ ì‚­ì œí•˜ê³  í´ë”ë¥¼ ë§Œë“­ë‹ˆë‹¤.\nif os.path.exists(target_dir) and not os.path.isdir(target_dir):\n    os.remove(target_dir)\n\nos.makedirs(target_dir, exist_ok=True)\n\n# íŒŒì¼ ë³µì‚¬\nsource_file = os.path.join(working_path, FILE_NAME)\ndestination_file = os.path.join(target_dir, FILE_NAME)\n\nif os.path.exists(source_file):\n    shutil.copy(source_file, destination_file)\n    print(f\"âœ… íŒŒì¼ ë³µì‚¬ ì™„ë£Œ: {destination_file}\")\nelse:\n    print(f\"âŒ ì›ë³¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {source_file}\")\n\n# 5. Commit & Push\n%cd {repo_path}\n!git add .\n!git commit -m \"Update: {SUB_DIR}/{FILE_NAME}\"\n!git push origin main\n\nprint(f\"\\nğŸš€ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë° ë°ì´í„° ì¦ê°• ì´í›„ íŒŒì¸íŠœë‹ (LoRA + DeepSpeed)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. í•™ìŠµ ì‹œí‚¬ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM,\n    BitsAndBytesConfig, \n    TrainingArguments,\n)\nfrom datasets import Dataset\nimport os, torch, json, wandb, subprocess\nfrom sklearn.model_selection import train_test_split\nimport torch.nn as nn\nfrom peft import (\n    get_peft_model,\n    LoraConfig, \n    TaskType,\n    prepare_model_for_kbit_training\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import snapshot_download\n\nsave_path = \"/kaggle/working/Exaone-3.5-2.4B-Instruct\"\n\nsnapshot_download(\n    repo_id=\"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\",\n    local_dir=save_path,\n    local_dir_use_symlinks=False,  # ì‹¤ì œ íŒŒì¼ì„ í•´ë‹¹ ê²½ë¡œì— ì§ì ‘ ë³µì‚¬/ë‹¤ìš´ë¡œë“œ\n    max_workers=8 # ë³‘ë ¬ ë‹¤ìš´ë¡œë“œ ì†ë„ í–¥ìƒ\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### DeepSpeed ë¼ì´ë¸ŒëŸ¬ë¦¬\nhttps://huggingface.co/docs/transformers/ko/deepspeed?zero-config=ZeRO-2","metadata":{}},{"cell_type":"code","source":"# DeepSpeed config ìƒì„±\nimport json\n\nds_config = {\n    \"fp16\": { \"enabled\": True }, # 16ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì  ì—°ì‚° (T4 GPU ìµœì í™”)\n    \"zero_optimization\": {\n        \"stage\": 2, # ì˜µí‹°ë§ˆì´ì €ì™€ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ë¶„ì‚° ì €ì¥\n        \"allgather_partitions\": True, # ë¶„ì‚° ì²˜ë¦¬ëœ GPU íŒŒë¼ë¯¸í„° ëª¨ì„ ë•Œ ë” íš¨ìœ¨ì ì¸ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©\n        \"allgather_bucket_size\": 2e8, # í†µì‹ í•  ë•Œ ë°ì´í„°ë¥¼ ë¬¶ì–´ì„œ ë³´ë‚´ëŠ” ë°”êµ¬ë‹ˆ í¬ê¸°\n        \"overlap_comm\": True, # ì—°ì‚°ê³¼ í†µì‹ ì„ ë™ì‹œì— ì§„í–‰, ë³‘ëª©ì„ ì¤„ì„\n        \"reduce_scatter\": True, # í†µì‹  ì˜¤ë²„í—¤ë“œ ê°ì†Œ\n        \"reduce_bucket_size\": 2e8, # ê·¸ë˜ë””ì–¸íŠ¸ ê°ì†Œ ì—°ì‚° ì‹œ ë°ì´í„°ë¥¼ ë¬¶ëŠ” í¬ê¸°\n        \"contiguous_gradients\": True # ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ë©”ëª¨ë¦¬ìƒì— ì—°ì†ì ìœ¼ë¡œ ë°°ì¹˜\n    },\n    \"gradient_accumulation_steps\": \"auto\", # TrainingArgumentsì— ì„¤ì •í•œ gradient_accumulation_steps ê°’\n    \"gradient_clipping\": \"auto\", # ê·¸ë˜ë””ì–¸íŠ¸ í­ì£¼ë¥¼ ë§‰ê¸° ìœ„í•œ í´ë¦¬í•‘(Max Norm) ê°’ì„ ìë™ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤\n    \"steps_per_print\": 10, # DeepSpeed ë‚´ë¶€ì˜ ìƒíƒœ(ì²˜ë¦¬ëŸ‰, ë©”ëª¨ë¦¬ ë“±)ë¥¼ ì–¼ë§ˆë‚˜ ìì£¼ ë¡œê·¸ì— ì°ì„ì§€\n    \"train_batch_size\": \"auto\", # \n    \"train_micro_batch_size_per_gpu\": \"auto\", # GPU í•œ ëŒ€ë‹¹ í•œ ë²ˆì— ì²˜ë¦¬í•˜ëŠ” ë°°ì¹˜ ì‚¬ì´ì¦ˆ\n    \"wall_clock_breakdown\": False\n}\n\nwith open(\"/kaggle/working/ds_config.json\", \"w\") as f:\n    json.dump(ds_config, f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(\"/kaggle/working/ds_config.json\", \"r\") as f:\n    print(f.read())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport os\nimport wandb\nfrom datetime import datetime\n\nif wandb.run is not None:\n    wandb.finish()\n\ntry:\n    user_secrets = UserSecretsClient()\n    wandb_key = user_secrets.get_secret(\"wandb\")\n    os.environ[\"WANDB_API_KEY\"] = wandb_key\n    wandb.login(key=wandb_key, relogin=True)\nexcept Exception as e:\n    print(f\"Could not retrieve W&B secret: {e}\")\n\nPROJECT_NAME=\"library-faq-finetune\"\n\nos.environ[\"WANDB_PROJECT\"] = PROJECT_NAME # í”„ë¡œì íŠ¸ ì´ë¦„\nos.environ[\"WANDB_RUN_ID\"] = \"library_QA_v1\" # ë…¸íŠ¸ë¶ ê³ ìœ  ID\nos.environ[\"WANDB_RESUME\"] = \"allow\" # í•´ë‹¹ ë…¸íŠ¸ë¶ì—ì„œ í•™ìŠµì„ ì´ì–´ì„œ ì§„í–‰í•  ê²ƒì¸ì§€\n\nwandb.init(\n    project=os.environ[\"WANDB_PROJECT\"],\n    id=os.environ[\"WANDB_RUN_ID\"],\n    resume=os.environ[\"WANDB_RESUME\"],\n    name=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\") # ëŒ€ì‹œë³´ë“œì— í‘œì‹œë  ì´ë¦„\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"DataCollatorForCompletionOnlyLM ì•ˆë˜ëŠ” í˜„ìƒ ë°œìƒ </br>\n!deepspeed --num_gpus=2 QA-FineTune/ë„ì„œê´€_QA_FineTune/train_with_deepspeed.py","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/QA-FineTune/ë„ì„œê´€_QA_FineTune/train_with_deepspeed.py\nimport os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom transformers.integrations import HfDeepSpeedConfig\nfrom peft import LoraConfig, get_peft_model\nfrom trl import SFTTrainer, SFTConfig\nfrom huggingface_hub import login\n\nMODEL_ID = \"/kaggle/working/Exaone-3.5-2.4B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n\ndef generate_prompts(example):  # ë‹¨ì¼ ì˜ˆì‹œ ì²˜ë¦¬\n    system_message = (\n        \"ë‹¹ì‹ ì€ ë„ì„œê´€ ìš´ì˜ì— ëŒ€í•œ ì „ë¬¸ì ì¸ ì§€ì‹ì„ ê°€ì§„ ì¸ê³µì§€ëŠ¥ ì‚¬ì„œì…ë‹ˆë‹¤.\"\n        \"ì œê³µëœ [ë„ì„œê´€ ì •ë³´]ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µí•˜ì‹­ì‹œì˜¤. \"\n        \"ì •ë³´ì— ì—†ëŠ” ë‚´ìš©ì€ í•¨ë¶€ë¡œ ì¶”ì¸¡í•˜ì§€ ë§ê³  ì •ì¤‘íˆ í™•ì¸ì´ ì–´ë µë‹¤ê³  ë‹µí•˜ì„¸ìš”.\"\n    )\n    \n    question = example[\"question\"]\n    answer = example[\"answer\"]\n    context = example[\"DES\"]\n    \n    user_content = (\n        f\"### [ë„ì„œê´€ ì •ë³´]\\n{context}\\n\\n\"\n        f\"### [ì§ˆë¬¸]\\n{question}\\n\\n\"\n        f\"### [ì§€ì‹œ ì‚¬í•­]\\n\"\n        f\"1. ì¹œì ˆí•œ ë§íˆ¬ë¡œ ê·œì •ì— ê·¼ê±°í•˜ì—¬ ë‹µë³€í•  ê²ƒ.\\n\"\n        f\"2. 3ë¬¸ë‹¨ ì´ë‚´ë¡œ ë‹µë³€í•  ê²ƒ.\\n\"\n        f\"3. ë‹µë³€ ëì— ì§€ì‹œ ì‚¬í•­ì„ ë°˜ë³µí•˜ì§€ ë§ ê²ƒ.\"\n    )\n    \n    messages = [\n        {\"role\": \"system\", \"content\": system_message},\n        {\"role\": \"user\", \"content\": user_content},\n        {\"role\": \"assistant\", \"content\": answer}\n    ]\n    \n    full_prompt = tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=False\n    )\n    \n    return full_prompt\n\ndef train_loop(dataset):\n    local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '29501'\n    \n    # DeepSpeed ìˆ˜ë™ ì´ˆê¸°í™” (NoneType ì—ëŸ¬ ë°©ì§€)\n    dschf = HfDeepSpeedConfig(\"/kaggle/working/ds_config.json\")\n    \n    train_dataset = dataset[\"train\"]\n    val_dataset = dataset[\"validation\"]\n    \n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_ID,\n        torch_dtype=torch.float16,\n        trust_remote_code=True\n    )  # device_map ì œê±°: DeepSpeedê°€ ì²˜ë¦¬\n    \n    model.get_input_embeddings = lambda: model.transformer.wte\n    \n    modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]\n    peft_config = LoraConfig(\n        task_type=\"CAUSAL_LM\",\n        inference_mode=False,\n        r=16,\n        lora_alpha=16,\n        lora_dropout=0.05,\n        target_modules=modules\n    )\n    model = get_peft_model(model, peft_config)\n    \n    training_args = SFTConfig(\n        output_dir=\"./SFT\",\n        num_train_epochs=1,\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=8,\n        optim=\"paged_adamw_32bit\",\n        learning_rate=5e-5,\n        bf16=False,\n        fp16=True,\n        max_seq_length=2048,\n        logging_steps=10,\n        completion_only_loss=False,\n        packing=False,\n        deepspeed=\"/kaggle/working/ds_config.json\",\n        report_to=\"none\"\n    )\n    \n    trainer = SFTTrainer(\n        model=model,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        args=training_args,\n        formatting_func=generate_prompts,\n        processing_class=tokenizer\n    )\n    \n    print(f\"Model device: {next(model.parameters()).device}\")\n    trainer.accelerator.print(trainer.model)\n    \n    trainer.train()\n    \n    final_save_path = \"./SFT/final\"\n    os.makedirs(final_save_path, exist_ok=True)\n    if trainer.is_world_process_zero():\n        trainer.model.save_model(final_save_path)\n        tokenizer.save_pretrained(final_save_path)\n\nif __name__ == \"__main__\":\n    hf_token = os.environ.get(\"HF_TOKEN\")\n    if hf_token:\n        login(token=hf_token)\n        print(\"Logged in to HuggingFace\")\n    dataset = load_dataset(\"JungSeong2/library-qa\", token=hf_token)\n    \n    train_loop(dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\n\nhf_token = user_secrets.get_secret(\"hf_token\")\n\nlogin(token=hf_token)\n\nos.environ[\"HF_TOKEN\"]=hf_token","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}